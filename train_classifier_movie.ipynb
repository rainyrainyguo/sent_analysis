{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open('movie/train.tsv','r') as f:\n",
    "    for line in f:\n",
    "        train.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PhraseId\\tSentenceId\\tPhrase\\tSentiment',\n",
       " '1\\t1\\tA series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\\t1',\n",
       " '2\\t1\\tA series of escapades demonstrating the adage that what is good for the goose\\t2']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62424.4"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78030.5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "train1 = train[1:80001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "label=[]\n",
    "for x in train1:\n",
    "    s = x.split('\\t')\n",
    "    text.append(s[2])\n",
    "    label.append(s[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie/mytrain.tsv','w') as f:\n",
    "    for i in range(60000):\n",
    "        f.write(text[i]+'\\t'+label[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie/mytest.tsv','w') as f:\n",
    "    for i in range(20000):\n",
    "        f.write(text[i+60000]+'\\t'+label[i+60000]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie/mytrain1.tsv','w') as f:\n",
    "    for i in range(30000):\n",
    "        f.write(text[i]+'\\t'+label[i]+'\\n')\n",
    "with open('movie/mytrain2.tsv','w') as f:\n",
    "    for i in range(30000):\n",
    "        f.write(text[i+30000]+'\\t'+label[i+30000]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train1[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=[]\n",
    "label2=[]\n",
    "for x in train2:\n",
    "    s = x.split('\\t')\n",
    "    text2.append(s[2])\n",
    "    label2.append(s[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie/mytrain3.tsv','w') as f:\n",
    "    for i in range(30000):\n",
    "        f.write(text2[i]+'\\t'+label2[i]+'\\n')\n",
    "with open('movie/mytrain4.tsv','w') as f:\n",
    "    for i in range(30000):\n",
    "        f.write(text2[i+30000]+'\\t'+label2[i+30000]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty300.tsv...\n"
     ]
    }
   ],
   "source": [
    "BeautyTEXT = data.Field(tokenize='spacy')\n",
    "BeautyLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300.tsv...\")\n",
    "Beautytrain  = data.TabularDataset.splits(\n",
    "        path='movie/', \n",
    "        train='mytrain1.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', BeautyTEXT),('Label', BeautyLABEL)])[0]\n",
    "\n",
    "BeautyTEXT.build_vocab(Beautytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "BeautyLABEL.build_vocab(Beautytrain)\n",
    "\n",
    "BeautyLABEL.vocab.stoi['1']=1\n",
    "BeautyLABEL.vocab.stoi['2']=2\n",
    "BeautyLABEL.vocab.stoi['3']=3\n",
    "BeautyLABEL.vocab.stoi['4']=4\n",
    "BeautyLABEL.vocab.stoi['0']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ApparelTEXT = data.Field(tokenize='spacy')\n",
    "ApparelLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "Appareltrain  = data.TabularDataset.splits(\n",
    "        path='movie/', \n",
    "        train='mytrain2.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ApparelTEXT),('Label', ApparelLABEL)])[0]\n",
    "\n",
    "ApparelTEXT.build_vocab(Appareltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ApparelLABEL.build_vocab(Appareltrain)\n",
    "\n",
    "ApparelLABEL.vocab.stoi['1']=1\n",
    "ApparelLABEL.vocab.stoi['2']=2\n",
    "ApparelLABEL.vocab.stoi['3']=3\n",
    "ApparelLABEL.vocab.stoi['4']=4\n",
    "ApparelLABEL.vocab.stoi['0']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Jewelry300.tsv...\n"
     ]
    }
   ],
   "source": [
    "JewelryTEXT = data.Field(tokenize='spacy')\n",
    "JewelryLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Jewelry300.tsv...\")\n",
    "Jewelrytrain  = data.TabularDataset.splits(\n",
    "        path='movie/', \n",
    "        train='mytrain3.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', JewelryTEXT),('Label', JewelryLABEL)])[0]\n",
    "\n",
    "JewelryTEXT.build_vocab(Jewelrytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "JewelryLABEL.build_vocab(Jewelrytrain)\n",
    "\n",
    "JewelryLABEL.vocab.stoi['1']=1\n",
    "JewelryLABEL.vocab.stoi['2']=2\n",
    "JewelryLABEL.vocab.stoi['3']=3\n",
    "JewelryLABEL.vocab.stoi['4']=4\n",
    "JewelryLABEL.vocab.stoi['0']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Shoes300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ShoesTEXT = data.Field(tokenize='spacy')\n",
    "ShoesLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Shoes300.tsv...\")\n",
    "Shoestrain  = data.TabularDataset.splits(\n",
    "        path='movie/', \n",
    "        train='mytrain4.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ShoesTEXT),('Label', ShoesLABEL)])[0]\n",
    "\n",
    "ShoesTEXT.build_vocab(Shoestrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ShoesLABEL.build_vocab(Shoestrain)\n",
    "\n",
    "ShoesLABEL.vocab.stoi['1']=1\n",
    "ShoesLABEL.vocab.stoi['2']=2\n",
    "ShoesLABEL.vocab.stoi['3']=3\n",
    "ShoesLABEL.vocab.stoi['4']=4\n",
    "ShoesLABEL.vocab.stoi['0']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "allTEXT = data.Field(tokenize='spacy')\n",
    "allLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "alltrain  = data.TabularDataset.splits(\n",
    "        path='movie/', \n",
    "        train='mytrain.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', allTEXT),('Label', allLABEL)])[0]\n",
    "\n",
    "allTEXT.build_vocab(alltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "allLABEL.build_vocab(alltrain)\n",
    "\n",
    "allLABEL.vocab.stoi['1']=1\n",
    "allLABEL.vocab.stoi['2']=2\n",
    "allLABEL.vocab.stoi['3']=3\n",
    "allLABEL.vocab.stoi['4']=4\n",
    "allLABEL.vocab.stoi['0']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_Beautyvocab = sorted(BeautyTEXT.vocab.freqs.items(), key=operator.itemgetter(1),reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common1 = set.intersection(set(BeautyTEXT.vocab.itos),set(ApparelTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ApparelTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BeautyTEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common2 = set.intersection(set(JewelryTEXT.vocab.itos),set(ShoesTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11061"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11420"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(JewelryTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11419"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ShoesTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport json\\nwith open('Apparel300_vocab','w') as f:\\n    json.dump(ApparelTEXT.vocab.stoi,f)\\n    \\nwith open('Beauty300_vocab','w') as f:\\n    json.dump(BeautyTEXT.vocab.stoi,f)\\n    \\nwith open('Jewelry300_vocab','w') as f:\\n    json.dump(JewelryTEXT.vocab.stoi,f)\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "with open('Apparel300_vocab','w') as f:\n",
    "    json.dump(ApparelTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Beauty300_vocab','w') as f:\n",
    "    json.dump(BeautyTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Jewelry300_vocab','w') as f:\n",
    "    json.dump(JewelryTEXT.vocab.stoi,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "Beautytrain, Beautyvalid = Beautytrain.split(split_ratio=0.8)\n",
    "Beautytrain_iterator, Beautyvalid_iterator = data.BucketIterator.splits(\n",
    "    (Beautytrain, Beautyvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Appareltrain, Apparelvalid = Appareltrain.split(split_ratio=0.8)\n",
    "Appareltrain_iterator, Apparelvalid_iterator = data.BucketIterator.splits(\n",
    "    (Appareltrain, Apparelvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltrain, allvalid = alltrain.split(split_ratio=0.8)\n",
    "alltrain_iterator, allvalid_iterator = data.BucketIterator.splits(\n",
    "    (alltrain, allvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_iterator = data.BucketIterator.splits(\\n    train, \\n    batch_size=BATCH_SIZE, \\n    sort_key=lambda x: len(x.Text), \\n    repeat=False)\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jewelrytrain, Jewelryvalid = Jewelrytrain.split(split_ratio=0.8)\n",
    "Jewelrytrain_iterator, Jewelryvalid_iterator = data.BucketIterator.splits(\n",
    "    (Jewelrytrain, Jewelryvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Shoestrain, Shoesvalid = Shoestrain.split(split_ratio=0.8)\n",
    "Shoestrain_iterator, Shoesvalid_iterator = data.BucketIterator.splits(\n",
    "    (Shoestrain, Shoesvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''\n",
    "train_iterator = data.BucketIterator.splits(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #print(\"embedded shape: \", embedded.shape)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #print(\"output.shape: \",output.shape)\n",
    "        #print(\"output[-1].shape: \",output[-1].shape)\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        #print(\"cell.shape: \",cell.shape)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        \n",
    "        y = self.fc(hidden.squeeze(0))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        #return self.fc(hidden.squeeze(0))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(11415, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "BeautyINPUT_DIM = len(BeautyTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "Beautymodel = RNN(BeautyINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Beautymodel parameters: \")\n",
    "print(Beautymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = BeautyTEXT.vocab.vectors\n",
    "\n",
    "Beautymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Beautyoptimizer = optim.Adam(Beautymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparelmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(11393, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ApparelINPUT_DIM = len(ApparelTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "Apparelmodel = RNN(ApparelINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Apparelmodel parameters: \")\n",
    "print(Apparelmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ApparelTEXT.vocab.vectors\n",
    "\n",
    "Apparelmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Appareloptimizer = optim.Adam(Apparelmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jewelrymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(11420, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "JewelryINPUT_DIM = len(JewelryTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "Jewelrymodel = RNN(JewelryINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Jewelrymodel parameters: \")\n",
    "print(Jewelrymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = JewelryTEXT.vocab.vectors\n",
    "\n",
    "Jewelrymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Jewelryoptimizer = optim.Adam(Jewelrymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(11419, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ShoesINPUT_DIM = len(ShoesTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "Shoesmodel = RNN(ShoesINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(Shoesmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ShoesTEXT.vocab.vectors\n",
    "\n",
    "Shoesmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Shoesoptimizer = optim.Adam(Shoesmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(11778, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "allINPUT_DIM = len(allTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "allmodel = RNN(allINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(allmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = allTEXT.vocab.vectors\n",
    "\n",
    "allmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "alloptimizer = optim.Adam(allmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "allmodel = allmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(preds,y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # turns on dropout and batch normalization and allow gradient update\n",
    "    \n",
    "    i=0\n",
    "    for batch in iterator:\n",
    "        i=i+1\n",
    "        \n",
    "        optimizer.zero_grad() # set accumulated gradient to 0 for every start of a batch\n",
    "        \n",
    "        predictions = model(batch.Text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Label)\n",
    "        \n",
    "        acc = accuracy(predictions, batch.Label)\n",
    "        \n",
    "        loss.backward() # calculate gradient\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"train batch loss: \", loss.item())\n",
    "            print(\"train accuracy: \", acc.item())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #turns off dropout and batch normalization\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for batch in iterator:\n",
    "            i=i+1\n",
    "            predictions = model(batch.Text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i%200 ==0:\n",
    "                print(\"eval batch loss: \", loss.item())\n",
    "                print(\"eval accuracy: \", acc.item())\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "#model = torch.load('fmodel')\n",
    "\n",
    "import timeit\n",
    "#start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.7599146366119385\n",
      "train accuracy:  0.46875\n",
      "train batch loss:  0.5592235326766968\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.399861216545105\n",
      "train accuracy:  0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.745, Train Acc: 52.43%, Val. Loss: 1.608, Val. Acc: 22.12%\n",
      "time duration:     7.029195936396718\n",
      "train batch loss:  0.47750961780548096\n",
      "train accuracy:  0.484375\n",
      "train batch loss:  0.4743742048740387\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.48272982239723206\n",
      "train accuracy:  0.59375\n",
      "Epoch: 02, Train Loss: 0.464, Train Acc: 59.97%, Val. Loss: 1.387, Val. Acc: 26.66%\n",
      "time duration:     7.0530944764614105\n",
      "train batch loss:  0.49393150210380554\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.26019906997680664\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.4359808564186096\n",
      "train accuracy:  0.609375\n",
      "Epoch: 03, Train Loss: 0.409, Train Acc: 62.63%, Val. Loss: 1.256, Val. Acc: 29.96%\n",
      "time duration:     6.9002900663763285\n",
      "train batch loss:  0.40850919485092163\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.27293825149536133\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.3005824685096741\n",
      "train accuracy:  0.734375\n",
      "Epoch: 04, Train Loss: 0.364, Train Acc: 64.68%, Val. Loss: 1.262, Val. Acc: 30.42%\n",
      "time duration:     6.906662514433265\n",
      "train batch loss:  0.39832788705825806\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.2786208987236023\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.384622722864151\n",
      "train accuracy:  0.640625\n",
      "Epoch: 05, Train Loss: 0.336, Train Acc: 66.01%, Val. Loss: 1.170, Val. Acc: 32.59%\n",
      "time duration:     6.869711764156818\n",
      "train batch loss:  0.28039005398750305\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4308304786682129\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.28744444251060486\n",
      "train accuracy:  0.671875\n",
      "Epoch: 06, Train Loss: 0.315, Train Acc: 67.45%, Val. Loss: 1.150, Val. Acc: 33.59%\n",
      "time duration:     7.005395084619522\n",
      "train batch loss:  0.2585982382297516\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.3590792417526245\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.19771435856819153\n",
      "train accuracy:  0.796875\n",
      "Epoch: 07, Train Loss: 0.303, Train Acc: 68.10%, Val. Loss: 0.844, Val. Acc: 40.61%\n",
      "time duration:     6.90680249966681\n",
      "train batch loss:  0.30049067735671997\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.3614524304866791\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.20578508079051971\n",
      "train accuracy:  0.78125\n",
      "Epoch: 08, Train Loss: 0.279, Train Acc: 69.57%, Val. Loss: 0.913, Val. Acc: 38.60%\n",
      "time duration:     6.985635790973902\n",
      "train batch loss:  0.2343437671661377\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.17197978496551514\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.25863850116729736\n",
      "train accuracy:  0.640625\n",
      "Epoch: 09, Train Loss: 0.267, Train Acc: 70.74%, Val. Loss: 0.943, Val. Acc: 37.21%\n",
      "time duration:     6.914595266804099\n",
      "train batch loss:  0.3096565008163452\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23544469475746155\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.32217472791671753\n",
      "train accuracy:  0.609375\n",
      "Epoch: 10, Train Loss: 0.254, Train Acc: 71.68%, Val. Loss: 0.912, Val. Acc: 38.76%\n",
      "time duration:     6.9226542841643095\n",
      "train batch loss:  0.33388185501098633\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.22435764968395233\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.25351792573928833\n",
      "train accuracy:  0.671875\n",
      "Epoch: 11, Train Loss: 0.243, Train Acc: 72.17%, Val. Loss: 0.948, Val. Acc: 37.27%\n",
      "time duration:     6.869219020009041\n",
      "train batch loss:  0.21247515082359314\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22498783469200134\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.23105810582637787\n",
      "train accuracy:  0.765625\n",
      "Epoch: 12, Train Loss: 0.228, Train Acc: 73.84%, Val. Loss: 0.921, Val. Acc: 38.92%\n",
      "time duration:     6.938972573727369\n",
      "train batch loss:  0.2482638955116272\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.11371967196464539\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.21234774589538574\n",
      "train accuracy:  0.75\n",
      "Epoch: 13, Train Loss: 0.219, Train Acc: 74.92%, Val. Loss: 0.918, Val. Acc: 39.73%\n",
      "time duration:     6.853662747889757\n",
      "train batch loss:  0.2698777914047241\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2174065113067627\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.20022061467170715\n",
      "train accuracy:  0.78125\n",
      "Epoch: 14, Train Loss: 0.215, Train Acc: 75.52%, Val. Loss: 0.913, Val. Acc: 39.24%\n",
      "time duration:     6.9771284237504005\n",
      "train batch loss:  0.20767904818058014\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.16973929107189178\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.22097474336624146\n",
      "train accuracy:  0.71875\n",
      "Epoch: 15, Train Loss: 0.205, Train Acc: 76.31%, Val. Loss: 0.874, Val. Acc: 40.88%\n",
      "time duration:     6.8368097599595785\n",
      "train batch loss:  0.2801325023174286\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.133230522274971\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.20457497239112854\n",
      "train accuracy:  0.796875\n",
      "Epoch: 16, Train Loss: 0.197, Train Acc: 77.03%, Val. Loss: 0.882, Val. Acc: 40.77%\n",
      "time duration:     6.984194988384843\n",
      "train batch loss:  0.1738964319229126\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.21209922432899475\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.20948371291160583\n",
      "train accuracy:  0.734375\n",
      "Epoch: 17, Train Loss: 0.191, Train Acc: 77.58%, Val. Loss: 0.885, Val. Acc: 40.68%\n",
      "time duration:     6.93242441304028\n",
      "train batch loss:  0.22345003485679626\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.18167924880981445\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.16389615833759308\n",
      "train accuracy:  0.796875\n",
      "Epoch: 18, Train Loss: 0.187, Train Acc: 78.16%, Val. Loss: 0.860, Val. Acc: 41.97%\n",
      "time duration:     6.966988295316696\n",
      "train batch loss:  0.15194982290267944\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.19297882914543152\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.24677960574626923\n",
      "train accuracy:  0.671875\n",
      "Epoch: 19, Train Loss: 0.177, Train Acc: 79.22%, Val. Loss: 0.847, Val. Acc: 42.53%\n",
      "time duration:     6.857882108539343\n",
      "train batch loss:  0.11534081399440765\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16654706001281738\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2364940047264099\n",
      "train accuracy:  0.765625\n",
      "Epoch: 20, Train Loss: 0.176, Train Acc: 79.10%, Val. Loss: 0.806, Val. Acc: 43.92%\n",
      "time duration:     6.933567376807332\n",
      "train batch loss:  0.2484283149242401\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.18463408946990967\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.21645033359527588\n",
      "train accuracy:  0.71875\n",
      "Epoch: 21, Train Loss: 0.166, Train Acc: 80.38%, Val. Loss: 0.810, Val. Acc: 43.54%\n",
      "time duration:     7.145254086703062\n",
      "train batch loss:  0.17345210909843445\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1696915477514267\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.11903845518827438\n",
      "train accuracy:  0.875\n",
      "Epoch: 22, Train Loss: 0.161, Train Acc: 80.82%, Val. Loss: 0.786, Val. Acc: 44.60%\n",
      "time duration:     6.982928292825818\n",
      "train batch loss:  0.10844019055366516\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1751190423965454\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1461806446313858\n",
      "train accuracy:  0.84375\n",
      "Epoch: 23, Train Loss: 0.159, Train Acc: 81.13%, Val. Loss: 0.800, Val. Acc: 44.20%\n",
      "time duration:     6.9734047669917345\n",
      "train batch loss:  0.14990586042404175\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17829778790473938\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22064968943595886\n",
      "train accuracy:  0.765625\n",
      "Epoch: 24, Train Loss: 0.151, Train Acc: 81.67%, Val. Loss: 0.779, Val. Acc: 45.82%\n",
      "time duration:     7.011264180764556\n",
      "train batch loss:  0.10619989037513733\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.2308959811925888\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1426430344581604\n",
      "train accuracy:  0.796875\n",
      "Epoch: 25, Train Loss: 0.150, Train Acc: 82.08%, Val. Loss: 0.772, Val. Acc: 46.61%\n",
      "time duration:     6.960920188575983\n",
      "train batch loss:  0.12169903516769409\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.22702109813690186\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.16464494168758392\n",
      "train accuracy:  0.796875\n",
      "Epoch: 26, Train Loss: 0.145, Train Acc: 82.66%, Val. Loss: 0.755, Val. Acc: 46.66%\n",
      "time duration:     7.1003147810697556\n",
      "train batch loss:  0.11443804949522018\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1825655847787857\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.14246217906475067\n",
      "train accuracy:  0.828125\n",
      "Epoch: 27, Train Loss: 0.140, Train Acc: 83.15%, Val. Loss: 0.775, Val. Acc: 46.85%\n",
      "time duration:     6.977094555273652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.11649331450462341\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18813759088516235\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.13505814969539642\n",
      "train accuracy:  0.859375\n",
      "Epoch: 28, Train Loss: 0.136, Train Acc: 83.63%, Val. Loss: 0.765, Val. Acc: 47.87%\n",
      "time duration:     7.017737442627549\n",
      "train batch loss:  0.17671096324920654\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1406605839729309\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13908788561820984\n",
      "train accuracy:  0.828125\n",
      "Epoch: 29, Train Loss: 0.134, Train Acc: 84.18%, Val. Loss: 0.783, Val. Acc: 47.35%\n",
      "time duration:     7.183441335335374\n",
      "train batch loss:  0.1367833912372589\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.153590589761734\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1259109228849411\n",
      "train accuracy:  0.875\n",
      "Epoch: 30, Train Loss: 0.133, Train Acc: 84.03%, Val. Loss: 0.624, Val. Acc: 51.35%\n",
      "time duration:     7.083108237013221\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Beautymodel, Beautytrain_iterator, Beautyoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Beautymodel, Beautyvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6224201917648315\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.802098274230957\n",
      "train accuracy:  0.453125\n",
      "train batch loss:  0.5952103137969971\n",
      "train accuracy:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.746, Train Acc: 52.95%, Val. Loss: 1.565, Val. Acc: 23.18%\n",
      "time duration:     6.969217656180263\n",
      "train batch loss:  0.4364822804927826\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.38269394636154175\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.4192928373813629\n",
      "train accuracy:  0.609375\n",
      "Epoch: 02, Train Loss: 0.468, Train Acc: 60.34%, Val. Loss: 1.353, Val. Acc: 27.76%\n",
      "time duration:     7.045009545981884\n",
      "train batch loss:  0.38159358501434326\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.3127586245536804\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.40788185596466064\n",
      "train accuracy:  0.640625\n",
      "Epoch: 03, Train Loss: 0.404, Train Acc: 62.85%, Val. Loss: 1.375, Val. Acc: 26.98%\n",
      "time duration:     7.060797564685345\n",
      "train batch loss:  0.2884298264980316\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.39535561203956604\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3794962763786316\n",
      "train accuracy:  0.59375\n",
      "Epoch: 04, Train Loss: 0.367, Train Acc: 64.84%, Val. Loss: 1.285, Val. Acc: 28.78%\n",
      "time duration:     7.169064961373806\n",
      "train batch loss:  0.5038593411445618\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.28045904636383057\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.23415371775627136\n",
      "train accuracy:  0.734375\n",
      "Epoch: 05, Train Loss: 0.332, Train Acc: 66.78%, Val. Loss: 1.169, Val. Acc: 32.11%\n",
      "time duration:     7.036950599402189\n",
      "train batch loss:  0.282617449760437\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.23762726783752441\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3225122392177582\n",
      "train accuracy:  0.6875\n",
      "Epoch: 06, Train Loss: 0.312, Train Acc: 68.02%, Val. Loss: 1.169, Val. Acc: 32.62%\n",
      "time duration:     6.9603667836636305\n",
      "train batch loss:  0.23982024192810059\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.26323723793029785\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3960646390914917\n",
      "train accuracy:  0.6875\n",
      "Epoch: 07, Train Loss: 0.294, Train Acc: 69.11%, Val. Loss: 1.160, Val. Acc: 33.05%\n",
      "time duration:     7.067764591425657\n",
      "train batch loss:  0.1854654997587204\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.27791503071784973\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.16130441427230835\n",
      "train accuracy:  0.75\n",
      "Epoch: 08, Train Loss: 0.275, Train Acc: 70.37%, Val. Loss: 1.136, Val. Acc: 33.51%\n",
      "time duration:     6.941379809752107\n",
      "train batch loss:  0.16059011220932007\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.22132468223571777\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2607603371143341\n",
      "train accuracy:  0.671875\n",
      "Epoch: 09, Train Loss: 0.266, Train Acc: 70.97%, Val. Loss: 0.917, Val. Acc: 38.77%\n",
      "time duration:     6.95358569547534\n",
      "train batch loss:  0.16359351575374603\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26533153653144836\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2745116949081421\n",
      "train accuracy:  0.6875\n",
      "Epoch: 10, Train Loss: 0.249, Train Acc: 72.60%, Val. Loss: 0.969, Val. Acc: 37.18%\n",
      "time duration:     6.925610478967428\n",
      "train batch loss:  0.18533705174922943\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26420068740844727\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2786579430103302\n",
      "train accuracy:  0.65625\n",
      "Epoch: 11, Train Loss: 0.239, Train Acc: 73.12%, Val. Loss: 0.985, Val. Acc: 36.80%\n",
      "time duration:     7.024179022759199\n",
      "train batch loss:  0.1931881308555603\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18775230646133423\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.30244043469429016\n",
      "train accuracy:  0.671875\n",
      "Epoch: 12, Train Loss: 0.224, Train Acc: 74.57%, Val. Loss: 0.909, Val. Acc: 39.06%\n",
      "time duration:     7.191507386043668\n",
      "train batch loss:  0.2596341371536255\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2955419421195984\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17963498830795288\n",
      "train accuracy:  0.765625\n",
      "Epoch: 13, Train Loss: 0.216, Train Acc: 75.22%, Val. Loss: 0.873, Val. Acc: 40.33%\n",
      "time duration:     7.206931225955486\n",
      "train batch loss:  0.19869181513786316\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.23800987005233765\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24555666744709015\n",
      "train accuracy:  0.796875\n",
      "Epoch: 14, Train Loss: 0.207, Train Acc: 76.35%, Val. Loss: 0.906, Val. Acc: 39.44%\n",
      "time duration:     7.042493686079979\n",
      "train batch loss:  0.3045065104961395\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.19244670867919922\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13386216759681702\n",
      "train accuracy:  0.828125\n",
      "Epoch: 15, Train Loss: 0.201, Train Acc: 76.95%, Val. Loss: 0.895, Val. Acc: 40.56%\n",
      "time duration:     7.157511921599507\n",
      "train batch loss:  0.1938907653093338\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17468127608299255\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.13774023950099945\n",
      "train accuracy:  0.84375\n",
      "Epoch: 16, Train Loss: 0.191, Train Acc: 77.62%, Val. Loss: 0.857, Val. Acc: 41.54%\n",
      "time duration:     7.003008171916008\n",
      "train batch loss:  0.21545109152793884\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.12677517533302307\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.23793745040893555\n",
      "train accuracy:  0.703125\n",
      "Epoch: 17, Train Loss: 0.185, Train Acc: 78.26%, Val. Loss: 0.904, Val. Acc: 40.25%\n",
      "time duration:     6.907631943002343\n",
      "train batch loss:  0.21036043763160706\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16355863213539124\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20978696644306183\n",
      "train accuracy:  0.765625\n",
      "Epoch: 18, Train Loss: 0.180, Train Acc: 78.89%, Val. Loss: 0.588, Val. Acc: 51.72%\n",
      "time duration:     7.087857479229569\n",
      "train batch loss:  0.1773347705602646\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2018263339996338\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.195256307721138\n",
      "train accuracy:  0.796875\n",
      "Epoch: 19, Train Loss: 0.174, Train Acc: 79.57%, Val. Loss: 0.605, Val. Acc: 50.95%\n",
      "time duration:     7.0304371025413275\n",
      "train batch loss:  0.19687888026237488\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2086147964000702\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17980223894119263\n",
      "train accuracy:  0.78125\n",
      "Epoch: 20, Train Loss: 0.166, Train Acc: 80.05%, Val. Loss: 0.601, Val. Acc: 50.58%\n",
      "time duration:     7.0354191437363625\n",
      "train batch loss:  0.12306287884712219\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1353015899658203\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13985112309455872\n",
      "train accuracy:  0.859375\n",
      "Epoch: 21, Train Loss: 0.161, Train Acc: 80.74%, Val. Loss: 0.620, Val. Acc: 49.45%\n",
      "time duration:     7.006889773532748\n",
      "train batch loss:  0.08765989542007446\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.12879197299480438\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.12831994891166687\n",
      "train accuracy:  0.859375\n",
      "Epoch: 22, Train Loss: 0.158, Train Acc: 81.26%, Val. Loss: 0.625, Val. Acc: 50.20%\n",
      "time duration:     7.01834668032825\n",
      "train batch loss:  0.22407753765583038\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.15187054872512817\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16003474593162537\n",
      "train accuracy:  0.84375\n",
      "Epoch: 23, Train Loss: 0.151, Train Acc: 81.83%, Val. Loss: 0.639, Val. Acc: 49.02%\n",
      "time duration:     6.868284560739994\n",
      "train batch loss:  0.09484873712062836\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1574854552745819\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.11123593151569366\n",
      "train accuracy:  0.859375\n",
      "Epoch: 24, Train Loss: 0.148, Train Acc: 82.33%, Val. Loss: 0.640, Val. Acc: 48.72%\n",
      "time duration:     7.094926016405225\n",
      "train batch loss:  0.11520734429359436\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15243496000766754\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1675816774368286\n",
      "train accuracy:  0.859375\n",
      "Epoch: 25, Train Loss: 0.144, Train Acc: 82.65%, Val. Loss: 0.616, Val. Acc: 50.67%\n",
      "time duration:     7.080058820545673\n",
      "train batch loss:  0.11273486167192459\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.16287440061569214\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1554722785949707\n",
      "train accuracy:  0.796875\n",
      "Epoch: 26, Train Loss: 0.139, Train Acc: 83.37%, Val. Loss: 0.664, Val. Acc: 48.27%\n",
      "time duration:     7.009847281500697\n",
      "train batch loss:  0.12465989589691162\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.17125752568244934\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.13455505669116974\n",
      "train accuracy:  0.84375\n",
      "Epoch: 27, Train Loss: 0.139, Train Acc: 83.26%, Val. Loss: 0.642, Val. Acc: 49.42%\n",
      "time duration:     7.190821401774883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.14477793872356415\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1033790111541748\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12290714681148529\n",
      "train accuracy:  0.84375\n",
      "Epoch: 28, Train Loss: 0.131, Train Acc: 84.30%, Val. Loss: 0.657, Val. Acc: 48.53%\n",
      "time duration:     6.913124153390527\n",
      "train batch loss:  0.06283252686262131\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07880149781703949\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.16976456344127655\n",
      "train accuracy:  0.8125\n",
      "Epoch: 29, Train Loss: 0.127, Train Acc: 84.81%, Val. Loss: 0.638, Val. Acc: 49.91%\n",
      "time duration:     6.996437193825841\n",
      "train batch loss:  0.10650628805160522\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11790578067302704\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12161348015069962\n",
      "train accuracy:  0.890625\n",
      "Epoch: 30, Train Loss: 0.124, Train Acc: 85.47%, Val. Loss: 0.654, Val. Acc: 49.43%\n",
      "time duration:     7.198407106101513\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Apparelmodel, Appareltrain_iterator, Appareloptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Apparelmodel, Apparelvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6301746368408203\n",
      "train accuracy:  0.546875\n",
      "train batch loss:  0.5328851342201233\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5159908533096313\n",
      "train accuracy:  0.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.736, Train Acc: 52.70%, Val. Loss: 1.471, Val. Acc: 24.63%\n",
      "time duration:     7.055991502478719\n",
      "train batch loss:  0.3635804057121277\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.5165541172027588\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.4380553662776947\n",
      "train accuracy:  0.65625\n",
      "Epoch: 02, Train Loss: 0.476, Train Acc: 59.88%, Val. Loss: 1.361, Val. Acc: 26.13%\n",
      "time duration:     6.959439035505056\n",
      "train batch loss:  0.4171964228153229\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.46422961354255676\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.37208107113838196\n",
      "train accuracy:  0.578125\n",
      "Epoch: 03, Train Loss: 0.412, Train Acc: 62.86%, Val. Loss: 1.288, Val. Acc: 28.27%\n",
      "time duration:     7.042623616755009\n",
      "train batch loss:  0.3214538097381592\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.2609157860279083\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6514191031455994\n",
      "train accuracy:  0.59375\n",
      "Epoch: 04, Train Loss: 0.370, Train Acc: 64.37%, Val. Loss: 1.208, Val. Acc: 30.76%\n",
      "time duration:     7.079696686938405\n",
      "train batch loss:  0.3982483148574829\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.24290479719638824\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.3700544238090515\n",
      "train accuracy:  0.59375\n",
      "Epoch: 05, Train Loss: 0.338, Train Acc: 66.58%, Val. Loss: 1.147, Val. Acc: 33.19%\n",
      "time duration:     6.929377222433686\n",
      "train batch loss:  0.23791006207466125\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2953246235847473\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.46553388237953186\n",
      "train accuracy:  0.625\n",
      "Epoch: 06, Train Loss: 0.320, Train Acc: 67.25%, Val. Loss: 1.146, Val. Acc: 33.59%\n",
      "time duration:     7.155843965709209\n",
      "train batch loss:  0.3510676920413971\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.22722238302230835\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2581310272216797\n",
      "train accuracy:  0.734375\n",
      "Epoch: 07, Train Loss: 0.294, Train Acc: 68.55%, Val. Loss: 1.061, Val. Acc: 36.11%\n",
      "time duration:     7.09697887301445\n",
      "train batch loss:  0.3038342595100403\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.20107883214950562\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1628284752368927\n",
      "train accuracy:  0.78125\n",
      "Epoch: 08, Train Loss: 0.279, Train Acc: 69.96%, Val. Loss: 0.812, Val. Acc: 42.24%\n",
      "time duration:     7.1933942418545485\n",
      "train batch loss:  0.29842162132263184\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.21585974097251892\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.3215491473674774\n",
      "train accuracy:  0.6875\n",
      "Epoch: 09, Train Loss: 0.268, Train Acc: 70.67%, Val. Loss: 0.821, Val. Acc: 41.37%\n",
      "time duration:     7.009547187015414\n",
      "train batch loss:  0.21554060280323029\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3232094943523407\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.279538631439209\n",
      "train accuracy:  0.703125\n",
      "Epoch: 10, Train Loss: 0.253, Train Acc: 72.03%, Val. Loss: 0.793, Val. Acc: 43.33%\n",
      "time duration:     7.065599853172898\n",
      "train batch loss:  0.21874433755874634\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24026259779930115\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2226664274930954\n",
      "train accuracy:  0.796875\n",
      "Epoch: 11, Train Loss: 0.240, Train Acc: 72.94%, Val. Loss: 0.806, Val. Acc: 43.35%\n",
      "time duration:     7.05329979211092\n",
      "train batch loss:  0.1911318302154541\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.17112916707992554\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16372573375701904\n",
      "train accuracy:  0.828125\n",
      "Epoch: 12, Train Loss: 0.230, Train Acc: 74.05%, Val. Loss: 0.805, Val. Acc: 43.12%\n",
      "time duration:     6.944287242367864\n",
      "train batch loss:  0.1913425326347351\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2759484052658081\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.31089532375335693\n",
      "train accuracy:  0.703125\n",
      "Epoch: 13, Train Loss: 0.224, Train Acc: 74.35%, Val. Loss: 0.811, Val. Acc: 42.17%\n",
      "time duration:     6.941960355266929\n",
      "train batch loss:  0.22646404802799225\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2850068211555481\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.22233939170837402\n",
      "train accuracy:  0.734375\n",
      "Epoch: 14, Train Loss: 0.218, Train Acc: 75.16%, Val. Loss: 0.787, Val. Acc: 43.67%\n",
      "time duration:     7.089880490675569\n",
      "train batch loss:  0.16704310476779938\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15088731050491333\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.17034661769866943\n",
      "train accuracy:  0.78125\n",
      "Epoch: 15, Train Loss: 0.203, Train Acc: 76.23%, Val. Loss: 0.788, Val. Acc: 43.50%\n",
      "time duration:     7.120376879349351\n",
      "train batch loss:  0.17691364884376526\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.33264005184173584\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.12714746594429016\n",
      "train accuracy:  0.828125\n",
      "Epoch: 16, Train Loss: 0.200, Train Acc: 76.60%, Val. Loss: 0.734, Val. Acc: 46.09%\n",
      "time duration:     7.0812922567129135\n",
      "train batch loss:  0.17115893959999084\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15934231877326965\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.22250281274318695\n",
      "train accuracy:  0.71875\n",
      "Epoch: 17, Train Loss: 0.192, Train Acc: 77.75%, Val. Loss: 0.766, Val. Acc: 44.85%\n",
      "time duration:     7.071014024317265\n",
      "train batch loss:  0.16293008625507355\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2380652129650116\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13764357566833496\n",
      "train accuracy:  0.828125\n",
      "Epoch: 18, Train Loss: 0.186, Train Acc: 78.18%, Val. Loss: 0.766, Val. Acc: 44.97%\n",
      "time duration:     7.102266199886799\n",
      "train batch loss:  0.19868098199367523\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.13011570274829865\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1979202926158905\n",
      "train accuracy:  0.8125\n",
      "Epoch: 19, Train Loss: 0.179, Train Acc: 78.58%, Val. Loss: 0.744, Val. Acc: 45.48%\n",
      "time duration:     7.0006515718996525\n",
      "train batch loss:  0.12788036465644836\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13932660222053528\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20405030250549316\n",
      "train accuracy:  0.765625\n",
      "Epoch: 20, Train Loss: 0.173, Train Acc: 79.51%, Val. Loss: 0.750, Val. Acc: 45.58%\n",
      "time duration:     7.0070111360400915\n",
      "train batch loss:  0.2138698548078537\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18553224205970764\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.14288330078125\n",
      "train accuracy:  0.890625\n",
      "Epoch: 21, Train Loss: 0.166, Train Acc: 80.40%, Val. Loss: 0.770, Val. Acc: 44.89%\n",
      "time duration:     7.118676407262683\n",
      "train batch loss:  0.18572841584682465\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1548662632703781\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10996104031801224\n",
      "train accuracy:  0.875\n",
      "Epoch: 22, Train Loss: 0.162, Train Acc: 80.55%, Val. Loss: 0.738, Val. Acc: 46.97%\n",
      "time duration:     7.02189483307302\n",
      "train batch loss:  0.21218469738960266\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.138429194688797\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17937515676021576\n",
      "train accuracy:  0.828125\n",
      "Epoch: 23, Train Loss: 0.158, Train Acc: 81.33%, Val. Loss: 0.742, Val. Acc: 46.85%\n",
      "time duration:     6.954452730715275\n",
      "train batch loss:  0.1506185233592987\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1497245728969574\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10552780330181122\n",
      "train accuracy:  0.890625\n",
      "Epoch: 24, Train Loss: 0.153, Train Acc: 81.77%, Val. Loss: 0.724, Val. Acc: 47.18%\n",
      "time duration:     7.162176324054599\n",
      "train batch loss:  0.14145052433013916\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.10211221128702164\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14551305770874023\n",
      "train accuracy:  0.78125\n",
      "Epoch: 25, Train Loss: 0.147, Train Acc: 82.32%, Val. Loss: 0.745, Val. Acc: 46.81%\n",
      "time duration:     6.996334947645664\n",
      "train batch loss:  0.13512006402015686\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10571801662445068\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13385792076587677\n",
      "train accuracy:  0.828125\n",
      "Epoch: 26, Train Loss: 0.145, Train Acc: 82.64%, Val. Loss: 0.745, Val. Acc: 47.16%\n",
      "time duration:     7.1527389623224735\n",
      "train batch loss:  0.12898102402687073\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12428285926580429\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.22422535717487335\n",
      "train accuracy:  0.71875\n",
      "Epoch: 27, Train Loss: 0.139, Train Acc: 83.17%, Val. Loss: 0.758, Val. Acc: 46.45%\n",
      "time duration:     7.039394907653332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.18968465924263\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1133405864238739\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1250794678926468\n",
      "train accuracy:  0.8125\n",
      "Epoch: 28, Train Loss: 0.137, Train Acc: 83.62%, Val. Loss: 0.754, Val. Acc: 46.52%\n",
      "time duration:     7.172634623944759\n",
      "train batch loss:  0.1180628165602684\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.07846075296401978\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.15739606320858002\n",
      "train accuracy:  0.78125\n",
      "Epoch: 29, Train Loss: 0.134, Train Acc: 83.96%, Val. Loss: 0.709, Val. Acc: 49.24%\n",
      "time duration:     7.107526823878288\n",
      "train batch loss:  0.08139052242040634\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.17068910598754883\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17654576897621155\n",
      "train accuracy:  0.796875\n",
      "Epoch: 30, Train Loss: 0.130, Train Acc: 84.48%, Val. Loss: 0.720, Val. Acc: 47.65%\n",
      "time duration:     7.125024737790227\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Jewelrymodel, Jewelrytrain_iterator, Jewelryoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Jewelrymodel, Jewelryvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.5651078224182129\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.6628104448318481\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.48524707555770874\n",
      "train accuracy:  0.703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.733, Train Acc: 52.95%, Val. Loss: 1.553, Val. Acc: 23.91%\n",
      "time duration:     7.042408537119627\n",
      "train batch loss:  0.3780459761619568\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.6475535035133362\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.6203875541687012\n",
      "train accuracy:  0.5625\n",
      "Epoch: 02, Train Loss: 0.461, Train Acc: 60.32%, Val. Loss: 1.382, Val. Acc: 26.90%\n",
      "time duration:     7.049519123509526\n",
      "train batch loss:  0.6357086896896362\n",
      "train accuracy:  0.546875\n",
      "train batch loss:  0.3618670701980591\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.44358542561531067\n",
      "train accuracy:  0.546875\n",
      "Epoch: 03, Train Loss: 0.407, Train Acc: 63.02%, Val. Loss: 1.422, Val. Acc: 25.61%\n",
      "time duration:     7.102396998554468\n",
      "train batch loss:  0.22764535248279572\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.40884461998939514\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.36230742931365967\n",
      "train accuracy:  0.65625\n",
      "Epoch: 04, Train Loss: 0.362, Train Acc: 64.98%, Val. Loss: 1.139, Val. Acc: 33.51%\n",
      "time duration:     6.959543919190764\n",
      "train batch loss:  0.37136924266815186\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3565632104873657\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.30291175842285156\n",
      "train accuracy:  0.65625\n",
      "Epoch: 05, Train Loss: 0.338, Train Acc: 66.26%, Val. Loss: 1.349, Val. Acc: 26.67%\n",
      "time duration:     6.973251538351178\n",
      "train batch loss:  0.2188396453857422\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3015994429588318\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.36997899413108826\n",
      "train accuracy:  0.625\n",
      "Epoch: 06, Train Loss: 0.311, Train Acc: 68.08%, Val. Loss: 1.235, Val. Acc: 29.80%\n",
      "time duration:     6.897506777197123\n",
      "train batch loss:  0.19611819088459015\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1667097806930542\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.29562005400657654\n",
      "train accuracy:  0.609375\n",
      "Epoch: 07, Train Loss: 0.289, Train Acc: 69.17%, Val. Loss: 1.144, Val. Acc: 33.01%\n",
      "time duration:     7.101320987567306\n",
      "train batch loss:  0.24811673164367676\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.25623685121536255\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2181093394756317\n",
      "train accuracy:  0.765625\n",
      "Epoch: 08, Train Loss: 0.276, Train Acc: 70.09%, Val. Loss: 1.130, Val. Acc: 32.85%\n",
      "time duration:     6.8337820414453745\n",
      "train batch loss:  0.3596344590187073\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24935486912727356\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.39382246136665344\n",
      "train accuracy:  0.625\n",
      "Epoch: 09, Train Loss: 0.261, Train Acc: 71.22%, Val. Loss: 1.075, Val. Acc: 35.05%\n",
      "time duration:     7.097547007724643\n",
      "train batch loss:  0.23953455686569214\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2777099609375\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.25739726424217224\n",
      "train accuracy:  0.609375\n",
      "Epoch: 10, Train Loss: 0.253, Train Acc: 72.11%, Val. Loss: 1.039, Val. Acc: 35.54%\n",
      "time duration:     7.091578612104058\n",
      "train batch loss:  0.20661407709121704\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11991797387599945\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.23372521996498108\n",
      "train accuracy:  0.71875\n",
      "Epoch: 11, Train Loss: 0.234, Train Acc: 73.34%, Val. Loss: 0.932, Val. Acc: 39.45%\n",
      "time duration:     6.905920669436455\n",
      "train batch loss:  0.29781943559646606\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1373729407787323\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2052001953125\n",
      "train accuracy:  0.796875\n",
      "Epoch: 12, Train Loss: 0.227, Train Acc: 74.07%, Val. Loss: 0.919, Val. Acc: 39.71%\n",
      "time duration:     7.126864040270448\n",
      "train batch loss:  0.15664902329444885\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2833670377731323\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.22242948412895203\n",
      "train accuracy:  0.75\n",
      "Epoch: 13, Train Loss: 0.222, Train Acc: 74.80%, Val. Loss: 1.003, Val. Acc: 37.01%\n",
      "time duration:     7.25712120346725\n",
      "train batch loss:  0.17495301365852356\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.27402862906455994\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.2040557563304901\n",
      "train accuracy:  0.765625\n",
      "Epoch: 14, Train Loss: 0.209, Train Acc: 76.00%, Val. Loss: 0.945, Val. Acc: 38.87%\n",
      "time duration:     7.1392229199409485\n",
      "train batch loss:  0.21799318492412567\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.21742676198482513\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.19546973705291748\n",
      "train accuracy:  0.734375\n",
      "Epoch: 15, Train Loss: 0.201, Train Acc: 76.68%, Val. Loss: 0.985, Val. Acc: 37.60%\n",
      "time duration:     7.0121261309832335\n",
      "train batch loss:  0.2935558557510376\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.1712510734796524\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17687717080116272\n",
      "train accuracy:  0.71875\n",
      "Epoch: 16, Train Loss: 0.196, Train Acc: 76.70%, Val. Loss: 1.014, Val. Acc: 36.95%\n",
      "time duration:     7.073336079716682\n",
      "train batch loss:  0.1491485983133316\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.2609822154045105\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.20788709819316864\n",
      "train accuracy:  0.78125\n",
      "Epoch: 17, Train Loss: 0.190, Train Acc: 77.52%, Val. Loss: 0.798, Val. Acc: 43.84%\n",
      "time duration:     7.029212908819318\n",
      "train batch loss:  0.19422107934951782\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.20514121651649475\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1772075593471527\n",
      "train accuracy:  0.796875\n",
      "Epoch: 18, Train Loss: 0.181, Train Acc: 78.38%, Val. Loss: 0.837, Val. Acc: 43.00%\n",
      "time duration:     7.017511866986752\n",
      "train batch loss:  0.1683724820613861\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15408287942409515\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2168262004852295\n",
      "train accuracy:  0.75\n",
      "Epoch: 19, Train Loss: 0.176, Train Acc: 79.10%, Val. Loss: 0.807, Val. Acc: 43.84%\n",
      "time duration:     7.102894749492407\n",
      "train batch loss:  0.18958741426467896\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.17720291018486023\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.18691065907478333\n",
      "train accuracy:  0.78125\n",
      "Epoch: 20, Train Loss: 0.169, Train Acc: 79.56%, Val. Loss: 0.812, Val. Acc: 44.44%\n",
      "time duration:     6.932460691779852\n",
      "train batch loss:  0.17464089393615723\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13586165010929108\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15376366674900055\n",
      "train accuracy:  0.8125\n",
      "Epoch: 21, Train Loss: 0.163, Train Acc: 80.35%, Val. Loss: 0.813, Val. Acc: 44.24%\n",
      "time duration:     6.9236002042889595\n",
      "train batch loss:  0.14799818396568298\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13868111371994019\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14844268560409546\n",
      "train accuracy:  0.78125\n",
      "Epoch: 22, Train Loss: 0.159, Train Acc: 80.92%, Val. Loss: 0.815, Val. Acc: 43.79%\n",
      "time duration:     7.134209142997861\n",
      "train batch loss:  0.1463882327079773\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10751097649335861\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.19639888405799866\n",
      "train accuracy:  0.84375\n",
      "Epoch: 23, Train Loss: 0.155, Train Acc: 81.52%, Val. Loss: 0.806, Val. Acc: 43.96%\n",
      "time duration:     6.928561242297292\n",
      "train batch loss:  0.11583168804645538\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11904199421405792\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13765646517276764\n",
      "train accuracy:  0.859375\n",
      "Epoch: 24, Train Loss: 0.149, Train Acc: 82.10%, Val. Loss: 0.798, Val. Acc: 44.32%\n",
      "time duration:     6.963853623718023\n",
      "train batch loss:  0.1278616040945053\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16092711687088013\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12761393189430237\n",
      "train accuracy:  0.8125\n",
      "Epoch: 25, Train Loss: 0.144, Train Acc: 82.67%, Val. Loss: 0.775, Val. Acc: 45.38%\n",
      "time duration:     7.046182772144675\n",
      "train batch loss:  0.21210193634033203\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.14411179721355438\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11698581278324127\n",
      "train accuracy:  0.84375\n",
      "Epoch: 26, Train Loss: 0.139, Train Acc: 83.10%, Val. Loss: 0.825, Val. Acc: 44.17%\n",
      "time duration:     6.9500522296875715\n",
      "train batch loss:  0.13994446396827698\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13174736499786377\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14071011543273926\n",
      "train accuracy:  0.828125\n",
      "Epoch: 27, Train Loss: 0.137, Train Acc: 83.67%, Val. Loss: 0.796, Val. Acc: 45.04%\n",
      "time duration:     7.102817162871361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.13759784400463104\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11462140083312988\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09522995352745056\n",
      "train accuracy:  0.890625\n",
      "Epoch: 28, Train Loss: 0.133, Train Acc: 84.27%, Val. Loss: 0.769, Val. Acc: 45.96%\n",
      "time duration:     6.961299763992429\n",
      "train batch loss:  0.0892253965139389\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12651070952415466\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.146209716796875\n",
      "train accuracy:  0.84375\n",
      "Epoch: 29, Train Loss: 0.129, Train Acc: 84.59%, Val. Loss: 0.647, Val. Acc: 49.24%\n",
      "time duration:     6.958750203251839\n",
      "train batch loss:  0.0991404578089714\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11766912788152695\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1149306446313858\n",
      "train accuracy:  0.859375\n",
      "Epoch: 30, Train Loss: 0.128, Train Acc: 84.97%, Val. Loss: 0.658, Val. Acc: 48.76%\n",
      "time duration:     6.884521793574095\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Shoesmodel, Shoestrain_iterator, Shoesoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Shoesmodel, Shoesvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.7896499633789062\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.39122092723846436\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5114561915397644\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.4810025691986084\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.6239391565322876\n",
      "train accuracy:  0.453125\n",
      "train batch loss:  0.4044605493545532\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.43543994426727295\n",
      "train accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.617, Train Acc: 56.06%, Val. Loss: 1.436, Val. Acc: 24.96%\n",
      "time duration:     14.02317874878645\n",
      "train batch loss:  0.47042539715766907\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.39015477895736694\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.32452279329299927\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.5021563172340393\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.3490838408470154\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.39797788858413696\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4024139642715454\n",
      "train accuracy:  0.609375\n",
      "Epoch: 02, Train Loss: 0.419, Train Acc: 62.09%, Val. Loss: 1.227, Val. Acc: 30.16%\n",
      "time duration:     13.823719572275877\n",
      "train batch loss:  0.38308122754096985\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.31786298751831055\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3738259971141815\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.37786468863487244\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.33816957473754883\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2876979112625122\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.23585271835327148\n",
      "train accuracy:  0.71875\n",
      "Epoch: 03, Train Loss: 0.368, Train Acc: 64.62%, Val. Loss: 1.066, Val. Acc: 35.31%\n",
      "time duration:     14.076206812635064\n",
      "train batch loss:  0.3146389424800873\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.3563193380832672\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.3501048982143402\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3744816780090332\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2924884557723999\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2999596893787384\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2571439743041992\n",
      "train accuracy:  0.703125\n",
      "Epoch: 04, Train Loss: 0.336, Train Acc: 66.25%, Val. Loss: 1.013, Val. Acc: 35.68%\n",
      "time duration:     13.820571782067418\n",
      "train batch loss:  0.235505610704422\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.37734749913215637\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2897810935974121\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28058934211730957\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.33861660957336426\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2925678789615631\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.27676087617874146\n",
      "train accuracy:  0.6875\n",
      "Epoch: 05, Train Loss: 0.310, Train Acc: 67.82%, Val. Loss: 0.968, Val. Acc: 37.48%\n",
      "time duration:     13.867913398891687\n",
      "train batch loss:  0.2414328008890152\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2916901409626007\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.31660524010658264\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.27002954483032227\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2998526096343994\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2560608685016632\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.32767441868782043\n",
      "train accuracy:  0.625\n",
      "Epoch: 06, Train Loss: 0.292, Train Acc: 68.90%, Val. Loss: 0.949, Val. Acc: 39.19%\n",
      "time duration:     14.232238959521055\n",
      "train batch loss:  0.2499021738767624\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.24749019742012024\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20930950343608856\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.26638051867485046\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.20644807815551758\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1886853128671646\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.22359803318977356\n",
      "train accuracy:  0.71875\n",
      "Epoch: 07, Train Loss: 0.277, Train Acc: 69.88%, Val. Loss: 0.760, Val. Acc: 44.50%\n",
      "time duration:     14.035382667556405\n",
      "train batch loss:  0.2940122187137604\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2665769159793854\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.17515549063682556\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.3093004822731018\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.39386484026908875\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.25430530309677124\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.27893853187561035\n",
      "train accuracy:  0.734375\n",
      "Epoch: 08, Train Loss: 0.263, Train Acc: 70.96%, Val. Loss: 0.781, Val. Acc: 43.99%\n",
      "time duration:     14.043731631711125\n",
      "train batch loss:  0.2610582113265991\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2353234589099884\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.21078041195869446\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2357562929391861\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.24474778771400452\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.21113111078739166\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2392505705356598\n",
      "train accuracy:  0.75\n",
      "Epoch: 09, Train Loss: 0.253, Train Acc: 71.98%, Val. Loss: 0.799, Val. Acc: 43.26%\n",
      "time duration:     14.003765035420656\n",
      "train batch loss:  0.20237496495246887\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2460208237171173\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.21332532167434692\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23500758409500122\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.21109814941883087\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.16606733202934265\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22297373414039612\n",
      "train accuracy:  0.6875\n",
      "Epoch: 10, Train Loss: 0.240, Train Acc: 72.79%, Val. Loss: 0.792, Val. Acc: 44.12%\n",
      "time duration:     14.008466817438602\n",
      "train batch loss:  0.20956766605377197\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.25287342071533203\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2728837728500366\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.18933641910552979\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1782035529613495\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22366394102573395\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2774866819381714\n",
      "train accuracy:  0.71875\n",
      "Epoch: 11, Train Loss: 0.231, Train Acc: 73.42%, Val. Loss: 0.719, Val. Acc: 46.67%\n",
      "time duration:     13.927230900153518\n",
      "train batch loss:  0.23863577842712402\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.26562532782554626\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.33263301849365234\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.24004808068275452\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.21056705713272095\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.19459591805934906\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19982706010341644\n",
      "train accuracy:  0.78125\n",
      "Epoch: 12, Train Loss: 0.227, Train Acc: 74.10%, Val. Loss: 0.553, Val. Acc: 52.67%\n",
      "time duration:     13.806809866800904\n",
      "train batch loss:  0.3206777572631836\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.17395935952663422\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.19913671910762787\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.321816623210907\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2216620296239853\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2172287553548813\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.246515154838562\n",
      "train accuracy:  0.6875\n",
      "Epoch: 13, Train Loss: 0.215, Train Acc: 75.06%, Val. Loss: 0.548, Val. Acc: 53.24%\n",
      "time duration:     14.070159893482924\n",
      "train batch loss:  0.17233796417713165\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19472560286521912\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.21348737180233002\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.235495924949646\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.18980124592781067\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.09994858503341675\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14779764413833618\n",
      "train accuracy:  0.8125\n",
      "Epoch: 14, Train Loss: 0.208, Train Acc: 75.61%, Val. Loss: 0.576, Val. Acc: 51.86%\n",
      "time duration:     14.116267843171954\n",
      "train batch loss:  0.21996231377124786\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.16856738924980164\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.28725531697273254\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.23191027343273163\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.20050060749053955\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24558524787425995\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.15229521691799164\n",
      "train accuracy:  0.828125\n",
      "Epoch: 15, Train Loss: 0.202, Train Acc: 76.16%, Val. Loss: 0.576, Val. Acc: 52.48%\n",
      "time duration:     14.068596860393882\n",
      "train batch loss:  0.14172056317329407\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2003221958875656\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2409750074148178\n",
      "train accuracy:  0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.1432146579027176\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20840467512607574\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17796704173088074\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13284364342689514\n",
      "train accuracy:  0.8125\n",
      "Epoch: 16, Train Loss: 0.198, Train Acc: 76.76%, Val. Loss: 0.553, Val. Acc: 53.36%\n",
      "time duration:     13.885107917711139\n",
      "train batch loss:  0.25081413984298706\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1867889165878296\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.20714765787124634\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.21228525042533875\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.14842166006565094\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1553899347782135\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.16281655430793762\n",
      "train accuracy:  0.796875\n",
      "Epoch: 17, Train Loss: 0.192, Train Acc: 77.21%, Val. Loss: 0.529, Val. Acc: 54.62%\n",
      "time duration:     13.87891311571002\n",
      "train batch loss:  0.12406158447265625\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20386724174022675\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.16540122032165527\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12494989484548569\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.192380890250206\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.17482785880565643\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.19168466329574585\n",
      "train accuracy:  0.78125\n",
      "Epoch: 18, Train Loss: 0.187, Train Acc: 77.72%, Val. Loss: 0.579, Val. Acc: 52.32%\n",
      "time duration:     13.972320904955268\n",
      "train batch loss:  0.16715694963932037\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.21608251333236694\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16678091883659363\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16407223045825958\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.15703687071800232\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.15433096885681152\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2908477187156677\n",
      "train accuracy:  0.640625\n",
      "Epoch: 19, Train Loss: 0.180, Train Acc: 78.52%, Val. Loss: 0.592, Val. Acc: 52.52%\n",
      "time duration:     14.034860014915466\n",
      "train batch loss:  0.15436941385269165\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.14297516644001007\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12411291152238846\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14002707600593567\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.14931535720825195\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.18129634857177734\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1525706946849823\n",
      "train accuracy:  0.796875\n",
      "Epoch: 20, Train Loss: 0.176, Train Acc: 79.06%, Val. Loss: 0.560, Val. Acc: 53.23%\n",
      "time duration:     14.109987849369645\n",
      "train batch loss:  0.1285283863544464\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1736583411693573\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.14246250689029694\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.24678213894367218\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.15102556347846985\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17281807959079742\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17359879612922668\n",
      "train accuracy:  0.78125\n",
      "Epoch: 21, Train Loss: 0.171, Train Acc: 79.33%, Val. Loss: 0.566, Val. Acc: 53.47%\n",
      "time duration:     13.891107330098748\n",
      "train batch loss:  0.19946423172950745\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.19023777544498444\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12718629837036133\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.248934805393219\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.14551031589508057\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.182157963514328\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.16450177133083344\n",
      "train accuracy:  0.859375\n",
      "Epoch: 22, Train Loss: 0.166, Train Acc: 79.85%, Val. Loss: 0.595, Val. Acc: 52.58%\n",
      "time duration:     13.971543276682496\n",
      "train batch loss:  0.1831732988357544\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.164683997631073\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.15268439054489136\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1510811150074005\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1189962700009346\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2235865294933319\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.16787004470825195\n",
      "train accuracy:  0.796875\n",
      "Epoch: 23, Train Loss: 0.162, Train Acc: 80.43%, Val. Loss: 0.504, Val. Acc: 55.53%\n",
      "time duration:     14.288752418011427\n",
      "train batch loss:  0.13244158029556274\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13029220700263977\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15271958708763123\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1907086819410324\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.1657164990901947\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11038649827241898\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.130399689078331\n",
      "train accuracy:  0.890625\n",
      "Epoch: 24, Train Loss: 0.157, Train Acc: 81.05%, Val. Loss: 0.505, Val. Acc: 55.84%\n",
      "time duration:     14.064898930490017\n",
      "train batch loss:  0.0908086821436882\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13964058458805084\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11977960169315338\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.16988764703273773\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17053398489952087\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.19397154450416565\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.26126161217689514\n",
      "train accuracy:  0.84375\n",
      "Epoch: 25, Train Loss: 0.154, Train Acc: 81.37%, Val. Loss: 0.504, Val. Acc: 55.92%\n",
      "time duration:     14.051408058032393\n",
      "train batch loss:  0.1422956883907318\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12409628927707672\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13223740458488464\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17691278457641602\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.13323557376861572\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1783081293106079\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1822025030851364\n",
      "train accuracy:  0.765625\n",
      "Epoch: 26, Train Loss: 0.150, Train Acc: 81.82%, Val. Loss: 0.501, Val. Acc: 55.84%\n",
      "time duration:     14.328318804502487\n",
      "train batch loss:  0.14892607927322388\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.22867608070373535\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.22318731248378754\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.07919018715620041\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.15334554016590118\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.06892652809619904\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.17649096250534058\n",
      "train accuracy:  0.765625\n",
      "Epoch: 27, Train Loss: 0.145, Train Acc: 82.27%, Val. Loss: 0.525, Val. Acc: 54.99%\n",
      "time duration:     14.088907733559608\n",
      "train batch loss:  0.14099164307117462\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15426066517829895\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10484851151704788\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15481287240982056\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14220412075519562\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2723652124404907\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.18685242533683777\n",
      "train accuracy:  0.796875\n",
      "Epoch: 28, Train Loss: 0.143, Train Acc: 82.71%, Val. Loss: 0.529, Val. Acc: 55.30%\n",
      "time duration:     13.763865588232875\n",
      "train batch loss:  0.16489459574222565\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2210351824760437\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.16153189539909363\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12242044508457184\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13733747601509094\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.20233400166034698\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.10509995371103287\n",
      "train accuracy:  0.859375\n",
      "Epoch: 29, Train Loss: 0.139, Train Acc: 82.91%, Val. Loss: 0.539, Val. Acc: 54.61%\n",
      "time duration:     13.998792473226786\n",
      "train batch loss:  0.15336108207702637\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16433197259902954\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.07585017383098602\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1466434895992279\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.18123428523540497\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1377173662185669\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.25431957840919495\n",
      "train accuracy:  0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train Loss: 0.136, Train Acc: 83.58%, Val. Loss: 0.524, Val. Acc: 55.32%\n",
      "time duration:     13.962910966947675\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(allmodel, alltrain_iterator, alloptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(allmodel, allvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Beautymodel.state_dict(, 'movie/{}.bin'.format('train1')) \n",
    "torch.save(Apparelmodel.state_dict(), 'movie/{}.bin'.format('train2')) \n",
    "torch.save(allmodel.state_dict(), 'movie/{}.bin'.format('trainall')) \n",
    "torch.save(Jewelrymodel.state_dict(), 'movie/{}.bin'.format('train3')) \n",
    "torch.save(Shoesmodel.state_dict(), 'movie/{}.bin'.format('train4')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautymodel.load_state_dict(torch.load('movie/train1.bin'))\n",
    "Apparelmodel.load_state_dict(torch.load('movie/train2.bin'))\n",
    "Jewelrymodel.load_state_dict(torch.load('movie/train3.bin'))\n",
    "Shoesmodel.load_state_dict(torch.load('movie/train4.bin'))\n",
    "allmodel.load_state_dict(torch.load('movie/trainall.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "allmodel = allmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#load model\n",
    "Apparelmodel = torch.load('movie/train2.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "Jewelrymodel = torch.load('movie/train3.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "Shoesmodel = torch.load('movie/train4.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "allmodel = torch.load('movie/trainall.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "#Apparelmodel = Apparelmodel.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# prediction\n",
    "####################\n",
    "\n",
    "'''\n",
    "print('loading frnn4:')\n",
    "model = torch.load('frnn4',map_location=lambda storage,loc:storage)\n",
    "'''\n",
    "    \n",
    "print(\"prediction of frnn8.....\")\n",
    "    \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "with open('../sent/ori_gender_data/male_sent_test_less700.tsv','r') as f:\n",
    "    mtest = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_test_less700.tsv','r') as f:\n",
    "    ftest = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftest]\n",
    "ms = [line.split('\\t')[0] for line in mtest]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtest]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftest]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fprem:\")\n",
    "print(fprem[:10])\n",
    "print(\"10 fpref:\")\n",
    "print(fpref[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file fpref_frnn8.txt...\")\n",
    "with open('fpref_frnn8.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file fprem_frnn8.txt...\")\n",
    "with open('fprem_frnn8.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "\n",
    "\n",
    "'''\n",
    "with open('../sent/ori_gender_data/male_sent_tmp_train.tsv','r') as f:\n",
    "    mtrain = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_tmp_train.tsv','r') as f:\n",
    "    ftrain = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftrain]\n",
    "ms = [line.split('\\t')[0] for line in mtrain]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtrain]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftrain]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fpref on female_sent_tmp_train.tsv:\")\n",
    "print(fpref[:10])\n",
    "print(\"10 fprem on male_sent_tmp_train.tsv:\")\n",
    "print(fprem[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file :fpre_female_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_female_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file :fpre_male_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_male_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext=[]\n",
    "testlabel=[]\n",
    "with open('movie/mytest.tsv','r') as f:\n",
    "    for line in f:\n",
    "        x = line.strip('\\n').split('\\t')\n",
    "        testtext.append(x[0])\n",
    "        testlabel.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence,model,TEXT):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.835625410079956"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Beautymodel,BeautyTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8109288215637207"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Apparelmodel,ApparelTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.170316457748413"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Jewelrymodel,JewelryTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6061437129974365"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Shoesmodel,ShoesTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0509753227233887"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],allmodel,allTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlabel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predall = []\n",
    "for x in testtext:\n",
    "    predall.append(predict_sentiment(x,allmodel,allTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = np.array([float(x) for x in testlabel])\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69695"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((testlabel-pred)<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = []\n",
    "pred2 = []\n",
    "pred3 = []\n",
    "pred4 = []\n",
    "for x in testtext:\n",
    "    pred1.append(predict_sentiment(x,Beautymodel,BeautyTEXT))\n",
    "    pred2.append(predict_sentiment(x,Apparelmodel,ApparelTEXT))\n",
    "    pred3.append(predict_sentiment(x,Jewelrymodel,JewelryTEXT))\n",
    "    pred4.append(predict_sentiment(x,Shoesmodel,ShoesTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.array(pred1)\n",
    "pred2 = np.array(pred2)\n",
    "pred3 = np.array(pred3)\n",
    "pred4 = np.array(pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie/test_prediction','w') as f:\n",
    "    f.write('label   preall   pre1   pre2   pre3   pre4')\n",
    "    for i in range(len(predall)):\n",
    "        f.write(str(testlabel[i])+'\\t'+str(predall[i])+'\\t'+str(pred1[i])+'\\t'+str(pred2[i])+'\\t'+str(pred3[i])+'\\t'+str(pred4[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
