{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBeautyTEXT = data.Field(tokenize=\\'spacy\\')\\nBeautyLABEL = data.LabelField(tensor_type=torch.FloatTensor)\\n\\nprint(\"loading dataset clean_Beauty300.tsv...\")\\nBeautytrain  = data.TabularDataset.splits(\\n        path=\\'../counter-sent-generation3/VAE/data/official_Amazon/\\', \\n        train=\\'clean_Beauty300.tsv\\',\\n        format=\\'tsv\\',\\n        fields=[(\\'Text\\', BeautyTEXT),(\\'Label\\', BeautyLABEL)])[0]\\n\\nBeautyTEXT.build_vocab(Beautytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\\nBeautyLABEL.build_vocab(Beautytrain)\\n\\nBeautyLABEL.vocab.stoi[\\'1\\']=1\\nBeautyLABEL.vocab.stoi[\\'2\\']=2\\nBeautyLABEL.vocab.stoi[\\'3\\']=3\\nBeautyLABEL.vocab.stoi[\\'4\\']=4\\nBeautyLABEL.vocab.stoi[\\'5\\']=5\\n\\nApparelTEXT = data.Field(tokenize=\\'spacy\\')\\nApparelLABEL = data.LabelField(tensor_type=torch.FloatTensor)\\n\\nprint(\"loading dataset clean_Apparel300.tsv...\")\\nAppareltrain  = data.TabularDataset.splits(\\n        path=\\'../counter-sent-generation3/VAE/data/official_Amazon/\\', \\n        train=\\'clean_Apparel300.tsv\\',\\n        format=\\'tsv\\',\\n        fields=[(\\'Text\\', ApparelTEXT),(\\'Label\\', ApparelLABEL)])[0]\\n\\nApparelTEXT.build_vocab(Appareltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\\nApparelLABEL.build_vocab(Appareltrain)\\n\\nApparelLABEL.vocab.stoi[\\'1\\']=1\\nApparelLABEL.vocab.stoi[\\'2\\']=2\\nApparelLABEL.vocab.stoi[\\'3\\']=3\\nApparelLABEL.vocab.stoi[\\'4\\']=4\\nApparelLABEL.vocab.stoi[\\'5\\']=5\\n\\nJewelryTEXT = data.Field(tokenize=\\'spacy\\')\\nJewelryLABEL = data.LabelField(tensor_type=torch.FloatTensor)\\n\\nprint(\"loading dataset clean_Jewelry300.tsv...\")\\nJewelrytrain  = data.TabularDataset.splits(\\n        path=\\'../counter-sent-generation3/VAE/data/official_Amazon/\\', \\n        train=\\'clean_Jewelry300.tsv\\',\\n        format=\\'tsv\\',\\n        fields=[(\\'Text\\', JewelryTEXT),(\\'Label\\', JewelryLABEL)])[0]\\n\\nJewelryTEXT.build_vocab(Jewelrytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\\nJewelryLABEL.build_vocab(Jewelrytrain)\\n\\nJewelryLABEL.vocab.stoi[\\'1\\']=1\\nJewelryLABEL.vocab.stoi[\\'2\\']=2\\nJewelryLABEL.vocab.stoi[\\'3\\']=3\\nJewelryLABEL.vocab.stoi[\\'4\\']=4\\nJewelryLABEL.vocab.stoi[\\'5\\']=5\\n\\nShoesTEXT = data.Field(tokenize=\\'spacy\\')\\nShoesLABEL = data.LabelField(tensor_type=torch.FloatTensor)\\n\\nprint(\"loading dataset clean_Shoes300.tsv...\")\\nShoestrain  = data.TabularDataset.splits(\\n        path=\\'../counter-sent-generation3/VAE/data/official_Amazon/\\', \\n        train=\\'clean_Shoes300.tsv\\',\\n        format=\\'tsv\\',\\n        fields=[(\\'Text\\', ShoesTEXT),(\\'Label\\', ShoesLABEL)])[0]\\n\\nShoesTEXT.build_vocab(Shoestrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\\nShoesLABEL.build_vocab(Shoestrain)\\n\\nShoesLABEL.vocab.stoi[\\'1\\']=1\\nShoesLABEL.vocab.stoi[\\'2\\']=2\\nShoesLABEL.vocab.stoi[\\'3\\']=3\\nShoesLABEL.vocab.stoi[\\'4\\']=4\\nShoesLABEL.vocab.stoi[\\'5\\']=5\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "'''\n",
    "BeautyTEXT = data.Field(tokenize='spacy')\n",
    "BeautyLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300.tsv...\")\n",
    "Beautytrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Beauty300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', BeautyTEXT),('Label', BeautyLABEL)])[0]\n",
    "\n",
    "BeautyTEXT.build_vocab(Beautytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "BeautyLABEL.build_vocab(Beautytrain)\n",
    "\n",
    "BeautyLABEL.vocab.stoi['1']=1\n",
    "BeautyLABEL.vocab.stoi['2']=2\n",
    "BeautyLABEL.vocab.stoi['3']=3\n",
    "BeautyLABEL.vocab.stoi['4']=4\n",
    "BeautyLABEL.vocab.stoi['5']=5\n",
    "\n",
    "ApparelTEXT = data.Field(tokenize='spacy')\n",
    "ApparelLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "Appareltrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Apparel300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ApparelTEXT),('Label', ApparelLABEL)])[0]\n",
    "\n",
    "ApparelTEXT.build_vocab(Appareltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ApparelLABEL.build_vocab(Appareltrain)\n",
    "\n",
    "ApparelLABEL.vocab.stoi['1']=1\n",
    "ApparelLABEL.vocab.stoi['2']=2\n",
    "ApparelLABEL.vocab.stoi['3']=3\n",
    "ApparelLABEL.vocab.stoi['4']=4\n",
    "ApparelLABEL.vocab.stoi['5']=5\n",
    "\n",
    "JewelryTEXT = data.Field(tokenize='spacy')\n",
    "JewelryLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Jewelry300.tsv...\")\n",
    "Jewelrytrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Jewelry300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', JewelryTEXT),('Label', JewelryLABEL)])[0]\n",
    "\n",
    "JewelryTEXT.build_vocab(Jewelrytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "JewelryLABEL.build_vocab(Jewelrytrain)\n",
    "\n",
    "JewelryLABEL.vocab.stoi['1']=1\n",
    "JewelryLABEL.vocab.stoi['2']=2\n",
    "JewelryLABEL.vocab.stoi['3']=3\n",
    "JewelryLABEL.vocab.stoi['4']=4\n",
    "JewelryLABEL.vocab.stoi['5']=5\n",
    "\n",
    "ShoesTEXT = data.Field(tokenize='spacy')\n",
    "ShoesLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Shoes300.tsv...\")\n",
    "Shoestrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Shoes300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ShoesTEXT),('Label', ShoesLABEL)])[0]\n",
    "\n",
    "ShoesTEXT.build_vocab(Shoestrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ShoesLABEL.build_vocab(Shoestrain)\n",
    "\n",
    "ShoesLABEL.vocab.stoi['1']=1\n",
    "ShoesLABEL.vocab.stoi['2']=2\n",
    "ShoesLABEL.vocab.stoi['3']=3\n",
    "ShoesLABEL.vocab.stoi['4']=4\n",
    "ShoesLABEL.vocab.stoi['5']=5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty_Apparel300.tsv...\")\n",
    "train  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Beauty_Apparel300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),('Label', LABEL)])[0]\n",
    "\n",
    "TEXT.build_vocab(train, max_size=60000, vectors=\"glove.6B.300d\",min_freq=1)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "with open('Apparel300_vocab','w') as f:\n",
    "    json.dump(ApparelTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Beauty300_vocab','w') as f:\n",
    "    json.dump(BeautyTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Jewelry300_vocab','w') as f:\n",
    "    json.dump(JewelryTEXT.vocab.stoi,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train, valid = train.split(split_ratio=0.8)\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train, valid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_iterator = data.BucketIterator.splits(\\n    train, \\n    batch_size=BATCH_SIZE, \\n    sort_key=lambda x: len(x.Text), \\n    repeat=False)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "Beautytrain, Beautyvalid = Beautytrain.split(split_ratio=0.8)\n",
    "Beautytrain_iterator, Beautyvalid_iterator = data.BucketIterator.splits(\n",
    "    (Beautytrain, Beautyvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Appareltrain, Apparelvalid = Appareltrain.split(split_ratio=0.999)\n",
    "Appareltrain_iterator, Apparelvalid_iterator = data.BucketIterator.splits(\n",
    "    (Appareltrain, Apparelvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Jewelrytrain, Jewelryvalid = Jewelrytrain.split(split_ratio=0.8)\n",
    "Jewelrytrain_iterator, Jewelryvalid_iterator = data.BucketIterator.splits(\n",
    "    (Jewelrytrain, Jewelryvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Shoestrain, Shoesvalid = Shoestrain.split(split_ratio=0.8)\n",
    "Shoestrain_iterator, Shoesvalid_iterator = data.BucketIterator.splits(\n",
    "    (Shoestrain, Shoesvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''\n",
    "train_iterator = data.BucketIterator.splits(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #print(\"embedded shape: \", embedded.shape)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #print(\"output.shape: \",output.shape)\n",
    "        #print(\"output[-1].shape: \",output[-1].shape)\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        #print(\"cell.shape: \",cell.shape)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        \n",
    "        y = self.fc(hidden.squeeze(0))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        #return self.fc(hidden.squeeze(0))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20219"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BeautyTEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(20219, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "BeautyINPUT_DIM = len(BeautyTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Beautymodel = RNN(BeautyINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Beautymodel parameters: \")\n",
    "print(Beautymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = BeautyTEXT.vocab.vectors\n",
    "\n",
    "Beautymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Beautyoptimizer = optim.Adam(Beautymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparelmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(18035, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ApparelINPUT_DIM = len(ApparelTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Apparelmodel = RNN(ApparelINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Apparelmodel parameters: \")\n",
    "print(Apparelmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ApparelTEXT.vocab.vectors\n",
    "\n",
    "Apparelmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Appareloptimizer = optim.Adam(Apparelmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jewelrymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(16904, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "JewelryINPUT_DIM = len(JewelryTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Jewelrymodel = RNN(JewelryINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Jewelrymodel parameters: \")\n",
    "print(Jewelrymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = JewelryTEXT.vocab.vectors\n",
    "\n",
    "Jewelrymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Jewelryoptimizer = optim.Adam(Jewelrymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(17486, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ShoesINPUT_DIM = len(ShoesTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Shoesmodel = RNN(ShoesINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(Shoesmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ShoesTEXT.vocab.vectors\n",
    "\n",
    "Shoesmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Shoesoptimizer = optim.Adam(Shoesmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(preds,y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # turns on dropout and batch normalization and allow gradient update\n",
    "    \n",
    "    i=0\n",
    "    for batch in iterator:\n",
    "        i=i+1\n",
    "        \n",
    "        optimizer.zero_grad() # set accumulated gradient to 0 for every start of a batch\n",
    "        \n",
    "        predictions = model(batch.Text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Label)\n",
    "        \n",
    "        acc = accuracy(predictions, batch.Label)\n",
    "        \n",
    "        loss.backward() # calculate gradient\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"train batch loss: \", loss.item())\n",
    "            print(\"train accuracy: \", acc.item())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #turns off dropout and batch normalization\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for batch in iterator:\n",
    "            i=i+1\n",
    "            predictions = model(batch.Text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i%200 ==0:\n",
    "                print(\"eval batch loss: \", loss.item())\n",
    "                print(\"eval accuracy: \", acc.item())\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "#model = torch.load('fmodel')\n",
    "\n",
    "import timeit\n",
    "#start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6944787502288818\n",
      "train accuracy:  0.25\n",
      "train batch loss:  1.1130222082138062\n",
      "train accuracy:  0.1875\n",
      "train batch loss:  0.42840540409088135\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.49681568145751953\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.7590948343276978\n",
      "train accuracy:  0.5\n",
      "train batch loss:  1.0518995523452759\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5709025263786316\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  1.2898609638214111\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5770605206489563\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5210078954696655\n",
      "train accuracy:  0.46875\n",
      "train batch loss:  0.6203491687774658\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.4728987216949463\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4706098437309265\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.7257290482521057\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.46271830797195435\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  1.1187175512313843\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.7729892730712891\n",
      "train accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.7729634046554565\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.9614523649215698\n",
      "eval accuracy:  0.5\n",
      "Epoch: 01, Train Loss: 0.880, Train Acc: 51.81%, Val. Loss: 1.550, Val. Acc: 48.15%\n",
      "time duration:     61.461701856926084\n",
      "train batch loss:  1.2095149755477905\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.46582889556884766\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  1.0260478258132935\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7600288987159729\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3890240788459778\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5501645803451538\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.6296619176864624\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.6033787727355957\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.7950732111930847\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  1.120182991027832\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.32525211572647095\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.7090725898742676\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4918961226940155\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.6308704614639282\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.6101078987121582\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.48233890533447266\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.6898689270019531\n",
      "train accuracy:  0.5625\n",
      "eval batch loss:  0.6861712336540222\n",
      "eval accuracy:  0.65625\n",
      "eval batch loss:  0.7685175538063049\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 02, Train Loss: 0.575, Train Acc: 60.90%, Val. Loss: 1.005, Val. Acc: 58.53%\n",
      "time duration:     63.987115731462836\n",
      "train batch loss:  0.9769548177719116\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7020853757858276\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5793048739433289\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.35020849108695984\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2054305076599121\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  1.0087592601776123\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.8122888207435608\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.09895996749401093\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.2993268370628357\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5882315039634705\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.43079373240470886\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3382006287574768\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2918552756309509\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.32438886165618896\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5257395505905151\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.26228296756744385\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5249297618865967\n",
      "train accuracy:  0.625\n",
      "eval batch loss:  0.6453770399093628\n",
      "eval accuracy:  0.65625\n",
      "eval batch loss:  0.6676900386810303\n",
      "eval accuracy:  0.46875\n",
      "Epoch: 03, Train Loss: 0.507, Train Acc: 63.55%, Val. Loss: 0.910, Val. Acc: 54.30%\n",
      "time duration:     66.10549740679562\n",
      "train batch loss:  0.3652540445327759\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4803811311721802\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.49160489439964294\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3665144741535187\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.26893576979637146\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5546836853027344\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4393306076526642\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.31738367676734924\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.20582519471645355\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5602772831916809\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4164278507232666\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.44739818572998047\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5062764286994934\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5851519703865051\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.945551335811615\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5309925079345703\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.7295385599136353\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6147526502609253\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.5083547830581665\n",
      "eval accuracy:  0.625\n",
      "Epoch: 04, Train Loss: 0.470, Train Acc: 65.21%, Val. Loss: 0.863, Val. Acc: 55.87%\n",
      "time duration:     65.48048909381032\n",
      "train batch loss:  0.420634925365448\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.5621141195297241\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.330331951379776\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.7480893731117249\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.30035942792892456\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.39139053225517273\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.37045466899871826\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.7229379415512085\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.6232441067695618\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.2707490921020508\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6749610900878906\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5940496921539307\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3424467444419861\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7704634070396423\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.359281063079834\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.37443920969963074\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.4152337312698364\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6419727206230164\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.5183955430984497\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 05, Train Loss: 0.439, Train Acc: 66.31%, Val. Loss: 0.827, Val. Acc: 60.19%\n",
      "time duration:     65.17329184524715\n",
      "train batch loss:  0.3269195556640625\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.43777841329574585\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3048725724220276\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26024141907691956\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.41194993257522583\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.30826300382614136\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.28788384795188904\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2424384504556656\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17134344577789307\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.6267094016075134\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3506917953491211\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3959586024284363\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.3463447093963623\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.7922376394271851\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4230286478996277\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.41716933250427246\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.29550063610076904\n",
      "train accuracy:  0.59375\n",
      "eval batch loss:  0.6891484260559082\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.527066707611084\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 06, Train Loss: 0.410, Train Acc: 67.45%, Val. Loss: 0.817, Val. Acc: 61.61%\n",
      "time duration:     64.86875606700778\n",
      "train batch loss:  0.36113300919532776\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1416870653629303\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.32553941011428833\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.20867997407913208\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.37415486574172974\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.32049620151519775\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.29419147968292236\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5719939470291138\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.23627442121505737\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24191337823867798\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.27522405982017517\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.686618447303772\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5005362033843994\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.8340542316436768\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.6820176839828491\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.4295312762260437\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.31743329763412476\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.5753248929977417\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5527840852737427\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 07, Train Loss: 0.392, Train Acc: 68.31%, Val. Loss: 0.806, Val. Acc: 62.17%\n",
      "time duration:     64.6489941868931\n",
      "train batch loss:  0.3334934711456299\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5944424867630005\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.49264979362487793\n",
      "train accuracy:  0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.37804922461509705\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3283217251300812\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3317772150039673\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3294980525970459\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3682803213596344\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.23413047194480896\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.4025174379348755\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.36335405707359314\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23460334539413452\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.3531022071838379\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5043470859527588\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3070678114891052\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4309746026992798\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.48209208250045776\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6830652952194214\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6398743391036987\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 08, Train Loss: 0.377, Train Acc: 69.36%, Val. Loss: 0.716, Val. Acc: 64.57%\n",
      "time duration:     65.13433141261339\n",
      "train batch loss:  0.38326045870780945\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.6411615014076233\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.4228762984275818\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.22093802690505981\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.3459961414337158\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.34084200859069824\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.44390860199928284\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.5280784964561462\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5976966619491577\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.357913076877594\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4018459618091583\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.38559165596961975\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2764211893081665\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1488812267780304\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.24249784648418427\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4439336657524109\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.31470000743865967\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.6700568795204163\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.6044360399246216\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 09, Train Loss: 0.362, Train Acc: 70.02%, Val. Loss: 0.697, Val. Acc: 65.47%\n",
      "time duration:     64.77526031062007\n",
      "train batch loss:  0.49614417552948\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.36990129947662354\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4415575861930847\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.694683313369751\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.33005931973457336\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.5681099891662598\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.31491973996162415\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5795221328735352\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.42833343148231506\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.721993088722229\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.20143216848373413\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5759652256965637\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4874257743358612\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4003485441207886\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22145524621009827\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1825898289680481\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.40516963601112366\n",
      "train accuracy:  0.65625\n",
      "eval batch loss:  0.7768604755401611\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.42945581674575806\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 10, Train Loss: 0.345, Train Acc: 70.60%, Val. Loss: 0.633, Val. Acc: 65.83%\n",
      "time duration:     65.08825089037418\n",
      "train batch loss:  0.2976674437522888\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4197145700454712\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.18362358212471008\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.214758962392807\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23769327998161316\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.32214200496673584\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.29512184858322144\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23752161860466003\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18766836822032928\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28675028681755066\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.35278528928756714\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3591375946998596\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2513686418533325\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6147065758705139\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.42832401394844055\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.6109654903411865\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2694217562675476\n",
      "train accuracy:  0.6875\n",
      "eval batch loss:  0.5274425745010376\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.7826899886131287\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 11, Train Loss: 0.331, Train Acc: 71.49%, Val. Loss: 0.668, Val. Acc: 66.55%\n",
      "time duration:     65.0604446362704\n",
      "train batch loss:  0.15514393150806427\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18327566981315613\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.698167085647583\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.5872822403907776\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.35742318630218506\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.72477126121521\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.19497565925121307\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.12775175273418427\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.21729697287082672\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.6118708252906799\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4881320893764496\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4450601041316986\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.26149657368659973\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.38034218549728394\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3644663691520691\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19595645368099213\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.36132973432540894\n",
      "train accuracy:  0.75\n",
      "eval batch loss:  0.6868850588798523\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.5453809499740601\n",
      "eval accuracy:  0.5\n",
      "Epoch: 12, Train Loss: 0.315, Train Acc: 72.23%, Val. Loss: 0.661, Val. Acc: 65.41%\n",
      "time duration:     65.12047510407865\n",
      "train batch loss:  0.2616892457008362\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.5521905422210693\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4805822968482971\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.13453252613544464\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.4897410273551941\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.31596291065216064\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2818089723587036\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.7784060835838318\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1315118819475174\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.24552498757839203\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20516839623451233\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3225706219673157\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.35877639055252075\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4492284655570984\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24394825100898743\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.195804163813591\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.36999985575675964\n",
      "train accuracy:  0.84375\n",
      "eval batch loss:  0.7308954000473022\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.6344813704490662\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 13, Train Loss: 0.311, Train Acc: 72.38%, Val. Loss: 0.626, Val. Acc: 65.53%\n",
      "time duration:     65.14614174701273\n",
      "train batch loss:  0.1705777794122696\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.20642131567001343\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18812213838100433\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.35176563262939453\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.14422696828842163\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16021817922592163\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.4905363917350769\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2942260801792145\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3226441740989685\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2048337161540985\n",
      "train accuracy:  0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.3432580232620239\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.27045273780822754\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17084985971450806\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3183668255805969\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2928873300552368\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26063424348831177\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.20320530235767365\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.6661416292190552\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6164524555206299\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 14, Train Loss: 0.301, Train Acc: 73.19%, Val. Loss: 0.655, Val. Acc: 65.66%\n",
      "time duration:     65.0846634004265\n",
      "train batch loss:  0.2700929045677185\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2551226317882538\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12264645099639893\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.26584261655807495\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23913303017616272\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.35731056332588196\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.3845929503440857\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19813621044158936\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.21857649087905884\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.7380242347717285\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.16726216673851013\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4967685043811798\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3238386809825897\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.24119573831558228\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2740669846534729\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.35979998111724854\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.1531888246536255\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.6906813979148865\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5761730074882507\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 15, Train Loss: 0.291, Train Acc: 73.80%, Val. Loss: 0.617, Val. Acc: 65.15%\n",
      "time duration:     64.7607021201402\n",
      "train batch loss:  0.19602037966251373\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2612307071685791\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.15778818726539612\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.3601011335849762\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23266714811325073\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11398052424192429\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14279577136039734\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.35728588700294495\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.22172628343105316\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18653005361557007\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1671086549758911\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22237318754196167\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3729792833328247\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.16631707549095154\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2041517198085785\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16337385773658752\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.24863433837890625\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.7570111751556396\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.8035313487052917\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 16, Train Loss: 0.281, Train Acc: 74.19%, Val. Loss: 0.663, Val. Acc: 66.05%\n",
      "time duration:     64.87291954644024\n",
      "train batch loss:  0.46751317381858826\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.21864929795265198\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.22391125559806824\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3845176100730896\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2481372356414795\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.34966611862182617\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.12780314683914185\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.28608742356300354\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16264861822128296\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.21620361506938934\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.4806903004646301\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.32791250944137573\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15427084267139435\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1749197095632553\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2747229337692261\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1927386224269867\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.195176899433136\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.6684154272079468\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6596065759658813\n",
      "eval accuracy:  0.5\n",
      "Epoch: 17, Train Loss: 0.271, Train Acc: 74.94%, Val. Loss: 0.643, Val. Acc: 66.00%\n",
      "time duration:     65.0987827796489\n",
      "train batch loss:  0.238729327917099\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5986758470535278\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.1344938576221466\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.2039276659488678\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17159107327461243\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.4173009991645813\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2896788716316223\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5015656352043152\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.27226102352142334\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.5821436643600464\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.339300274848938\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.1956527829170227\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2643859386444092\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18448558449745178\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2683703303337097\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23763850331306458\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28572580218315125\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.7056087255477905\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.5818093419075012\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 18, Train Loss: 0.261, Train Acc: 75.45%, Val. Loss: 0.680, Val. Acc: 65.56%\n",
      "time duration:     64.94138947874308\n",
      "train batch loss:  0.3672240376472473\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19960153102874756\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.20460394024848938\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2223706841468811\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08655233681201935\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.3087875247001648\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24967491626739502\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.36652636528015137\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19135238230228424\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.33380454778671265\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17247605323791504\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22989532351493835\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4490343928337097\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.22760671377182007\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.10328425467014313\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.4623459577560425\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.16722367703914642\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.7248859405517578\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5342824459075928\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 19, Train Loss: 0.253, Train Acc: 76.16%, Val. Loss: 0.677, Val. Acc: 65.30%\n",
      "time duration:     65.20977753400803\n",
      "train batch loss:  0.2620387077331543\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1917487233877182\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22305592894554138\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2669419050216675\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19561882317066193\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23810388147830963\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24251951277256012\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.08557289093732834\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.2730984687805176\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.3542650043964386\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1964203119277954\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18238666653633118\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11651552468538284\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2347961813211441\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2049972414970398\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20733310282230377\n",
      "train accuracy:  0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.2177867889404297\n",
      "train accuracy:  0.75\n",
      "eval batch loss:  0.7113252878189087\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.5569972991943359\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 20, Train Loss: 0.245, Train Acc: 76.76%, Val. Loss: 0.686, Val. Acc: 65.28%\n",
      "time duration:     65.28521787002683\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Shoesmodel, Shoestrain_iterator, Shoesoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Shoesmodel, Shoesvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# prediction\n",
    "####################\n",
    "\n",
    "'''\n",
    "print('loading frnn4:')\n",
    "model = torch.load('frnn4',map_location=lambda storage,loc:storage)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "print(\"valid loss: \",valid_loss)\n",
    "print(\"valid acc: \",valid_acc)\n",
    "\n",
    "    \n",
    "print(\"prediction of frnn8.....\")\n",
    "    \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "with open('../sent/ori_gender_data/male_sent_test_less700.tsv','r') as f:\n",
    "    mtest = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_test_less700.tsv','r') as f:\n",
    "    ftest = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftest]\n",
    "ms = [line.split('\\t')[0] for line in mtest]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtest]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftest]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fprem:\")\n",
    "print(fprem[:10])\n",
    "print(\"10 fpref:\")\n",
    "print(fpref[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file fpref_frnn8.txt...\")\n",
    "with open('fpref_frnn8.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file fprem_frnn8.txt...\")\n",
    "with open('fprem_frnn8.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "\n",
    "\n",
    "'''\n",
    "with open('../sent/ori_gender_data/male_sent_tmp_train.tsv','r') as f:\n",
    "    mtrain = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_tmp_train.tsv','r') as f:\n",
    "    ftrain = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftrain]\n",
    "ms = [line.split('\\t')[0] for line in mtrain]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtrain]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftrain]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fpref on female_sent_tmp_train.tsv:\")\n",
    "print(fpref[:10])\n",
    "print(\"10 fprem on male_sent_tmp_train.tsv:\")\n",
    "print(fprem[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file :fpre_female_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_female_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file :fpre_male_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_male_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
