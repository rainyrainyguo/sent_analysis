{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty300.tsv...\n"
     ]
    }
   ],
   "source": [
    "BeautyTEXT = data.Field(tokenize='spacy')\n",
    "BeautyLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300.tsv...\")\n",
    "Beautytrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain1.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', BeautyTEXT),('Label', BeautyLABEL)])[0]\n",
    "\n",
    "BeautyTEXT.build_vocab(Beautytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "BeautyLABEL.build_vocab(Beautytrain)\n",
    "\n",
    "for a,b in BeautyLABEL.vocab.stoi.items():\n",
    "    BeautyLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ApparelTEXT = data.Field(tokenize='spacy')\n",
    "ApparelLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "Appareltrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain2.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ApparelTEXT),('Label', ApparelLABEL)])[0]\n",
    "\n",
    "ApparelTEXT.build_vocab(Appareltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ApparelLABEL.build_vocab(Appareltrain)\n",
    "\n",
    "for a,b in ApparelLABEL.vocab.stoi.items():\n",
    "    ApparelLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Jewelry300.tsv...\n"
     ]
    }
   ],
   "source": [
    "JewelryTEXT = data.Field(tokenize='spacy')\n",
    "JewelryLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Jewelry300.tsv...\")\n",
    "Jewelrytrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain3.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', JewelryTEXT),('Label', JewelryLABEL)])[0]\n",
    "\n",
    "JewelryTEXT.build_vocab(Jewelrytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "JewelryLABEL.build_vocab(Jewelrytrain)\n",
    "\n",
    "for a,b in JewelryLABEL.vocab.stoi.items():\n",
    "    JewelryLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Shoes300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ShoesTEXT = data.Field(tokenize='spacy')\n",
    "ShoesLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Shoes300.tsv...\")\n",
    "Shoestrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ShoesTEXT),('Label', ShoesLABEL)])[0]\n",
    "\n",
    "ShoesTEXT.build_vocab(Shoestrain, max_size=60000, vectors=\"glove.6B.200d\",min_freq=1)\n",
    "ShoesLABEL.build_vocab(Shoestrain)\n",
    "\n",
    "for a,b in ShoesLABEL.vocab.stoi.items():\n",
    "    ShoesLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "allTEXT = data.Field(tokenize='spacy')\n",
    "allLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "alltrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', allTEXT),('Label', allLABEL)])[0]\n",
    "\n",
    "allTEXT.build_vocab(alltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "allLABEL.build_vocab(alltrain)\n",
    "\n",
    "for a,b in allLABEL.vocab.stoi.items():\n",
    "    allLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTEXT = data.Field(tokenize='spacy')\n",
    "allLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "alltrain  = data.TabularDataset.splits(\n",
    "        path='../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/', \n",
    "        train='mytrain.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', allTEXT),('Label', allLABEL)])[0]\n",
    "\n",
    "allTEXT.build_vocab(alltrain, max_size=60000, vectors=\"glove.6B.300d\",min_freq=1)\n",
    "allLABEL.build_vocab(alltrain)\n",
    "\n",
    "for a,b in allLABEL.vocab.stoi.items():\n",
    "    allLABEL.vocab.stoi[a]=float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "common1 = set.intersection(set(BeautyTEXT.vocab.itos),set(ApparelTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20197"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20492"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ApparelTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20528"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BeautyTEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "common2 = set.intersection(set(JewelryTEXT.vocab.itos),set(ShoesTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20197"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20528"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(JewelryTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20492"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ShoesTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport json\\nwith open('Apparel300_vocab','w') as f:\\n    json.dump(ApparelTEXT.vocab.stoi,f)\\n    \\nwith open('Beauty300_vocab','w') as f:\\n    json.dump(BeautyTEXT.vocab.stoi,f)\\n    \\nwith open('Jewelry300_vocab','w') as f:\\n    json.dump(JewelryTEXT.vocab.stoi,f)\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "with open('Apparel300_vocab','w') as f:\n",
    "    json.dump(ApparelTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Beauty300_vocab','w') as f:\n",
    "    json.dump(BeautyTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Jewelry300_vocab','w') as f:\n",
    "    json.dump(JewelryTEXT.vocab.stoi,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_iterator = data.BucketIterator.splits(\\n    train, \\n    batch_size=BATCH_SIZE, \\n    sort_key=lambda x: len(x.Text), \\n    repeat=False)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "Beautytrain, Beautyvalid = Beautytrain.split(split_ratio=0.8)\n",
    "Beautytrain_iterator, Beautyvalid_iterator = data.BucketIterator.splits(\n",
    "    (Beautytrain, Beautyvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Appareltrain, Apparelvalid = Appareltrain.split(split_ratio=0.8)\n",
    "Appareltrain_iterator, Apparelvalid_iterator = data.BucketIterator.splits(\n",
    "    (Appareltrain, Apparelvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Jewelrytrain, Jewelryvalid = Jewelrytrain.split(split_ratio=0.8)\n",
    "Jewelrytrain_iterator, Jewelryvalid_iterator = data.BucketIterator.splits(\n",
    "    (Jewelrytrain, Jewelryvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Shoestrain, Shoesvalid = Shoestrain.split(split_ratio=0.8)\n",
    "Shoestrain_iterator, Shoesvalid_iterator = data.BucketIterator.splits(\n",
    "    (Shoestrain, Shoesvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "alltrain, allvalid = alltrain.split(split_ratio=0.8)\n",
    "alltrain_iterator, allvalid_iterator = data.BucketIterator.splits(\n",
    "    (alltrain, allvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''\n",
    "train_iterator = data.BucketIterator.splits(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #print(\"embedded shape: \", embedded.shape)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #print(\"output.shape: \",output.shape)\n",
    "        #print(\"output[-1].shape: \",output[-1].shape)\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        #print(\"cell.shape: \",cell.shape)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        \n",
    "        y = self.fc(hidden.squeeze(0))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        #return self.fc(hidden.squeeze(0))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(20528, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "BeautyINPUT_DIM = len(BeautyTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "\n",
    "Beautymodel = RNN(BeautyINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Beautymodel parameters: \")\n",
    "print(Beautymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = BeautyTEXT.vocab.vectors\n",
    "\n",
    "Beautymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Beautyoptimizer = optim.Adam(Beautymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ApparelINPUT_DIM = len(ApparelTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "Apparelmodel = RNN(ApparelINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Apparelmodel parameters: \")\n",
    "print(Apparelmodel.parameters)\n",
    "pretrained_embeddings = ApparelTEXT.vocab.vectors\n",
    "Apparelmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "import torch.optim as optim\n",
    "Appareloptimizer = optim.Adam(Apparelmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "JewelryINPUT_DIM = len(JewelryTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "Jewelrymodel = RNN(JewelryINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Jewelrymodel parameters: \")\n",
    "print(Jewelrymodel.parameters)\n",
    "pretrained_embeddings = JewelryTEXT.vocab.vectors\n",
    "Jewelrymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "import torch.optim as optim\n",
    "Jewelryoptimizer = optim.Adam(Jewelrymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(20823, 200)\n",
      "  (rnn): LSTM(200, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n",
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(20823, 300)\n",
      "  (rnn): LSTM(300, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ShoesINPUT_DIM = len(ShoesTEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "Shoesmodel = RNN(ShoesINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(Shoesmodel.parameters)\n",
    "pretrained_embeddings = ShoesTEXT.vocab.vectors\n",
    "Shoesmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "import torch.optim as optim\n",
    "Shoesoptimizer = optim.Adam(Shoesmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "allINPUT_DIM = len(allTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.4\n",
    "allmodel = RNN(allINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(allmodel.parameters)\n",
    "pretrained_embeddings = allTEXT.vocab.vectors\n",
    "allmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "import torch.optim as optim\n",
    "alloptimizer = optim.Adam(allmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "allmodel = allmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def newaccuracy(preds,y):\n",
    "    correct = (abs(preds-y)<0.5).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def accuracy(preds,y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    y = torch.round(y)\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # turns on dropout and batch normalization and allow gradient update\n",
    "    \n",
    "    i=0\n",
    "    for batch in iterator:\n",
    "        i=i+1\n",
    "        \n",
    "        optimizer.zero_grad() # set accumulated gradient to 0 for every start of a batch\n",
    "        \n",
    "        predictions = model(batch.Text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Label)\n",
    "        \n",
    "        acc = newaccuracy(predictions, batch.Label)\n",
    "        \n",
    "        loss.backward() # calculate gradient\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"train batch loss: \", loss.item())\n",
    "            print(\"train accuracy: \", acc.item())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #turns off dropout and batch normalization\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for batch in iterator:\n",
    "            i=i+1\n",
    "            predictions = model(batch.Text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = newaccuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i%200 ==0:\n",
    "                print(\"eval batch loss: \", loss.item())\n",
    "                print(\"eval accuracy: \", acc.item())\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "#model = torch.load('fmodel')\n",
    "\n",
    "import timeit\n",
    "#start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shoesmodel.load_state_dict(torch.load('SSTmodel/SSTtrainall_glove200.bin'))\n",
    "allmodel.load_state_dict(torch.load('SSTmodel/SSTtrainall_fasttext300.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.18910574913024902\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.15086856484413147\n",
      "eval accuracy:  0.796875\n",
      "eval batch loss:  0.13925477862358093\n",
      "eval accuracy:  0.796875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.23502305030822754, 0.760825)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(allmodel, allvalid_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.16349555552005768\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.06887661665678024\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.17193764448165894\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.12999629974365234\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.14343485236167908\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1734437644481659\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14471544325351715\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.0973501205444336\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.14218400418758392\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14049634337425232\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.17450502514839172\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.21879222989082336\n",
      "train accuracy:  0.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.26153433322906494\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 01, Train Loss: 0.156, Train Acc: 81.61%, Val. Loss: 0.641, Val. Acc: 54.17%\n",
      "time duration:     25.46607632562518\n",
      "train batch loss:  0.10805551707744598\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.13635918498039246\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12995633482933044\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.07429099082946777\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.13740114867687225\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20459121465682983\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1282656192779541\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14140895009040833\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1422383040189743\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11648783832788467\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15149559080600739\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1187417134642601\n",
      "train accuracy:  0.84375\n",
      "eval batch loss:  0.3075079023838043\n",
      "eval accuracy:  0.625\n",
      "Epoch: 02, Train Loss: 0.151, Train Acc: 82.25%, Val. Loss: 0.510, Val. Acc: 57.73%\n",
      "time duration:     25.40062351152301\n",
      "train batch loss:  0.1122535765171051\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10223324596881866\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11070314794778824\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10728736966848373\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14126478135585785\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1663207709789276\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12232710421085358\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.18389657139778137\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.10772503167390823\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13162730634212494\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.18064990639686584\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1591484248638153\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.25221556425094604\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 03, Train Loss: 0.145, Train Acc: 82.76%, Val. Loss: 0.541, Val. Acc: 56.61%\n",
      "time duration:     25.103133633732796\n",
      "train batch loss:  0.12758247554302216\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15407942235469818\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13486739993095398\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16016799211502075\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12121614813804626\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.12405042350292206\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12796196341514587\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.23633305728435516\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.13986171782016754\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.15489250421524048\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14554467797279358\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10582242161035538\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.2774674892425537\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 04, Train Loss: 0.141, Train Acc: 83.46%, Val. Loss: 0.525, Val. Acc: 57.45%\n",
      "time duration:     25.7534831892699\n",
      "train batch loss:  0.1239708736538887\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.15596427023410797\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09944826364517212\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14408040046691895\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14907243847846985\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15860150754451752\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17438122630119324\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13765856623649597\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.07140535116195679\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.13554328680038452\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.09032365679740906\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11365920305252075\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.2790752351284027\n",
      "eval accuracy:  0.625\n",
      "Epoch: 05, Train Loss: 0.137, Train Acc: 84.08%, Val. Loss: 0.545, Val. Acc: 56.74%\n",
      "time duration:     25.28571587614715\n",
      "train batch loss:  0.10618016868829727\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1069730594754219\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1286173164844513\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13215193152427673\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.14350378513336182\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.15112468600273132\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11872263252735138\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14371153712272644\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11761215329170227\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.14644679427146912\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15647943317890167\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12203420698642731\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.28084707260131836\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 06, Train Loss: 0.132, Train Acc: 84.67%, Val. Loss: 0.595, Val. Acc: 55.68%\n",
      "time duration:     25.43738849274814\n",
      "train batch loss:  0.16578629612922668\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.08320768922567368\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.11615464091300964\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10532905161380768\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11134596914052963\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10686173290014267\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10312169790267944\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14436312019824982\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.09235470741987228\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.13699787855148315\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.07932602614164352\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.20318138599395752\n",
      "train accuracy:  0.765625\n",
      "eval batch loss:  0.2530730366706848\n",
      "eval accuracy:  0.6875\n",
      "Epoch: 07, Train Loss: 0.128, Train Acc: 85.11%, Val. Loss: 0.551, Val. Acc: 56.58%\n",
      "time duration:     25.902790147811174\n",
      "train batch loss:  0.11828534305095673\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11904274672269821\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.07762343436479568\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.1144227683544159\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15159094333648682\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13282155990600586\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1392294317483902\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09452147036790848\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11939692497253418\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10508261620998383\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.20180314779281616\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18045157194137573\n",
      "train accuracy:  0.8125\n",
      "eval batch loss:  0.26782411336898804\n",
      "eval accuracy:  0.6875\n",
      "Epoch: 08, Train Loss: 0.124, Train Acc: 85.68%, Val. Loss: 0.554, Val. Acc: 56.78%\n",
      "time duration:     25.47659025155008\n",
      "train batch loss:  0.08993162959814072\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1320268213748932\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.07841067016124725\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.14351241290569305\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.100274458527565\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09030941128730774\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08401330560445786\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1229608803987503\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1163458377122879\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1514224410057068\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1246795654296875\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10892891883850098\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.25255799293518066\n",
      "eval accuracy:  0.71875\n",
      "Epoch: 09, Train Loss: 0.120, Train Acc: 86.10%, Val. Loss: 0.550, Val. Acc: 57.78%\n",
      "time duration:     25.389412857592106\n",
      "train batch loss:  0.1612427532672882\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.08219469338655472\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.12165699899196625\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.12156271934509277\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13106897473335266\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12960462272167206\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11221606284379959\n",
      "train accuracy:  0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.13128796219825745\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08260514587163925\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.16039136052131653\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12517400085926056\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10918809473514557\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.2968624532222748\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 10, Train Loss: 0.116, Train Acc: 86.63%, Val. Loss: 0.591, Val. Acc: 55.81%\n",
      "time duration:     25.338993368670344\n",
      "train batch loss:  0.061482369899749756\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.1354914903640747\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09003391861915588\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.09464041888713837\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13632391393184662\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.07250578701496124\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.11342155188322067\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1362454891204834\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10543219745159149\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10597449541091919\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.12866413593292236\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10723155736923218\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.267306923866272\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 11, Train Loss: 0.113, Train Acc: 87.12%, Val. Loss: 0.535, Val. Acc: 57.04%\n",
      "time duration:     25.467918995767832\n",
      "train batch loss:  0.15743643045425415\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11790571361780167\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1085711196064949\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12390973418951035\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12319391220808029\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14009778201580048\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08712689578533173\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10222247987985611\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10656730830669403\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.15120375156402588\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.08970072865486145\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13809746503829956\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.2920202314853668\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 12, Train Loss: 0.110, Train Acc: 87.48%, Val. Loss: 0.533, Val. Acc: 57.51%\n",
      "time duration:     25.15811819769442\n",
      "train batch loss:  0.0720081552863121\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.14056643843650818\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11044375598430634\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09055610001087189\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.08497126400470734\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.09770901501178741\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10444628447294235\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16563662886619568\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13365322351455688\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13382785022258759\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11008889973163605\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09986092150211334\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.28083914518356323\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 13, Train Loss: 0.106, Train Acc: 88.02%, Val. Loss: 0.529, Val. Acc: 57.04%\n",
      "time duration:     24.953708542510867\n",
      "train batch loss:  0.14164389669895172\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11077050864696503\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0679713785648346\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.12393052875995636\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11691796779632568\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.08267351984977722\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11432894319295883\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.07592569291591644\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1242150291800499\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15371078252792358\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.08631265163421631\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.0921546071767807\n",
      "train accuracy:  0.90625\n",
      "eval batch loss:  0.26618486642837524\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 14, Train Loss: 0.104, Train Acc: 88.32%, Val. Loss: 0.565, Val. Acc: 56.77%\n",
      "time duration:     24.98969181999564\n",
      "train batch loss:  0.07828226685523987\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.08664388209581375\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11493781954050064\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10872860252857208\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.09209176898002625\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10465212166309357\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10631318390369415\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1017700582742691\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1104891449213028\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12081097811460495\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08494026958942413\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.09830732643604279\n",
      "train accuracy:  0.921875\n",
      "eval batch loss:  0.2884519100189209\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 15, Train Loss: 0.101, Train Acc: 88.76%, Val. Loss: 0.494, Val. Acc: 58.33%\n",
      "time duration:     25.110577126964927\n",
      "train batch loss:  0.11516239494085312\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10849422961473465\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.09193804860115051\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10950981825590134\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0987497866153717\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.06132793799042702\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.11373674124479294\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09899726510047913\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14112085103988647\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11409541964530945\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1507713794708252\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11321023851633072\n",
      "train accuracy:  0.953125\n",
      "eval batch loss:  0.32027149200439453\n",
      "eval accuracy:  0.625\n",
      "Epoch: 16, Train Loss: 0.100, Train Acc: 88.99%, Val. Loss: 0.480, Val. Acc: 58.72%\n",
      "time duration:     25.49692130088806\n",
      "train batch loss:  0.06659143418073654\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.06795438379049301\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.06979551911354065\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.1271030306816101\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09979960322380066\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09431696683168411\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11852049827575684\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08398833125829697\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1019236221909523\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.08963403105735779\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.07707688957452774\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07504710555076599\n",
      "train accuracy:  0.90625\n",
      "eval batch loss:  0.2868533134460449\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 17, Train Loss: 0.096, Train Acc: 89.57%, Val. Loss: 0.540, Val. Acc: 57.53%\n",
      "time duration:     25.49270787090063\n",
      "train batch loss:  0.18794988095760345\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.09498471021652222\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.09664079546928406\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1176246851682663\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13388901948928833\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08369410783052444\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.10795664042234421\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10884302854537964\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.06417082250118256\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.08346642553806305\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.15058501064777374\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1137852594256401\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.29181596636772156\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 18, Train Loss: 0.094, Train Acc: 89.80%, Val. Loss: 0.512, Val. Acc: 58.17%\n",
      "time duration:     25.08346811681986\n",
      "train batch loss:  0.0748818963766098\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.09356697648763657\n",
      "train accuracy:  0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.09135754406452179\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.06334997713565826\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.08926304429769516\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11374905705451965\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10224299132823944\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06145993620157242\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.14335931837558746\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08616364002227783\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.08936581015586853\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08214139938354492\n",
      "train accuracy:  0.953125\n",
      "eval batch loss:  0.27347445487976074\n",
      "eval accuracy:  0.6875\n",
      "Epoch: 19, Train Loss: 0.091, Train Acc: 90.21%, Val. Loss: 0.505, Val. Acc: 58.19%\n",
      "time duration:     25.454413456842303\n",
      "train batch loss:  0.0966172069311142\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06630294024944305\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.12014415860176086\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08515438437461853\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08699110150337219\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.054358139634132385\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.1231263279914856\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10964365303516388\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09449222683906555\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10996704548597336\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.060177505016326904\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.11254972964525223\n",
      "train accuracy:  0.84375\n",
      "eval batch loss:  0.2909539043903351\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 20, Train Loss: 0.089, Train Acc: 90.41%, Val. Loss: 0.535, Val. Acc: 57.67%\n",
      "time duration:     24.787584401667118\n",
      "train batch loss:  0.05823682248592377\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.08835285902023315\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07194781303405762\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.0726497545838356\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.10215696692466736\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08477213978767395\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07519993185997009\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.08920514583587646\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07423362880945206\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.12430447340011597\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08890342712402344\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08285780996084213\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.27917107939720154\n",
      "eval accuracy:  0.671875\n",
      "Epoch: 21, Train Loss: 0.086, Train Acc: 90.94%, Val. Loss: 0.508, Val. Acc: 58.54%\n",
      "time duration:     25.292828742414713\n",
      "train batch loss:  0.1226065456867218\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.08719554543495178\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.07749179005622864\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06862466037273407\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09766650199890137\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11849556863307953\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.07198579609394073\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.07319173961877823\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.06620147079229355\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.12899550795555115\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0941665917634964\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.051788579672575\n",
      "train accuracy:  0.96875\n",
      "eval batch loss:  0.26902881264686584\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 22, Train Loss: 0.084, Train Acc: 91.27%, Val. Loss: 0.513, Val. Acc: 58.33%\n",
      "time duration:     25.70067359507084\n",
      "train batch loss:  0.06694013625383377\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.08712077885866165\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.104977086186409\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07347364723682404\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.0960189700126648\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.09177811443805695\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.11073696613311768\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10268796235322952\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10248640179634094\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08607806265354156\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.12446609139442444\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09570324420928955\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.3000198304653168\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 23, Train Loss: 0.083, Train Acc: 91.41%, Val. Loss: 0.455, Val. Acc: 59.22%\n",
      "time duration:     25.28101424127817\n",
      "train batch loss:  0.08634965121746063\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06252601742744446\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.06277883052825928\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.12638942897319794\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.05099964141845703\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.0512571781873703\n",
      "train accuracy:  0.984375\n",
      "train batch loss:  0.04682303965091705\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.0942174643278122\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1298820972442627\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.08175259083509445\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.08649098873138428\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07787679880857468\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.3060569167137146\n",
      "eval accuracy:  0.625\n",
      "Epoch: 24, Train Loss: 0.080, Train Acc: 91.90%, Val. Loss: 0.469, Val. Acc: 59.11%\n",
      "time duration:     25.06617157906294\n",
      "train batch loss:  0.1195501908659935\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.0754106342792511\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.09690923988819122\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.06749996542930603\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.0648949071764946\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.04208474978804588\n",
      "train accuracy:  0.984375\n",
      "train batch loss:  0.08008578419685364\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.07420484721660614\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08620524406433105\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.09945645183324814\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.0817001685500145\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.0676952600479126\n",
      "train accuracy:  0.921875\n",
      "eval batch loss:  0.31349265575408936\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 25, Train Loss: 0.078, Train Acc: 92.11%, Val. Loss: 0.474, Val. Acc: 59.12%\n",
      "time duration:     25.03201407007873\n",
      "train batch loss:  0.08228843659162521\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.08375585824251175\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.08942015469074249\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07536061108112335\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.10900707542896271\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08910921216011047\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.057057756930589676\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.07161968946456909\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06811529397964478\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06819669902324677\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.08180371671915054\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07987460494041443\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.314655601978302\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 26, Train Loss: 0.077, Train Acc: 92.25%, Val. Loss: 0.464, Val. Acc: 59.28%\n",
      "time duration:     24.705167979002\n",
      "train batch loss:  0.06712351739406586\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06405195593833923\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.06787493824958801\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.06541068106889725\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.07472306489944458\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06606002151966095\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.062289655208587646\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.10305854678153992\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.07063671946525574\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.07546249777078629\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.0765255019068718\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08985050022602081\n",
      "train accuracy:  0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.319740355014801\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 27, Train Loss: 0.075, Train Acc: 92.58%, Val. Loss: 0.469, Val. Acc: 59.24%\n",
      "time duration:     25.40863990597427\n",
      "train batch loss:  0.0563834123313427\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.09224729239940643\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08285953104496002\n",
      "train accuracy:  0.984375\n",
      "train batch loss:  0.06493589282035828\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.08963499218225479\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.06346846371889114\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.05770495533943176\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06878219544887543\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.05998744070529938\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06126087158918381\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.09708655625581741\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11575930565595627\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.31994515657424927\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 28, Train Loss: 0.073, Train Acc: 92.85%, Val. Loss: 0.473, Val. Acc: 59.02%\n",
      "time duration:     25.07441819831729\n",
      "train batch loss:  0.08089642971754074\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10293205082416534\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07712084800004959\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.05766105279326439\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.06587043404579163\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.05874525010585785\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.13785505294799805\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.061417106539011\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06012517958879471\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.07883735001087189\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06860537827014923\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.07331911474466324\n",
      "train accuracy:  0.953125\n",
      "eval batch loss:  0.3026772737503052\n",
      "eval accuracy:  0.625\n",
      "Epoch: 29, Train Loss: 0.072, Train Acc: 93.04%, Val. Loss: 0.492, Val. Acc: 58.71%\n",
      "time duration:     24.914223082363605\n",
      "train batch loss:  0.07160712778568268\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.07091152667999268\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.044885214418172836\n",
      "train accuracy:  0.984375\n",
      "train batch loss:  0.06915539503097534\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.10196778178215027\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07525844126939774\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06273693591356277\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.04936406761407852\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.05468829348683357\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.06362901628017426\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.11248309165239334\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.050907157361507416\n",
      "train accuracy:  0.984375\n",
      "eval batch loss:  0.3126027286052704\n",
      "eval accuracy:  0.609375\n",
      "Epoch: 30, Train Loss: 0.070, Train Acc: 93.46%, Val. Loss: 0.509, Val. Acc: 57.77%\n",
      "time duration:     25.140009105205536\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Beautymodel, Beautytrain_iterator, Beautyoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Beautymodel, Beautyvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6224201917648315\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.802098274230957\n",
      "train accuracy:  0.453125\n",
      "train batch loss:  0.5952103137969971\n",
      "train accuracy:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.746, Train Acc: 52.95%, Val. Loss: 1.565, Val. Acc: 23.18%\n",
      "time duration:     6.969217656180263\n",
      "train batch loss:  0.4364822804927826\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.38269394636154175\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.4192928373813629\n",
      "train accuracy:  0.609375\n",
      "Epoch: 02, Train Loss: 0.468, Train Acc: 60.34%, Val. Loss: 1.353, Val. Acc: 27.76%\n",
      "time duration:     7.045009545981884\n",
      "train batch loss:  0.38159358501434326\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.3127586245536804\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.40788185596466064\n",
      "train accuracy:  0.640625\n",
      "Epoch: 03, Train Loss: 0.404, Train Acc: 62.85%, Val. Loss: 1.375, Val. Acc: 26.98%\n",
      "time duration:     7.060797564685345\n",
      "train batch loss:  0.2884298264980316\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.39535561203956604\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3794962763786316\n",
      "train accuracy:  0.59375\n",
      "Epoch: 04, Train Loss: 0.367, Train Acc: 64.84%, Val. Loss: 1.285, Val. Acc: 28.78%\n",
      "time duration:     7.169064961373806\n",
      "train batch loss:  0.5038593411445618\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.28045904636383057\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.23415371775627136\n",
      "train accuracy:  0.734375\n",
      "Epoch: 05, Train Loss: 0.332, Train Acc: 66.78%, Val. Loss: 1.169, Val. Acc: 32.11%\n",
      "time duration:     7.036950599402189\n",
      "train batch loss:  0.282617449760437\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.23762726783752441\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3225122392177582\n",
      "train accuracy:  0.6875\n",
      "Epoch: 06, Train Loss: 0.312, Train Acc: 68.02%, Val. Loss: 1.169, Val. Acc: 32.62%\n",
      "time duration:     6.9603667836636305\n",
      "train batch loss:  0.23982024192810059\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.26323723793029785\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3960646390914917\n",
      "train accuracy:  0.6875\n",
      "Epoch: 07, Train Loss: 0.294, Train Acc: 69.11%, Val. Loss: 1.160, Val. Acc: 33.05%\n",
      "time duration:     7.067764591425657\n",
      "train batch loss:  0.1854654997587204\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.27791503071784973\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.16130441427230835\n",
      "train accuracy:  0.75\n",
      "Epoch: 08, Train Loss: 0.275, Train Acc: 70.37%, Val. Loss: 1.136, Val. Acc: 33.51%\n",
      "time duration:     6.941379809752107\n",
      "train batch loss:  0.16059011220932007\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.22132468223571777\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2607603371143341\n",
      "train accuracy:  0.671875\n",
      "Epoch: 09, Train Loss: 0.266, Train Acc: 70.97%, Val. Loss: 0.917, Val. Acc: 38.77%\n",
      "time duration:     6.95358569547534\n",
      "train batch loss:  0.16359351575374603\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26533153653144836\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2745116949081421\n",
      "train accuracy:  0.6875\n",
      "Epoch: 10, Train Loss: 0.249, Train Acc: 72.60%, Val. Loss: 0.969, Val. Acc: 37.18%\n",
      "time duration:     6.925610478967428\n",
      "train batch loss:  0.18533705174922943\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26420068740844727\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2786579430103302\n",
      "train accuracy:  0.65625\n",
      "Epoch: 11, Train Loss: 0.239, Train Acc: 73.12%, Val. Loss: 0.985, Val. Acc: 36.80%\n",
      "time duration:     7.024179022759199\n",
      "train batch loss:  0.1931881308555603\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18775230646133423\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.30244043469429016\n",
      "train accuracy:  0.671875\n",
      "Epoch: 12, Train Loss: 0.224, Train Acc: 74.57%, Val. Loss: 0.909, Val. Acc: 39.06%\n",
      "time duration:     7.191507386043668\n",
      "train batch loss:  0.2596341371536255\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2955419421195984\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17963498830795288\n",
      "train accuracy:  0.765625\n",
      "Epoch: 13, Train Loss: 0.216, Train Acc: 75.22%, Val. Loss: 0.873, Val. Acc: 40.33%\n",
      "time duration:     7.206931225955486\n",
      "train batch loss:  0.19869181513786316\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.23800987005233765\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24555666744709015\n",
      "train accuracy:  0.796875\n",
      "Epoch: 14, Train Loss: 0.207, Train Acc: 76.35%, Val. Loss: 0.906, Val. Acc: 39.44%\n",
      "time duration:     7.042493686079979\n",
      "train batch loss:  0.3045065104961395\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.19244670867919922\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13386216759681702\n",
      "train accuracy:  0.828125\n",
      "Epoch: 15, Train Loss: 0.201, Train Acc: 76.95%, Val. Loss: 0.895, Val. Acc: 40.56%\n",
      "time duration:     7.157511921599507\n",
      "train batch loss:  0.1938907653093338\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17468127608299255\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.13774023950099945\n",
      "train accuracy:  0.84375\n",
      "Epoch: 16, Train Loss: 0.191, Train Acc: 77.62%, Val. Loss: 0.857, Val. Acc: 41.54%\n",
      "time duration:     7.003008171916008\n",
      "train batch loss:  0.21545109152793884\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.12677517533302307\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.23793745040893555\n",
      "train accuracy:  0.703125\n",
      "Epoch: 17, Train Loss: 0.185, Train Acc: 78.26%, Val. Loss: 0.904, Val. Acc: 40.25%\n",
      "time duration:     6.907631943002343\n",
      "train batch loss:  0.21036043763160706\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16355863213539124\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20978696644306183\n",
      "train accuracy:  0.765625\n",
      "Epoch: 18, Train Loss: 0.180, Train Acc: 78.89%, Val. Loss: 0.588, Val. Acc: 51.72%\n",
      "time duration:     7.087857479229569\n",
      "train batch loss:  0.1773347705602646\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2018263339996338\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.195256307721138\n",
      "train accuracy:  0.796875\n",
      "Epoch: 19, Train Loss: 0.174, Train Acc: 79.57%, Val. Loss: 0.605, Val. Acc: 50.95%\n",
      "time duration:     7.0304371025413275\n",
      "train batch loss:  0.19687888026237488\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2086147964000702\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17980223894119263\n",
      "train accuracy:  0.78125\n",
      "Epoch: 20, Train Loss: 0.166, Train Acc: 80.05%, Val. Loss: 0.601, Val. Acc: 50.58%\n",
      "time duration:     7.0354191437363625\n",
      "train batch loss:  0.12306287884712219\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1353015899658203\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13985112309455872\n",
      "train accuracy:  0.859375\n",
      "Epoch: 21, Train Loss: 0.161, Train Acc: 80.74%, Val. Loss: 0.620, Val. Acc: 49.45%\n",
      "time duration:     7.006889773532748\n",
      "train batch loss:  0.08765989542007446\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.12879197299480438\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.12831994891166687\n",
      "train accuracy:  0.859375\n",
      "Epoch: 22, Train Loss: 0.158, Train Acc: 81.26%, Val. Loss: 0.625, Val. Acc: 50.20%\n",
      "time duration:     7.01834668032825\n",
      "train batch loss:  0.22407753765583038\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.15187054872512817\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16003474593162537\n",
      "train accuracy:  0.84375\n",
      "Epoch: 23, Train Loss: 0.151, Train Acc: 81.83%, Val. Loss: 0.639, Val. Acc: 49.02%\n",
      "time duration:     6.868284560739994\n",
      "train batch loss:  0.09484873712062836\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1574854552745819\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.11123593151569366\n",
      "train accuracy:  0.859375\n",
      "Epoch: 24, Train Loss: 0.148, Train Acc: 82.33%, Val. Loss: 0.640, Val. Acc: 48.72%\n",
      "time duration:     7.094926016405225\n",
      "train batch loss:  0.11520734429359436\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15243496000766754\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1675816774368286\n",
      "train accuracy:  0.859375\n",
      "Epoch: 25, Train Loss: 0.144, Train Acc: 82.65%, Val. Loss: 0.616, Val. Acc: 50.67%\n",
      "time duration:     7.080058820545673\n",
      "train batch loss:  0.11273486167192459\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.16287440061569214\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1554722785949707\n",
      "train accuracy:  0.796875\n",
      "Epoch: 26, Train Loss: 0.139, Train Acc: 83.37%, Val. Loss: 0.664, Val. Acc: 48.27%\n",
      "time duration:     7.009847281500697\n",
      "train batch loss:  0.12465989589691162\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.17125752568244934\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.13455505669116974\n",
      "train accuracy:  0.84375\n",
      "Epoch: 27, Train Loss: 0.139, Train Acc: 83.26%, Val. Loss: 0.642, Val. Acc: 49.42%\n",
      "time duration:     7.190821401774883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.14477793872356415\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1033790111541748\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12290714681148529\n",
      "train accuracy:  0.84375\n",
      "Epoch: 28, Train Loss: 0.131, Train Acc: 84.30%, Val. Loss: 0.657, Val. Acc: 48.53%\n",
      "time duration:     6.913124153390527\n",
      "train batch loss:  0.06283252686262131\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.07880149781703949\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.16976456344127655\n",
      "train accuracy:  0.8125\n",
      "Epoch: 29, Train Loss: 0.127, Train Acc: 84.81%, Val. Loss: 0.638, Val. Acc: 49.91%\n",
      "time duration:     6.996437193825841\n",
      "train batch loss:  0.10650628805160522\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11790578067302704\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12161348015069962\n",
      "train accuracy:  0.890625\n",
      "Epoch: 30, Train Loss: 0.124, Train Acc: 85.47%, Val. Loss: 0.654, Val. Acc: 49.43%\n",
      "time duration:     7.198407106101513\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Apparelmodel, Appareltrain_iterator, Appareloptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Apparelmodel, Apparelvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6301746368408203\n",
      "train accuracy:  0.546875\n",
      "train batch loss:  0.5328851342201233\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5159908533096313\n",
      "train accuracy:  0.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.736, Train Acc: 52.70%, Val. Loss: 1.471, Val. Acc: 24.63%\n",
      "time duration:     7.055991502478719\n",
      "train batch loss:  0.3635804057121277\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.5165541172027588\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.4380553662776947\n",
      "train accuracy:  0.65625\n",
      "Epoch: 02, Train Loss: 0.476, Train Acc: 59.88%, Val. Loss: 1.361, Val. Acc: 26.13%\n",
      "time duration:     6.959439035505056\n",
      "train batch loss:  0.4171964228153229\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.46422961354255676\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.37208107113838196\n",
      "train accuracy:  0.578125\n",
      "Epoch: 03, Train Loss: 0.412, Train Acc: 62.86%, Val. Loss: 1.288, Val. Acc: 28.27%\n",
      "time duration:     7.042623616755009\n",
      "train batch loss:  0.3214538097381592\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.2609157860279083\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6514191031455994\n",
      "train accuracy:  0.59375\n",
      "Epoch: 04, Train Loss: 0.370, Train Acc: 64.37%, Val. Loss: 1.208, Val. Acc: 30.76%\n",
      "time duration:     7.079696686938405\n",
      "train batch loss:  0.3982483148574829\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.24290479719638824\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.3700544238090515\n",
      "train accuracy:  0.59375\n",
      "Epoch: 05, Train Loss: 0.338, Train Acc: 66.58%, Val. Loss: 1.147, Val. Acc: 33.19%\n",
      "time duration:     6.929377222433686\n",
      "train batch loss:  0.23791006207466125\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2953246235847473\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.46553388237953186\n",
      "train accuracy:  0.625\n",
      "Epoch: 06, Train Loss: 0.320, Train Acc: 67.25%, Val. Loss: 1.146, Val. Acc: 33.59%\n",
      "time duration:     7.155843965709209\n",
      "train batch loss:  0.3510676920413971\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.22722238302230835\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2581310272216797\n",
      "train accuracy:  0.734375\n",
      "Epoch: 07, Train Loss: 0.294, Train Acc: 68.55%, Val. Loss: 1.061, Val. Acc: 36.11%\n",
      "time duration:     7.09697887301445\n",
      "train batch loss:  0.3038342595100403\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.20107883214950562\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1628284752368927\n",
      "train accuracy:  0.78125\n",
      "Epoch: 08, Train Loss: 0.279, Train Acc: 69.96%, Val. Loss: 0.812, Val. Acc: 42.24%\n",
      "time duration:     7.1933942418545485\n",
      "train batch loss:  0.29842162132263184\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.21585974097251892\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.3215491473674774\n",
      "train accuracy:  0.6875\n",
      "Epoch: 09, Train Loss: 0.268, Train Acc: 70.67%, Val. Loss: 0.821, Val. Acc: 41.37%\n",
      "time duration:     7.009547187015414\n",
      "train batch loss:  0.21554060280323029\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3232094943523407\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.279538631439209\n",
      "train accuracy:  0.703125\n",
      "Epoch: 10, Train Loss: 0.253, Train Acc: 72.03%, Val. Loss: 0.793, Val. Acc: 43.33%\n",
      "time duration:     7.065599853172898\n",
      "train batch loss:  0.21874433755874634\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24026259779930115\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2226664274930954\n",
      "train accuracy:  0.796875\n",
      "Epoch: 11, Train Loss: 0.240, Train Acc: 72.94%, Val. Loss: 0.806, Val. Acc: 43.35%\n",
      "time duration:     7.05329979211092\n",
      "train batch loss:  0.1911318302154541\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.17112916707992554\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16372573375701904\n",
      "train accuracy:  0.828125\n",
      "Epoch: 12, Train Loss: 0.230, Train Acc: 74.05%, Val. Loss: 0.805, Val. Acc: 43.12%\n",
      "time duration:     6.944287242367864\n",
      "train batch loss:  0.1913425326347351\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2759484052658081\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.31089532375335693\n",
      "train accuracy:  0.703125\n",
      "Epoch: 13, Train Loss: 0.224, Train Acc: 74.35%, Val. Loss: 0.811, Val. Acc: 42.17%\n",
      "time duration:     6.941960355266929\n",
      "train batch loss:  0.22646404802799225\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2850068211555481\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.22233939170837402\n",
      "train accuracy:  0.734375\n",
      "Epoch: 14, Train Loss: 0.218, Train Acc: 75.16%, Val. Loss: 0.787, Val. Acc: 43.67%\n",
      "time duration:     7.089880490675569\n",
      "train batch loss:  0.16704310476779938\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15088731050491333\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.17034661769866943\n",
      "train accuracy:  0.78125\n",
      "Epoch: 15, Train Loss: 0.203, Train Acc: 76.23%, Val. Loss: 0.788, Val. Acc: 43.50%\n",
      "time duration:     7.120376879349351\n",
      "train batch loss:  0.17691364884376526\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.33264005184173584\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.12714746594429016\n",
      "train accuracy:  0.828125\n",
      "Epoch: 16, Train Loss: 0.200, Train Acc: 76.60%, Val. Loss: 0.734, Val. Acc: 46.09%\n",
      "time duration:     7.0812922567129135\n",
      "train batch loss:  0.17115893959999084\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15934231877326965\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.22250281274318695\n",
      "train accuracy:  0.71875\n",
      "Epoch: 17, Train Loss: 0.192, Train Acc: 77.75%, Val. Loss: 0.766, Val. Acc: 44.85%\n",
      "time duration:     7.071014024317265\n",
      "train batch loss:  0.16293008625507355\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2380652129650116\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13764357566833496\n",
      "train accuracy:  0.828125\n",
      "Epoch: 18, Train Loss: 0.186, Train Acc: 78.18%, Val. Loss: 0.766, Val. Acc: 44.97%\n",
      "time duration:     7.102266199886799\n",
      "train batch loss:  0.19868098199367523\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.13011570274829865\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1979202926158905\n",
      "train accuracy:  0.8125\n",
      "Epoch: 19, Train Loss: 0.179, Train Acc: 78.58%, Val. Loss: 0.744, Val. Acc: 45.48%\n",
      "time duration:     7.0006515718996525\n",
      "train batch loss:  0.12788036465644836\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13932660222053528\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20405030250549316\n",
      "train accuracy:  0.765625\n",
      "Epoch: 20, Train Loss: 0.173, Train Acc: 79.51%, Val. Loss: 0.750, Val. Acc: 45.58%\n",
      "time duration:     7.0070111360400915\n",
      "train batch loss:  0.2138698548078537\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18553224205970764\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.14288330078125\n",
      "train accuracy:  0.890625\n",
      "Epoch: 21, Train Loss: 0.166, Train Acc: 80.40%, Val. Loss: 0.770, Val. Acc: 44.89%\n",
      "time duration:     7.118676407262683\n",
      "train batch loss:  0.18572841584682465\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1548662632703781\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10996104031801224\n",
      "train accuracy:  0.875\n",
      "Epoch: 22, Train Loss: 0.162, Train Acc: 80.55%, Val. Loss: 0.738, Val. Acc: 46.97%\n",
      "time duration:     7.02189483307302\n",
      "train batch loss:  0.21218469738960266\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.138429194688797\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17937515676021576\n",
      "train accuracy:  0.828125\n",
      "Epoch: 23, Train Loss: 0.158, Train Acc: 81.33%, Val. Loss: 0.742, Val. Acc: 46.85%\n",
      "time duration:     6.954452730715275\n",
      "train batch loss:  0.1506185233592987\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1497245728969574\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10552780330181122\n",
      "train accuracy:  0.890625\n",
      "Epoch: 24, Train Loss: 0.153, Train Acc: 81.77%, Val. Loss: 0.724, Val. Acc: 47.18%\n",
      "time duration:     7.162176324054599\n",
      "train batch loss:  0.14145052433013916\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.10211221128702164\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14551305770874023\n",
      "train accuracy:  0.78125\n",
      "Epoch: 25, Train Loss: 0.147, Train Acc: 82.32%, Val. Loss: 0.745, Val. Acc: 46.81%\n",
      "time duration:     6.996334947645664\n",
      "train batch loss:  0.13512006402015686\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10571801662445068\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13385792076587677\n",
      "train accuracy:  0.828125\n",
      "Epoch: 26, Train Loss: 0.145, Train Acc: 82.64%, Val. Loss: 0.745, Val. Acc: 47.16%\n",
      "time duration:     7.1527389623224735\n",
      "train batch loss:  0.12898102402687073\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12428285926580429\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.22422535717487335\n",
      "train accuracy:  0.71875\n",
      "Epoch: 27, Train Loss: 0.139, Train Acc: 83.17%, Val. Loss: 0.758, Val. Acc: 46.45%\n",
      "time duration:     7.039394907653332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.18968465924263\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1133405864238739\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1250794678926468\n",
      "train accuracy:  0.8125\n",
      "Epoch: 28, Train Loss: 0.137, Train Acc: 83.62%, Val. Loss: 0.754, Val. Acc: 46.52%\n",
      "time duration:     7.172634623944759\n",
      "train batch loss:  0.1180628165602684\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.07846075296401978\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.15739606320858002\n",
      "train accuracy:  0.78125\n",
      "Epoch: 29, Train Loss: 0.134, Train Acc: 83.96%, Val. Loss: 0.709, Val. Acc: 49.24%\n",
      "time duration:     7.107526823878288\n",
      "train batch loss:  0.08139052242040634\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.17068910598754883\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17654576897621155\n",
      "train accuracy:  0.796875\n",
      "Epoch: 30, Train Loss: 0.130, Train Acc: 84.48%, Val. Loss: 0.720, Val. Acc: 47.65%\n",
      "time duration:     7.125024737790227\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Jewelrymodel, Jewelrytrain_iterator, Jewelryoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Jewelrymodel, Jewelryvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.5651078224182129\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.6628104448318481\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.48524707555770874\n",
      "train accuracy:  0.703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.733, Train Acc: 52.95%, Val. Loss: 1.553, Val. Acc: 23.91%\n",
      "time duration:     7.042408537119627\n",
      "train batch loss:  0.3780459761619568\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.6475535035133362\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.6203875541687012\n",
      "train accuracy:  0.5625\n",
      "Epoch: 02, Train Loss: 0.461, Train Acc: 60.32%, Val. Loss: 1.382, Val. Acc: 26.90%\n",
      "time duration:     7.049519123509526\n",
      "train batch loss:  0.6357086896896362\n",
      "train accuracy:  0.546875\n",
      "train batch loss:  0.3618670701980591\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.44358542561531067\n",
      "train accuracy:  0.546875\n",
      "Epoch: 03, Train Loss: 0.407, Train Acc: 63.02%, Val. Loss: 1.422, Val. Acc: 25.61%\n",
      "time duration:     7.102396998554468\n",
      "train batch loss:  0.22764535248279572\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.40884461998939514\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.36230742931365967\n",
      "train accuracy:  0.65625\n",
      "Epoch: 04, Train Loss: 0.362, Train Acc: 64.98%, Val. Loss: 1.139, Val. Acc: 33.51%\n",
      "time duration:     6.959543919190764\n",
      "train batch loss:  0.37136924266815186\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3565632104873657\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.30291175842285156\n",
      "train accuracy:  0.65625\n",
      "Epoch: 05, Train Loss: 0.338, Train Acc: 66.26%, Val. Loss: 1.349, Val. Acc: 26.67%\n",
      "time duration:     6.973251538351178\n",
      "train batch loss:  0.2188396453857422\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3015994429588318\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.36997899413108826\n",
      "train accuracy:  0.625\n",
      "Epoch: 06, Train Loss: 0.311, Train Acc: 68.08%, Val. Loss: 1.235, Val. Acc: 29.80%\n",
      "time duration:     6.897506777197123\n",
      "train batch loss:  0.19611819088459015\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1667097806930542\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.29562005400657654\n",
      "train accuracy:  0.609375\n",
      "Epoch: 07, Train Loss: 0.289, Train Acc: 69.17%, Val. Loss: 1.144, Val. Acc: 33.01%\n",
      "time duration:     7.101320987567306\n",
      "train batch loss:  0.24811673164367676\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.25623685121536255\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2181093394756317\n",
      "train accuracy:  0.765625\n",
      "Epoch: 08, Train Loss: 0.276, Train Acc: 70.09%, Val. Loss: 1.130, Val. Acc: 32.85%\n",
      "time duration:     6.8337820414453745\n",
      "train batch loss:  0.3596344590187073\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24935486912727356\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.39382246136665344\n",
      "train accuracy:  0.625\n",
      "Epoch: 09, Train Loss: 0.261, Train Acc: 71.22%, Val. Loss: 1.075, Val. Acc: 35.05%\n",
      "time duration:     7.097547007724643\n",
      "train batch loss:  0.23953455686569214\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2777099609375\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.25739726424217224\n",
      "train accuracy:  0.609375\n",
      "Epoch: 10, Train Loss: 0.253, Train Acc: 72.11%, Val. Loss: 1.039, Val. Acc: 35.54%\n",
      "time duration:     7.091578612104058\n",
      "train batch loss:  0.20661407709121704\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11991797387599945\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.23372521996498108\n",
      "train accuracy:  0.71875\n",
      "Epoch: 11, Train Loss: 0.234, Train Acc: 73.34%, Val. Loss: 0.932, Val. Acc: 39.45%\n",
      "time duration:     6.905920669436455\n",
      "train batch loss:  0.29781943559646606\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1373729407787323\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2052001953125\n",
      "train accuracy:  0.796875\n",
      "Epoch: 12, Train Loss: 0.227, Train Acc: 74.07%, Val. Loss: 0.919, Val. Acc: 39.71%\n",
      "time duration:     7.126864040270448\n",
      "train batch loss:  0.15664902329444885\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2833670377731323\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.22242948412895203\n",
      "train accuracy:  0.75\n",
      "Epoch: 13, Train Loss: 0.222, Train Acc: 74.80%, Val. Loss: 1.003, Val. Acc: 37.01%\n",
      "time duration:     7.25712120346725\n",
      "train batch loss:  0.17495301365852356\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.27402862906455994\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.2040557563304901\n",
      "train accuracy:  0.765625\n",
      "Epoch: 14, Train Loss: 0.209, Train Acc: 76.00%, Val. Loss: 0.945, Val. Acc: 38.87%\n",
      "time duration:     7.1392229199409485\n",
      "train batch loss:  0.21799318492412567\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.21742676198482513\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.19546973705291748\n",
      "train accuracy:  0.734375\n",
      "Epoch: 15, Train Loss: 0.201, Train Acc: 76.68%, Val. Loss: 0.985, Val. Acc: 37.60%\n",
      "time duration:     7.0121261309832335\n",
      "train batch loss:  0.2935558557510376\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.1712510734796524\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17687717080116272\n",
      "train accuracy:  0.71875\n",
      "Epoch: 16, Train Loss: 0.196, Train Acc: 76.70%, Val. Loss: 1.014, Val. Acc: 36.95%\n",
      "time duration:     7.073336079716682\n",
      "train batch loss:  0.1491485983133316\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.2609822154045105\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.20788709819316864\n",
      "train accuracy:  0.78125\n",
      "Epoch: 17, Train Loss: 0.190, Train Acc: 77.52%, Val. Loss: 0.798, Val. Acc: 43.84%\n",
      "time duration:     7.029212908819318\n",
      "train batch loss:  0.19422107934951782\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.20514121651649475\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1772075593471527\n",
      "train accuracy:  0.796875\n",
      "Epoch: 18, Train Loss: 0.181, Train Acc: 78.38%, Val. Loss: 0.837, Val. Acc: 43.00%\n",
      "time duration:     7.017511866986752\n",
      "train batch loss:  0.1683724820613861\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15408287942409515\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2168262004852295\n",
      "train accuracy:  0.75\n",
      "Epoch: 19, Train Loss: 0.176, Train Acc: 79.10%, Val. Loss: 0.807, Val. Acc: 43.84%\n",
      "time duration:     7.102894749492407\n",
      "train batch loss:  0.18958741426467896\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.17720291018486023\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.18691065907478333\n",
      "train accuracy:  0.78125\n",
      "Epoch: 20, Train Loss: 0.169, Train Acc: 79.56%, Val. Loss: 0.812, Val. Acc: 44.44%\n",
      "time duration:     6.932460691779852\n",
      "train batch loss:  0.17464089393615723\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13586165010929108\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15376366674900055\n",
      "train accuracy:  0.8125\n",
      "Epoch: 21, Train Loss: 0.163, Train Acc: 80.35%, Val. Loss: 0.813, Val. Acc: 44.24%\n",
      "time duration:     6.9236002042889595\n",
      "train batch loss:  0.14799818396568298\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13868111371994019\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14844268560409546\n",
      "train accuracy:  0.78125\n",
      "Epoch: 22, Train Loss: 0.159, Train Acc: 80.92%, Val. Loss: 0.815, Val. Acc: 43.79%\n",
      "time duration:     7.134209142997861\n",
      "train batch loss:  0.1463882327079773\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10751097649335861\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.19639888405799866\n",
      "train accuracy:  0.84375\n",
      "Epoch: 23, Train Loss: 0.155, Train Acc: 81.52%, Val. Loss: 0.806, Val. Acc: 43.96%\n",
      "time duration:     6.928561242297292\n",
      "train batch loss:  0.11583168804645538\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11904199421405792\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13765646517276764\n",
      "train accuracy:  0.859375\n",
      "Epoch: 24, Train Loss: 0.149, Train Acc: 82.10%, Val. Loss: 0.798, Val. Acc: 44.32%\n",
      "time duration:     6.963853623718023\n",
      "train batch loss:  0.1278616040945053\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16092711687088013\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12761393189430237\n",
      "train accuracy:  0.8125\n",
      "Epoch: 25, Train Loss: 0.144, Train Acc: 82.67%, Val. Loss: 0.775, Val. Acc: 45.38%\n",
      "time duration:     7.046182772144675\n",
      "train batch loss:  0.21210193634033203\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.14411179721355438\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11698581278324127\n",
      "train accuracy:  0.84375\n",
      "Epoch: 26, Train Loss: 0.139, Train Acc: 83.10%, Val. Loss: 0.825, Val. Acc: 44.17%\n",
      "time duration:     6.9500522296875715\n",
      "train batch loss:  0.13994446396827698\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13174736499786377\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14071011543273926\n",
      "train accuracy:  0.828125\n",
      "Epoch: 27, Train Loss: 0.137, Train Acc: 83.67%, Val. Loss: 0.796, Val. Acc: 45.04%\n",
      "time duration:     7.102817162871361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.13759784400463104\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11462140083312988\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09522995352745056\n",
      "train accuracy:  0.890625\n",
      "Epoch: 28, Train Loss: 0.133, Train Acc: 84.27%, Val. Loss: 0.769, Val. Acc: 45.96%\n",
      "time duration:     6.961299763992429\n",
      "train batch loss:  0.0892253965139389\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12651070952415466\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.146209716796875\n",
      "train accuracy:  0.84375\n",
      "Epoch: 29, Train Loss: 0.129, Train Acc: 84.59%, Val. Loss: 0.647, Val. Acc: 49.24%\n",
      "time duration:     6.958750203251839\n",
      "train batch loss:  0.0991404578089714\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11766912788152695\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1149306446313858\n",
      "train accuracy:  0.859375\n",
      "Epoch: 30, Train Loss: 0.128, Train Acc: 84.97%, Val. Loss: 0.658, Val. Acc: 48.76%\n",
      "time duration:     6.884521793574095\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Shoesmodel, Shoestrain_iterator, Shoesoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Shoesmodel, Shoesvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.5732278823852539\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.553144097328186\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.3743804097175598\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.5528630614280701\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.43953073024749756\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5726202726364136\n",
      "train accuracy:  0.515625\n",
      "train batch loss:  0.29325783252716064\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.4839208722114563\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4082096517086029\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5365681648254395\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.429838091135025\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.4333556890487671\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.4339776933193207\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.35442399978637695\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.30895447731018066\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3093678057193756\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4695914387702942\n",
      "train accuracy:  0.46875\n",
      "train batch loss:  0.2726195752620697\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.4016585946083069\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.44307851791381836\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.4496075510978699\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.38615211844444275\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.47693368792533875\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.252733051776886\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.4096713364124298\n",
      "train accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  1.3751654624938965\n",
      "eval accuracy:  0.109375\n",
      "eval batch loss:  0.4810355305671692\n",
      "eval accuracy:  0.5\n",
      "eval batch loss:  0.7392482757568359\n",
      "eval accuracy:  0.453125\n",
      "Epoch: 01, Train Loss: 0.478, Train Acc: 60.32%, Val. Loss: 1.334, Val. Acc: 35.56%\n",
      "time duration:     49.65726375207305\n",
      "train batch loss:  0.25881290435791016\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28375744819641113\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.29423654079437256\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.3405742049217224\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.40630850195884705\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3452374041080475\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.34945812821388245\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.25606805086135864\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.2698666453361511\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3322017192840576\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.31939631700515747\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.2997615933418274\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26634424924850464\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.23848098516464233\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.5090618133544922\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.2554721236228943\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26014605164527893\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.3548198342323303\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4009794294834137\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2092827558517456\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.3464515209197998\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.272676557302475\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.29110482335090637\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.23558750748634338\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.505536675453186\n",
      "train accuracy:  0.609375\n",
      "eval batch loss:  0.7382081747055054\n",
      "eval accuracy:  0.28125\n",
      "eval batch loss:  0.43985944986343384\n",
      "eval accuracy:  0.546875\n",
      "eval batch loss:  0.6845897436141968\n",
      "eval accuracy:  0.546875\n",
      "Epoch: 02, Train Loss: 0.333, Train Acc: 66.92%, Val. Loss: 0.955, Val. Acc: 42.67%\n",
      "time duration:     50.240052392706275\n",
      "train batch loss:  0.24606698751449585\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.36510589718818665\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.26917535066604614\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.25952979922294617\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1886070817708969\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.23813800513744354\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.31887567043304443\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.2659909129142761\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3622587323188782\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3693122863769531\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.2590721845626831\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.34296804666519165\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.44262486696243286\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.27497953176498413\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.27641910314559937\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.29681509733200073\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.44603657722473145\n",
      "train accuracy:  0.578125\n",
      "train batch loss:  0.28848928213119507\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.30577874183654785\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3628385066986084\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.18434715270996094\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.22466325759887695\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.17706066370010376\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2955694794654846\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.323574960231781\n",
      "train accuracy:  0.65625\n",
      "eval batch loss:  0.391230970621109\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.3854699730873108\n",
      "eval accuracy:  0.640625\n",
      "eval batch loss:  0.5125792026519775\n",
      "eval accuracy:  0.5\n",
      "Epoch: 03, Train Loss: 0.287, Train Acc: 69.72%, Val. Loss: 0.728, Val. Acc: 50.86%\n",
      "time duration:     50.00046772696078\n",
      "train batch loss:  0.38514843583106995\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2990209460258484\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2156245857477188\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18067950010299683\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2902643382549286\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.17377789318561554\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26906490325927734\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23902694880962372\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.26127949357032776\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.30540674924850464\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.30144625902175903\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3379902243614197\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.20636945962905884\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.25284168124198914\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24674898386001587\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2649852931499481\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.24118365347385406\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.35139209032058716\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.19575850665569305\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.22583651542663574\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.22604139149188995\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.284204363822937\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.350395143032074\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.25272828340530396\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.3190408945083618\n",
      "train accuracy:  0.671875\n",
      "eval batch loss:  0.4336157441139221\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.4164770245552063\n",
      "eval accuracy:  0.609375\n",
      "eval batch loss:  0.5193488597869873\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 04, Train Loss: 0.260, Train Acc: 71.77%, Val. Loss: 0.694, Val. Acc: 50.54%\n",
      "time duration:     49.988112453371286\n",
      "train batch loss:  0.22141964733600616\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2282545268535614\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.26496386528015137\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.19999991357326508\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2716826796531677\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2575126886367798\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24186968803405762\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26864075660705566\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.35121601819992065\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2202378809452057\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.21245382726192474\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22089219093322754\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.21655476093292236\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.25898072123527527\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.267076313495636\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2085699588060379\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22545279562473297\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.24547165632247925\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.31651341915130615\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.1934536099433899\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.14273351430892944\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.21969181299209595\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2347116768360138\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.23691719770431519\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.17469874024391174\n",
      "train accuracy:  0.8125\n",
      "eval batch loss:  0.34935277700424194\n",
      "eval accuracy:  0.703125\n",
      "eval batch loss:  0.388458788394928\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.4202626943588257\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 05, Train Loss: 0.241, Train Acc: 73.13%, Val. Loss: 0.611, Val. Acc: 54.61%\n",
      "time duration:     50.195252407342196\n",
      "train batch loss:  0.15656080842018127\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2798422574996948\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.3494267761707306\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.37027835845947266\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2516126036643982\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.23698870837688446\n",
      "train accuracy:  0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.24628780782222748\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.25828680396080017\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.25184354186058044\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.21931076049804688\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2553141117095947\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.24701836705207825\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1742476224899292\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.14521870017051697\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.3044028878211975\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.21051590144634247\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.21022284030914307\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3049008250236511\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.15801768004894257\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.23516519367694855\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.21710844337940216\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.30484023690223694\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.31870073080062866\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2489171028137207\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.21750615537166595\n",
      "train accuracy:  0.734375\n",
      "eval batch loss:  0.24593520164489746\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.35898351669311523\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.45305848121643066\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 06, Train Loss: 0.224, Train Acc: 74.41%, Val. Loss: 0.554, Val. Acc: 58.31%\n",
      "time duration:     49.193345725536346\n",
      "train batch loss:  0.23635390400886536\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1760191172361374\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.20131748914718628\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.15677353739738464\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.22226864099502563\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2213001400232315\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.20297878980636597\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.20897062122821808\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19666308164596558\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.20698556303977966\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.23179712891578674\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.23469066619873047\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13893459737300873\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.20867514610290527\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17209705710411072\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2218548059463501\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.16025583446025848\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.25237855315208435\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.17276184260845184\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.16443508863449097\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.27604424953460693\n",
      "train accuracy:  0.671875\n",
      "train batch loss:  0.2273971438407898\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18064124882221222\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2882046699523926\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.21402546763420105\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.23769058287143707\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.36535587906837463\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.40573951601982117\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 07, Train Loss: 0.212, Train Acc: 75.63%, Val. Loss: 0.537, Val. Acc: 59.56%\n",
      "time duration:     49.75636734068394\n",
      "train batch loss:  0.2457573264837265\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14004164934158325\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18569113314151764\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.17473801970481873\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.18085628747940063\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.1349993646144867\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15074771642684937\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.21113482117652893\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.31414180994033813\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14159725606441498\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.15705540776252747\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11195322126150131\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.255168080329895\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18052271008491516\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14699073135852814\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24067971110343933\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1439199447631836\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19037076830863953\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14414766430854797\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.17988190054893494\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17453646659851074\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18356940150260925\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.173078253865242\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.27038347721099854\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.15971985459327698\n",
      "train accuracy:  0.8125\n",
      "eval batch loss:  0.24028955399990082\n",
      "eval accuracy:  0.734375\n",
      "eval batch loss:  0.3456288278102875\n",
      "eval accuracy:  0.609375\n",
      "eval batch loss:  0.4211597442626953\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 08, Train Loss: 0.202, Train Acc: 76.57%, Val. Loss: 0.481, Val. Acc: 60.18%\n",
      "time duration:     49.64998617768288\n",
      "train batch loss:  0.1923503577709198\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.2009013146162033\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.22131800651550293\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1837538778781891\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12760426104068756\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.22185510396957397\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13440081477165222\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1638127863407135\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11463063210248947\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1494053602218628\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.22330127656459808\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1495186984539032\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19008798897266388\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1938330978155136\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.152740478515625\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.25649040937423706\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.20713002979755402\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.21555684506893158\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.11805039644241333\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.21373018622398376\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.2642682194709778\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.20712275803089142\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.18802383542060852\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.27060461044311523\n",
      "train accuracy:  0.609375\n",
      "train batch loss:  0.17197513580322266\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.2205306589603424\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.3673444390296936\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.43144315481185913\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 09, Train Loss: 0.193, Train Acc: 77.35%, Val. Loss: 0.491, Val. Acc: 61.10%\n",
      "time duration:     50.53104126639664\n",
      "train batch loss:  0.19664287567138672\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.19852185249328613\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.14948365092277527\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.2249516248703003\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19788599014282227\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2037467658519745\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.13548427820205688\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2370847761631012\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1350005716085434\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16711962223052979\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.25955456495285034\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13595211505889893\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15100593864917755\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.22753283381462097\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.23899367451667786\n",
      "train accuracy:  0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.17627328634262085\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19403547048568726\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15372586250305176\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.21217334270477295\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2320992797613144\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.20633676648139954\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2858677804470062\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.17241352796554565\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20999936759471893\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.21775123476982117\n",
      "train accuracy:  0.671875\n",
      "eval batch loss:  0.2163565456867218\n",
      "eval accuracy:  0.796875\n",
      "eval batch loss:  0.3243652284145355\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.43236467242240906\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 10, Train Loss: 0.186, Train Acc: 78.03%, Val. Loss: 0.475, Val. Acc: 62.84%\n",
      "time duration:     50.08746659196913\n",
      "train batch loss:  0.183902770280838\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18940751254558563\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.10234232246875763\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.15737947821617126\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.22076749801635742\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.18021054565906525\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1301792711019516\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14762459695339203\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1899152398109436\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1460483968257904\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17464300990104675\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.24797983467578888\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.21858397126197815\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.15894509851932526\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.2309005856513977\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1835251748561859\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13693158328533173\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1722276210784912\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.10281221568584442\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11021397262811661\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.18207570910453796\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1889638602733612\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.21627619862556458\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.14180108904838562\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16739746928215027\n",
      "train accuracy:  0.765625\n",
      "eval batch loss:  0.21479357779026031\n",
      "eval accuracy:  0.765625\n",
      "eval batch loss:  0.3396030068397522\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.4029659628868103\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 11, Train Loss: 0.179, Train Acc: 78.69%, Val. Loss: 0.519, Val. Acc: 62.31%\n",
      "time duration:     49.71745399571955\n",
      "train batch loss:  0.19438333809375763\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2054484784603119\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18814751505851746\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.16234704852104187\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12700101733207703\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12804138660430908\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17192354798316956\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12834292650222778\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.20541849732398987\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1296720951795578\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.19194047152996063\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1652294099330902\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26777884364128113\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.24388791620731354\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18071101605892181\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19327077269554138\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.12371022999286652\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.24455565214157104\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.16209152340888977\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.17041653394699097\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.09910756349563599\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1987493485212326\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.1591646373271942\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16491791605949402\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18329304456710815\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.23203055560588837\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.3526724576950073\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.3884868621826172\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 12, Train Loss: 0.172, Train Acc: 79.47%, Val. Loss: 0.467, Val. Acc: 63.00%\n",
      "time duration:     50.152280101552606\n",
      "train batch loss:  0.14720356464385986\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.21028244495391846\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.12362004071474075\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.19200661778450012\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1379033923149109\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.11895181238651276\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13848833739757538\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.17024461925029755\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1910013109445572\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.20304760336875916\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.18794892728328705\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.13825827836990356\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16196712851524353\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1337316632270813\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18404728174209595\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.18210706114768982\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1628585159778595\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15422269701957703\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.15984581410884857\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23798434436321259\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1282530277967453\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15146121382713318\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1460893601179123\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15782299637794495\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1638476550579071\n",
      "train accuracy:  0.765625\n",
      "eval batch loss:  0.18550673127174377\n",
      "eval accuracy:  0.765625\n",
      "eval batch loss:  0.3370177149772644\n",
      "eval accuracy:  0.609375\n",
      "eval batch loss:  0.38421696424484253\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 13, Train Loss: 0.166, Train Acc: 80.15%, Val. Loss: 0.451, Val. Acc: 63.59%\n",
      "time duration:     49.68380616419017\n",
      "train batch loss:  0.11328141391277313\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1748655140399933\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.14603659510612488\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16716605424880981\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.12650898098945618\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.17272256314754486\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.08900004625320435\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1318766176700592\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.18272113800048828\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.17132189869880676\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.09852442145347595\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.214547261595726\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.18317246437072754\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14172330498695374\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.170265793800354\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.10938100516796112\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1533941626548767\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12040209770202637\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.21699023246765137\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20214678347110748\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.15291252732276917\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1676199734210968\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.2527390420436859\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2590371370315552\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.16316837072372437\n",
      "train accuracy:  0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.2445169985294342\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.35206788778305054\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.3857034742832184\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 14, Train Loss: 0.160, Train Acc: 80.87%, Val. Loss: 0.458, Val. Acc: 62.57%\n",
      "time duration:     49.828183172270656\n",
      "train batch loss:  0.13005003333091736\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13616567850112915\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.15226948261260986\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1796751618385315\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.11798711121082306\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10637106746435165\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10889523476362228\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16244959831237793\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17386457324028015\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.158323734998703\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.12298546731472015\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1928214132785797\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.19964757561683655\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.10789395123720169\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.11382982134819031\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1690472811460495\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.13145646452903748\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.18221844732761383\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1556747853755951\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.11294762045145035\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11804530024528503\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14832325279712677\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.15755456686019897\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20775455236434937\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.14247946441173553\n",
      "train accuracy:  0.734375\n",
      "eval batch loss:  0.24755725264549255\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.36203789710998535\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.38660871982574463\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 15, Train Loss: 0.156, Train Acc: 81.33%, Val. Loss: 0.515, Val. Acc: 62.20%\n",
      "time duration:     49.85671414807439\n",
      "train batch loss:  0.11148229986429214\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10856913775205612\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11276320368051529\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1693829894065857\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11570176482200623\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1710415780544281\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1403619945049286\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.24650010466575623\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1355750858783722\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10263187438249588\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.2170936018228531\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.19627362489700317\n",
      "train accuracy:  0.703125\n",
      "train batch loss:  0.1333692967891693\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1679328978061676\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1294339895248413\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15117177367210388\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.14410820603370667\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1326868087053299\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.21018530428409576\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.16108089685440063\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.23294991254806519\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.24824854731559753\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.11788016557693481\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12046976387500763\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13621638715267181\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.23829758167266846\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.38292694091796875\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.3889732360839844\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 16, Train Loss: 0.151, Train Acc: 81.86%, Val. Loss: 0.460, Val. Acc: 62.34%\n",
      "time duration:     49.40127642825246\n",
      "train batch loss:  0.1553983986377716\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1509007215499878\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.14067889750003815\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.20787371695041656\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.09013550728559494\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1292487531900406\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1632276028394699\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.10524725914001465\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13065040111541748\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1266639679670334\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12096117436885834\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.20667286217212677\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1734461784362793\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.13543444871902466\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15308600664138794\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12605613470077515\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.07361510396003723\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1164998784661293\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.16027092933654785\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16818943619728088\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1729329228401184\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.15693926811218262\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.13208723068237305\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.10869698226451874\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.17679420113563538\n",
      "train accuracy:  0.765625\n",
      "eval batch loss:  0.29487481713294983\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.35459646582603455\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.37877947092056274\n",
      "eval accuracy:  0.640625\n",
      "Epoch: 17, Train Loss: 0.147, Train Acc: 82.28%, Val. Loss: 0.499, Val. Acc: 62.33%\n",
      "time duration:     49.85703367367387\n",
      "train batch loss:  0.14340583980083466\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1573484092950821\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.09757868945598602\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.17746004462242126\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1404641717672348\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.18206237256526947\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.0991373062133789\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.09743154048919678\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.18042096495628357\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16060812771320343\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1816607564687729\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.15193307399749756\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12291181087493896\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15862832963466644\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13892701268196106\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.14639092981815338\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.18383510410785675\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.19132326543331146\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.14072591066360474\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.10854719579219818\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.12003530561923981\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12902815639972687\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11790148913860321\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.12673448026180267\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.11999127268791199\n",
      "train accuracy:  0.890625\n",
      "eval batch loss:  0.25755202770233154\n",
      "eval accuracy:  0.734375\n",
      "eval batch loss:  0.3639013469219208\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.3730611801147461\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 18, Train Loss: 0.143, Train Acc: 82.98%, Val. Loss: 0.464, Val. Acc: 63.26%\n",
      "time duration:     50.20654084905982\n",
      "train batch loss:  0.13065814971923828\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.21516132354736328\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11194532364606857\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1335463970899582\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.17462560534477234\n",
      "train accuracy:  0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.11063848435878754\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16799569129943848\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1259874850511551\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09580294787883759\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10911422967910767\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.130499005317688\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13818088173866272\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12051960080862045\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13784179091453552\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1524081826210022\n",
      "train accuracy:  0.765625\n",
      "train batch loss:  0.08271807432174683\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.13188377022743225\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.27174869179725647\n",
      "train accuracy:  0.640625\n",
      "train batch loss:  0.08129177987575531\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.15459473431110382\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16008615493774414\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1582401990890503\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1555718034505844\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.14894625544548035\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13540251553058624\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.30097776651382446\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.3645685315132141\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.3718176484107971\n",
      "eval accuracy:  0.625\n",
      "Epoch: 19, Train Loss: 0.139, Train Acc: 83.42%, Val. Loss: 0.436, Val. Acc: 63.31%\n",
      "time duration:     49.94875018298626\n",
      "train batch loss:  0.1602543145418167\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.12328584492206573\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11355215311050415\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.15539196133613586\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14447376132011414\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13350613415241241\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15771740674972534\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13470548391342163\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.10330869257450104\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13636143505573273\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1926637887954712\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15258704125881195\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12812022864818573\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14776286482810974\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1529340296983719\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1527441442012787\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.10304699838161469\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.15291348099708557\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12444618344306946\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17067532241344452\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1796366423368454\n",
      "train accuracy:  0.734375\n",
      "train batch loss:  0.07219409942626953\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.1280055195093155\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.15132446587085724\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13612142205238342\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.2644004821777344\n",
      "eval accuracy:  0.703125\n",
      "eval batch loss:  0.36974242329597473\n",
      "eval accuracy:  0.5625\n",
      "eval batch loss:  0.39660391211509705\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 20, Train Loss: 0.135, Train Acc: 83.90%, Val. Loss: 0.428, Val. Acc: 63.48%\n",
      "time duration:     51.372396832332015\n",
      "train batch loss:  0.13188490271568298\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1025366485118866\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.11755499243736267\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.14632616937160492\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11854948103427887\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11973169445991516\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1395181119441986\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.12730300426483154\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09892905503511429\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.06294051557779312\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.12310685217380524\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.1767592877149582\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1660563051700592\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13950812816619873\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09002436697483063\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11136400699615479\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.09282246232032776\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.13386788964271545\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.11101487278938293\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1424691081047058\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12412592023611069\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12191437184810638\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1359766721725464\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.18098001182079315\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14546899497509003\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.2933049201965332\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.3673800230026245\n",
      "eval accuracy:  0.5625\n",
      "eval batch loss:  0.3957096338272095\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 21, Train Loss: 0.131, Train Acc: 84.34%, Val. Loss: 0.441, Val. Acc: 63.20%\n",
      "time duration:     51.29029029235244\n",
      "train batch loss:  0.1204204112291336\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.17970216274261475\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.11135469377040863\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1927415430545807\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12286942452192307\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12824849784374237\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11045504361391068\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16508078575134277\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1390097439289093\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13505470752716064\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11702743172645569\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1353471428155899\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11490713059902191\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1320302039384842\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.10749755054712296\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13514459133148193\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13200122117996216\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.15562523901462555\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14748135209083557\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16789528727531433\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12181589007377625\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.16734100878238678\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.10861518979072571\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.16229334473609924\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.16160619258880615\n",
      "train accuracy:  0.8125\n",
      "eval batch loss:  0.32314062118530273\n",
      "eval accuracy:  0.671875\n",
      "eval batch loss:  0.35830891132354736\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.4109913110733032\n",
      "eval accuracy:  0.515625\n",
      "Epoch: 22, Train Loss: 0.128, Train Acc: 84.72%, Val. Loss: 0.421, Val. Acc: 63.20%\n",
      "time duration:     50.75081103295088\n",
      "train batch loss:  0.13000726699829102\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1310073733329773\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1142536848783493\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12676849961280823\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16490790247917175\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.07443636655807495\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.11921055614948273\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09923015534877777\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11236521601676941\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1088344156742096\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0974440723657608\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.16434837877750397\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.15238958597183228\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.16950012743473053\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1185416430234909\n",
      "train accuracy:  0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.13491857051849365\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12136860936880112\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.13288433849811554\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22030410170555115\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.11257661879062653\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11319422721862793\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1561526656150818\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13400891423225403\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1345992237329483\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.10463088750839233\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.2705013155937195\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.33637264370918274\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.38556623458862305\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 23, Train Loss: 0.125, Train Acc: 85.11%, Val. Loss: 0.416, Val. Acc: 62.72%\n",
      "time duration:     51.28791013918817\n",
      "train batch loss:  0.13373780250549316\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10068225860595703\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1324118971824646\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.06822410225868225\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.10264614224433899\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13279011845588684\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12131191790103912\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1263418197631836\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0868266224861145\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1247505247592926\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.13225552439689636\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12550762295722961\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.14275991916656494\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12690342962741852\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09293130040168762\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11007402837276459\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1549038141965866\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1388736218214035\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.15992790460586548\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11946593970060349\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10420375317335129\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1575491577386856\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.07685345411300659\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10294103622436523\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.16987481713294983\n",
      "train accuracy:  0.8125\n",
      "eval batch loss:  0.27582141757011414\n",
      "eval accuracy:  0.703125\n",
      "eval batch loss:  0.3595292866230011\n",
      "eval accuracy:  0.59375\n",
      "eval batch loss:  0.402401328086853\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 24, Train Loss: 0.122, Train Acc: 85.68%, Val. Loss: 0.416, Val. Acc: 62.24%\n",
      "time duration:     51.68119639158249\n",
      "train batch loss:  0.11936073005199432\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08933638036251068\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.07794252038002014\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12511080503463745\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12596429884433746\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12521639466285706\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08993952721357346\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08169608563184738\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.09474801272153854\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08119510114192963\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.1229984387755394\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11687600612640381\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11824081093072891\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14520516991615295\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20450259745121002\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.15030021965503693\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12709452211856842\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09018755704164505\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12748560309410095\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11572108417749405\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08369902521371841\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.11281681060791016\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11304139345884323\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11219579726457596\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10263414680957794\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.2783597409725189\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.33921757340431213\n",
      "eval accuracy:  0.578125\n",
      "eval batch loss:  0.39269697666168213\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 25, Train Loss: 0.119, Train Acc: 86.15%, Val. Loss: 0.385, Val. Acc: 64.26%\n",
      "time duration:     50.46734051220119\n",
      "train batch loss:  0.13296358287334442\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10512728989124298\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13700610399246216\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.09296783804893494\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11057087779045105\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1026214212179184\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.14305329322814941\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09463603794574738\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1357821822166443\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.1523178219795227\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.14262372255325317\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.10788342356681824\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11760826408863068\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.08014574646949768\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.14925718307495117\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.14536309242248535\n",
      "train accuracy:  0.796875\n",
      "train batch loss:  0.10112033784389496\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12851108610630035\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0784442201256752\n",
      "train accuracy:  0.953125\n",
      "train batch loss:  0.1432832032442093\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09454295784235\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.14147387444972992\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13658763468265533\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12304327636957169\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14894405007362366\n",
      "train accuracy:  0.828125\n",
      "eval batch loss:  0.3055567741394043\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.3666526675224304\n",
      "eval accuracy:  0.546875\n",
      "eval batch loss:  0.4034765958786011\n",
      "eval accuracy:  0.5\n",
      "Epoch: 26, Train Loss: 0.116, Train Acc: 86.46%, Val. Loss: 0.414, Val. Acc: 62.72%\n",
      "time duration:     51.00796053931117\n",
      "train batch loss:  0.10934494435787201\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.05990016832947731\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.06604339927434921\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.13251814246177673\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.10605044662952423\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.07849118113517761\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11444228887557983\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09795728325843811\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1294601857662201\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.14444813132286072\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.15869426727294922\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.0838741809129715\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.1242646872997284\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12212289124727249\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09737332165241241\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.07089646905660629\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.10141170769929886\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.09758883714675903\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.08800645172595978\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.09983031451702118\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09166399389505386\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09955857694149017\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1360156387090683\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08904634416103363\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.12734916806221008\n",
      "train accuracy:  0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.3590579926967621\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.38079455494880676\n",
      "eval accuracy:  0.546875\n",
      "eval batch loss:  0.4152812957763672\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 27, Train Loss: 0.113, Train Acc: 86.84%, Val. Loss: 0.405, Val. Acc: 63.46%\n",
      "time duration:     50.77375719882548\n",
      "train batch loss:  0.08955539762973785\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09150268882513046\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.1280057728290558\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.14954036474227905\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11098577082157135\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.14812417328357697\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.15356695652008057\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16233499348163605\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.09209002554416656\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.19764770567417145\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.11585691571235657\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.17946824431419373\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.11325225979089737\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.08735118806362152\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.13774345815181732\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.10572116822004318\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.13405096530914307\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.11604157090187073\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.10198809951543808\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13663595914840698\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09751468151807785\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.09859554469585419\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1272488385438919\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.13221174478530884\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.09150473773479462\n",
      "train accuracy:  0.90625\n",
      "eval batch loss:  0.3233764171600342\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.3787309527397156\n",
      "eval accuracy:  0.53125\n",
      "eval batch loss:  0.43619823455810547\n",
      "eval accuracy:  0.546875\n",
      "Epoch: 28, Train Loss: 0.111, Train Acc: 87.01%, Val. Loss: 0.391, Val. Acc: 64.00%\n",
      "time duration:     50.553020970895886\n",
      "train batch loss:  0.11255016922950745\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08012809604406357\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10157100856304169\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.10634853690862656\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.19277752935886383\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.13009703159332275\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.12337161600589752\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.12165109813213348\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.08597572147846222\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11609058082103729\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.0940556675195694\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.12290191650390625\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10257583856582642\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.11661974340677261\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12438394874334335\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.09534619748592377\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10784714668989182\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.16513946652412415\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.12111715972423553\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.0781838670372963\n",
      "train accuracy:  0.9375\n",
      "train batch loss:  0.10114814341068268\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12242799997329712\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11447379738092422\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.1283535361289978\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.12536340951919556\n",
      "train accuracy:  0.859375\n",
      "eval batch loss:  0.4085126221179962\n",
      "eval accuracy:  0.65625\n",
      "eval batch loss:  0.3530924916267395\n",
      "eval accuracy:  0.609375\n",
      "eval batch loss:  0.424568772315979\n",
      "eval accuracy:  0.515625\n",
      "Epoch: 29, Train Loss: 0.108, Train Acc: 87.52%, Val. Loss: 0.416, Val. Acc: 62.63%\n",
      "time duration:     50.685450196266174\n",
      "train batch loss:  0.09922435879707336\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.1113220676779747\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.06882484257221222\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.10096291452646255\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.08138446509838104\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.10836504399776459\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.11248001456260681\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.14190955460071564\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.10496450215578079\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11881942301988602\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10828268527984619\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.11505211889743805\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.15758875012397766\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.13923053443431854\n",
      "train accuracy:  0.828125\n",
      "train batch loss:  0.08464652299880981\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.10870690643787384\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.08307430148124695\n",
      "train accuracy:  0.890625\n",
      "train batch loss:  0.12700802087783813\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.07611549645662308\n",
      "train accuracy:  0.921875\n",
      "train batch loss:  0.09988273680210114\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.08738937973976135\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.11507987976074219\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1211036890745163\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.12592309713363647\n",
      "train accuracy:  0.859375\n",
      "train batch loss:  0.1092318519949913\n",
      "train accuracy:  0.84375\n",
      "eval batch loss:  0.4605403542518616\n",
      "eval accuracy:  0.640625\n",
      "eval batch loss:  0.3581949770450592\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.4131110906600952\n",
      "eval accuracy:  0.578125\n",
      "Epoch: 30, Train Loss: 0.107, Train Acc: 87.77%, Val. Loss: 0.416, Val. Acc: 62.96%\n",
      "time duration:     50.955538999289274\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(allmodel, alltrain_iterator, alloptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(allmodel, allvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Beautymodel.state_dict(), 'SSTmodel/{}.bin'.format('SSTtrain1')) \n",
    "torch.save(Apparelmodel.state_dict(), 'SSTmodel/{}.bin'.format('SSTtrain2')) \n",
    "torch.save(allmodel.state_dict(), 'SSTmodel/{}.bin'.format('SSTtrainall')) \n",
    "torch.save(Jewelrymodel.state_dict(), 'SSTmodel/{}.bin'.format('SSTtrain3')) \n",
    "torch.save(Shoesmodel.state_dict(), 'SSTmodel/{}.bin'.format('SSTtrain4')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautymodel.load_state_dict(torch.load('SSTmodel/SSTtrain1.bin'))\n",
    "Apparelmodel.load_state_dict(torch.load('SSTmodel/SSTtrain2.bin'))\n",
    "Jewelrymodel.load_state_dict(torch.load('SSTmodel/SSTtrain3.bin'))\n",
    "Shoesmodel.load_state_dict(torch.load('SSTmodel/SSTtrain4.bin'))\n",
    "allmodel.load_state_dict(torch.load('SSTmodel/SSTtrainall.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "allmodel = allmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#load model\n",
    "Apparelmodel = torch.load('movie/train2.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "Jewelrymodel = torch.load('movie/train3.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "Shoesmodel = torch.load('movie/train4.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "allmodel = torch.load('movie/trainall.bin', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "#Apparelmodel = Apparelmodel.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# prediction\n",
    "####################\n",
    "\n",
    "'''\n",
    "print('loading frnn4:')\n",
    "model = torch.load('frnn4',map_location=lambda storage,loc:storage)\n",
    "'''\n",
    "    \n",
    "print(\"prediction of frnn8.....\")\n",
    "    \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "with open('../sent/ori_gender_data/male_sent_test_less700.tsv','r') as f:\n",
    "    mtest = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_test_less700.tsv','r') as f:\n",
    "    ftest = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftest]\n",
    "ms = [line.split('\\t')[0] for line in mtest]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtest]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftest]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fprem:\")\n",
    "print(fprem[:10])\n",
    "print(\"10 fpref:\")\n",
    "print(fpref[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file fpref_frnn8.txt...\")\n",
    "with open('fpref_frnn8.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file fprem_frnn8.txt...\")\n",
    "with open('fprem_frnn8.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "\n",
    "\n",
    "'''\n",
    "with open('../sent/ori_gender_data/male_sent_tmp_train.tsv','r') as f:\n",
    "    mtrain = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_tmp_train.tsv','r') as f:\n",
    "    ftrain = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftrain]\n",
    "ms = [line.split('\\t')[0] for line in mtrain]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtrain]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftrain]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fpref on female_sent_tmp_train.tsv:\")\n",
    "print(fpref[:10])\n",
    "print(\"10 fprem on male_sent_tmp_train.tsv:\")\n",
    "print(fprem[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file :fpre_female_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_female_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file :fpre_male_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_male_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext=[]\n",
    "testlabel=[]\n",
    "with open('../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/mytest.tsv','r') as f:\n",
    "    for line in f:\n",
    "        x = line.strip('\\n').split('\\t')\n",
    "        testtext.append(x[0])\n",
    "        testlabel.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence,model,TEXT):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.835625410079956"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Beautymodel,BeautyTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8109288215637207"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Apparelmodel,ApparelTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.170316457748413"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Jewelrymodel,JewelryTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4366986751556396"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],Shoesmodel,ShoesTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.588078498840332"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(testtext[0],allmodel,allTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlabel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext=[]\n",
    "testlabel=[]\n",
    "with open('../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/mytest.tsv','r') as f:\n",
    "    for line in f:\n",
    "        x = line.strip('\\n').split('\\t')\n",
    "        testtext.append(x[0])\n",
    "        testlabel.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39231"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence,model,TEXT):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predall = []\n",
    "pred1 = []\n",
    "pred2 = []\n",
    "pred3 = []\n",
    "pred4 = []\n",
    "for x in testtext:\n",
    "    predall.append(predict_sentiment(x,allmodel,allTEXT))\n",
    "    pred1.append(predict_sentiment(x,Beautymodel,BeautyTEXT))\n",
    "    pred2.append(predict_sentiment(x,Apparelmodel,ApparelTEXT))\n",
    "    pred3.append(predict_sentiment(x,Jewelrymodel,JewelryTEXT))\n",
    "    pred4.append(predict_sentiment(x,Shoesmodel,ShoesTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = []\n",
    "pred2 = []\n",
    "for x in testtext:\n",
    "    pred1.append(predict_sentiment(x,Shoesmodel,ShoesTEXT))\n",
    "    pred2.append(predict_sentiment(x,allmodel,allTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = np.array(testtext)\n",
    "testlabel = np.array(testlabel)\n",
    "predall = np.array(predall)\n",
    "pred1 = np.array(pred1)\n",
    "pred2 = np.array(pred2)\n",
    "pred3 = np.array(pred3)\n",
    "pred4 = np.array(pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = np.array(testtext)\n",
    "testlabel = np.array([float(x) for x in testlabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SSTmodel/testprediction.txt','w') as f:\n",
    "    f.write('label   preall   pre1   pre2   pre3   pre4')\n",
    "    for i in range(len(predall)):\n",
    "        f.write(str(testlabel[i])+'\\t'+str(predall[i])+'\\t'+str(pred1[i])+'\\t'+str(pred2[i])+'\\t'+str(pred3[i])+'\\t'+str(pred4[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SSTmodel/testprediction2.txt','w') as f:\n",
    "    f.write('label   preall_glove200   preall_fasttext300' + '\\n')\n",
    "    for i in range(len(pred1)):\n",
    "        f.write(str(testlabel[i])+'\\t'+str(pred1[i])+'\\t'+str(pred2[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'label':testlabel,'predall':predall,'pred1':pred1,'pred2':pred2,'pred3':pred3,'pred4':pred4,'sent':testtext,'dif1':predall-pred1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  trainall  glove300, glove200, fasttest300\n",
    "##  SSTmodel/: SSTtrainall.bin, SSTtrainall_glove200.bin, SSTtrainall_fasttext300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "with open('SSTmodel/testprediction.txt','r') as f:\n",
    "    tmp = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3=[]\n",
    "for x in tmp[1:]:\n",
    "    pred3.append(float(x.split('\\t')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1  glove200\n",
    "# pred2  fasttext300\n",
    "# pred3  glove300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77125232596671"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((testlabel-pred3)<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'label':testlabel,'glove300':pred3,'glove200':pred1,'fasttext300':pred2,'sent':testtext,'dif1':abs(pred1-pred3),'dif2':abs(pred2-pred3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>glove300</th>\n",
       "      <th>glove200</th>\n",
       "      <th>fasttext300</th>\n",
       "      <th>sent</th>\n",
       "      <th>dif1</th>\n",
       "      <th>dif2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32871</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.927944</td>\n",
       "      <td>-0.325369</td>\n",
       "      <td>-0.504698</td>\n",
       "      <td>Lisa Rinzler 's cinematography</td>\n",
       "      <td>3.253313</td>\n",
       "      <td>3.432642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>2.222200</td>\n",
       "      <td>3.553551</td>\n",
       "      <td>1.059837</td>\n",
       "      <td>0.957885</td>\n",
       "      <td>something else altogether -- clownish</td>\n",
       "      <td>2.493714</td>\n",
       "      <td>2.595666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36179</th>\n",
       "      <td>3.125000</td>\n",
       "      <td>1.132634</td>\n",
       "      <td>3.513343</td>\n",
       "      <td>3.706977</td>\n",
       "      <td>mainstream audiences have rarely seen</td>\n",
       "      <td>2.380709</td>\n",
       "      <td>2.574344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25756</th>\n",
       "      <td>1.597200</td>\n",
       "      <td>4.084795</td>\n",
       "      <td>1.787006</td>\n",
       "      <td>1.586255</td>\n",
       "      <td>When the painted backdrops in a movie are more...</td>\n",
       "      <td>2.297788</td>\n",
       "      <td>2.498540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22858</th>\n",
       "      <td>3.888900</td>\n",
       "      <td>3.944363</td>\n",
       "      <td>1.543783</td>\n",
       "      <td>1.478400</td>\n",
       "      <td>A full world has been presented onscreen , not...</td>\n",
       "      <td>2.400580</td>\n",
       "      <td>2.465962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12730</th>\n",
       "      <td>3.263900</td>\n",
       "      <td>3.305025</td>\n",
       "      <td>1.114592</td>\n",
       "      <td>0.857292</td>\n",
       "      <td>Perry and Hurley make inspiring efforts to bre...</td>\n",
       "      <td>2.190433</td>\n",
       "      <td>2.447733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35887</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.626773</td>\n",
       "      <td>2.883826</td>\n",
       "      <td>1.265866</td>\n",
       "      <td>Charlotte 's web</td>\n",
       "      <td>0.742947</td>\n",
       "      <td>2.360907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11105</th>\n",
       "      <td>0.972200</td>\n",
       "      <td>1.657110</td>\n",
       "      <td>3.595458</td>\n",
       "      <td>3.985744</td>\n",
       "      <td>you wo n't be talking about the film once you ...</td>\n",
       "      <td>1.938348</td>\n",
       "      <td>2.328634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20382</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.535370</td>\n",
       "      <td>2.756330</td>\n",
       "      <td>2.768387</td>\n",
       "      <td>Margaret Thatcher 's</td>\n",
       "      <td>2.220960</td>\n",
       "      <td>2.233017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28114</th>\n",
       "      <td>2.222200</td>\n",
       "      <td>-0.169390</td>\n",
       "      <td>1.586828</td>\n",
       "      <td>2.052366</td>\n",
       "      <td>apartheid drama</td>\n",
       "      <td>1.756218</td>\n",
       "      <td>2.221756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13151</th>\n",
       "      <td>2.222200</td>\n",
       "      <td>3.159950</td>\n",
       "      <td>2.334090</td>\n",
       "      <td>0.968168</td>\n",
       "      <td>punching up</td>\n",
       "      <td>0.825860</td>\n",
       "      <td>2.191781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28218</th>\n",
       "      <td>2.083350</td>\n",
       "      <td>0.235095</td>\n",
       "      <td>2.952049</td>\n",
       "      <td>2.409434</td>\n",
       "      <td>Desperately</td>\n",
       "      <td>2.716953</td>\n",
       "      <td>2.174339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5782</th>\n",
       "      <td>1.736100</td>\n",
       "      <td>0.104627</td>\n",
       "      <td>2.307682</td>\n",
       "      <td>2.266327</td>\n",
       "      <td>a sad trust</td>\n",
       "      <td>2.203055</td>\n",
       "      <td>2.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>3.055550</td>\n",
       "      <td>3.131547</td>\n",
       "      <td>1.129413</td>\n",
       "      <td>0.983412</td>\n",
       "      <td>Alexandre Dumas classic</td>\n",
       "      <td>2.002134</td>\n",
       "      <td>2.148135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10720</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.190324</td>\n",
       "      <td>3.294830</td>\n",
       "      <td>3.331331</td>\n",
       "      <td>a fourth-rate Jim Carrey</td>\n",
       "      <td>2.104506</td>\n",
       "      <td>2.141007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>2.916650</td>\n",
       "      <td>0.542034</td>\n",
       "      <td>2.385976</td>\n",
       "      <td>2.671306</td>\n",
       "      <td>cop comedy</td>\n",
       "      <td>1.843942</td>\n",
       "      <td>2.129272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32580</th>\n",
       "      <td>2.777800</td>\n",
       "      <td>0.543297</td>\n",
       "      <td>2.869552</td>\n",
       "      <td>2.672216</td>\n",
       "      <td>many young people</td>\n",
       "      <td>2.326255</td>\n",
       "      <td>2.128919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32749</th>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.189387</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>2.317148</td>\n",
       "      <td>I was sent a copyof this film to review on DVD...</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>2.127761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13020</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.483625</td>\n",
       "      <td>2.451274</td>\n",
       "      <td>2.604971</td>\n",
       "      <td>cop drama</td>\n",
       "      <td>1.967649</td>\n",
       "      <td>2.121346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>3.611100</td>\n",
       "      <td>3.566671</td>\n",
       "      <td>3.362190</td>\n",
       "      <td>1.457098</td>\n",
       "      <td>pack some serious suspense</td>\n",
       "      <td>0.204481</td>\n",
       "      <td>2.109573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1.944450</td>\n",
       "      <td>3.244617</td>\n",
       "      <td>2.425539</td>\n",
       "      <td>1.137075</td>\n",
       "      <td>To build a feel-good fantasy around a vain dic...</td>\n",
       "      <td>0.819078</td>\n",
       "      <td>2.107542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6225</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.118294</td>\n",
       "      <td>3.770263</td>\n",
       "      <td>3.223149</td>\n",
       "      <td>What you get with Empire is a movie</td>\n",
       "      <td>2.651969</td>\n",
       "      <td>2.104855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27832</th>\n",
       "      <td>2.638900</td>\n",
       "      <td>3.055491</td>\n",
       "      <td>1.737216</td>\n",
       "      <td>0.954200</td>\n",
       "      <td>Had the film boasted a clearer , more memorabl...</td>\n",
       "      <td>1.318275</td>\n",
       "      <td>2.101291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26925</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.895097</td>\n",
       "      <td>2.741926</td>\n",
       "      <td>-0.200494</td>\n",
       "      <td>friend David Cross</td>\n",
       "      <td>0.846830</td>\n",
       "      <td>2.095590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>2.222200</td>\n",
       "      <td>0.904465</td>\n",
       "      <td>2.188704</td>\n",
       "      <td>2.976865</td>\n",
       "      <td>the movie 's inescapable air</td>\n",
       "      <td>1.284240</td>\n",
       "      <td>2.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35243</th>\n",
       "      <td>1.944450</td>\n",
       "      <td>-0.079572</td>\n",
       "      <td>1.486724</td>\n",
       "      <td>1.990554</td>\n",
       "      <td>many false scares</td>\n",
       "      <td>1.566296</td>\n",
       "      <td>2.070126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.574664</td>\n",
       "      <td>2.134101</td>\n",
       "      <td>0.509909</td>\n",
       "      <td>pack it</td>\n",
       "      <td>0.440563</td>\n",
       "      <td>2.064755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7736</th>\n",
       "      <td>1.805550</td>\n",
       "      <td>2.981412</td>\n",
       "      <td>1.128193</td>\n",
       "      <td>0.928380</td>\n",
       "      <td>even the corniest</td>\n",
       "      <td>1.853219</td>\n",
       "      <td>2.053032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8312</th>\n",
       "      <td>2.361100</td>\n",
       "      <td>0.188519</td>\n",
       "      <td>2.031244</td>\n",
       "      <td>2.218029</td>\n",
       "      <td>grab the old lady at the end of my aisle 's wa...</td>\n",
       "      <td>1.842725</td>\n",
       "      <td>2.029509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16764</th>\n",
       "      <td>1.527800</td>\n",
       "      <td>3.143464</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>1.145528</td>\n",
       "      <td>idiotic court maneuvers</td>\n",
       "      <td>2.320804</td>\n",
       "      <td>1.997936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.277780</td>\n",
       "      <td>0.918013</td>\n",
       "      <td>0.881543</td>\n",
       "      <td>0.917683</td>\n",
       "      <td>is so deadly dull that watching the proverbial...</td>\n",
       "      <td>0.036470</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35819</th>\n",
       "      <td>2.569450</td>\n",
       "      <td>2.865540</td>\n",
       "      <td>2.787867</td>\n",
       "      <td>2.865227</td>\n",
       "      <td>be quirky at moments</td>\n",
       "      <td>0.077673</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>3.333350</td>\n",
       "      <td>3.576277</td>\n",
       "      <td>3.623662</td>\n",
       "      <td>3.575974</td>\n",
       "      <td>hugely enjoyable in its own right though not r...</td>\n",
       "      <td>0.047385</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20447</th>\n",
       "      <td>3.819450</td>\n",
       "      <td>3.577230</td>\n",
       "      <td>3.539680</td>\n",
       "      <td>3.577519</td>\n",
       "      <td>is the edge of wild , lunatic invention that w...</td>\n",
       "      <td>0.037550</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27051</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.633994</td>\n",
       "      <td>2.886008</td>\n",
       "      <td>2.634254</td>\n",
       "      <td>the closing credits roll</td>\n",
       "      <td>0.252013</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24077</th>\n",
       "      <td>1.319450</td>\n",
       "      <td>1.241668</td>\n",
       "      <td>1.112144</td>\n",
       "      <td>1.241419</td>\n",
       "      <td>rambling , repetitive dialogue</td>\n",
       "      <td>0.129524</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12583</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.760242</td>\n",
       "      <td>2.550757</td>\n",
       "      <td>2.760021</td>\n",
       "      <td>religious and</td>\n",
       "      <td>0.209485</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13017</th>\n",
       "      <td>4.027800</td>\n",
       "      <td>2.038023</td>\n",
       "      <td>2.579744</td>\n",
       "      <td>2.037807</td>\n",
       "      <td>far more likened to a treasure than a lengthy ...</td>\n",
       "      <td>0.541720</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>2.152800</td>\n",
       "      <td>1.782701</td>\n",
       "      <td>1.991923</td>\n",
       "      <td>1.782487</td>\n",
       "      <td>its own head</td>\n",
       "      <td>0.209222</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040</th>\n",
       "      <td>3.194450</td>\n",
       "      <td>2.404213</td>\n",
       "      <td>2.351467</td>\n",
       "      <td>2.404012</td>\n",
       "      <td>is a few bits funnier than Malle 's dud , if o...</td>\n",
       "      <td>0.052746</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>4.583350</td>\n",
       "      <td>3.730188</td>\n",
       "      <td>4.015489</td>\n",
       "      <td>3.729991</td>\n",
       "      <td>the most transporting or gripping film from Ir...</td>\n",
       "      <td>0.285301</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16760</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.348883</td>\n",
       "      <td>3.079825</td>\n",
       "      <td>3.349073</td>\n",
       "      <td>an act of spiritual faith --</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28392</th>\n",
       "      <td>0.416665</td>\n",
       "      <td>0.544675</td>\n",
       "      <td>0.522990</td>\n",
       "      <td>0.544498</td>\n",
       "      <td>'s a bad sign when you 're rooting for the fil...</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12909</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.719110</td>\n",
       "      <td>2.705459</td>\n",
       "      <td>2.718934</td>\n",
       "      <td>skateboards , and motorcycles</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.803282</td>\n",
       "      <td>3.018486</td>\n",
       "      <td>2.803449</td>\n",
       "      <td>Christophe Honor</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>1.319450</td>\n",
       "      <td>1.577036</td>\n",
       "      <td>1.445697</td>\n",
       "      <td>1.576869</td>\n",
       "      <td>Reign of Fire may be little more than another ...</td>\n",
       "      <td>0.131339</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20839</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.630355</td>\n",
       "      <td>2.428773</td>\n",
       "      <td>2.630512</td>\n",
       "      <td>Collision Course</td>\n",
       "      <td>0.201582</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33660</th>\n",
       "      <td>4.236100</td>\n",
       "      <td>3.974033</td>\n",
       "      <td>4.076910</td>\n",
       "      <td>3.974186</td>\n",
       "      <td>An important movie , a reminder of the power o...</td>\n",
       "      <td>0.102877</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36865</th>\n",
       "      <td>3.680550</td>\n",
       "      <td>3.563960</td>\n",
       "      <td>3.915903</td>\n",
       "      <td>3.564102</td>\n",
       "      <td>... works on some levels and is certainly wort...</td>\n",
       "      <td>0.351943</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32211</th>\n",
       "      <td>2.777800</td>\n",
       "      <td>1.775960</td>\n",
       "      <td>1.930781</td>\n",
       "      <td>1.775829</td>\n",
       "      <td>This is really just another genre picture .</td>\n",
       "      <td>0.154821</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>1.666650</td>\n",
       "      <td>2.179513</td>\n",
       "      <td>2.399901</td>\n",
       "      <td>2.179390</td>\n",
       "      <td>no getting around the fact that this is Reveng...</td>\n",
       "      <td>0.220388</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>4.236100</td>\n",
       "      <td>4.583037</td>\n",
       "      <td>4.647821</td>\n",
       "      <td>4.582917</td>\n",
       "      <td>flat-out amusing , sometimes endearing and oft...</td>\n",
       "      <td>0.064784</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24700</th>\n",
       "      <td>1.944450</td>\n",
       "      <td>1.703803</td>\n",
       "      <td>1.889505</td>\n",
       "      <td>1.703713</td>\n",
       "      <td>been made 40 years ago</td>\n",
       "      <td>0.185702</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24012</th>\n",
       "      <td>2.916650</td>\n",
       "      <td>3.834652</td>\n",
       "      <td>3.608588</td>\n",
       "      <td>3.834574</td>\n",
       "      <td>that Besson has written in years</td>\n",
       "      <td>0.226065</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19639</th>\n",
       "      <td>2.361100</td>\n",
       "      <td>2.603801</td>\n",
       "      <td>2.508075</td>\n",
       "      <td>2.603735</td>\n",
       "      <td>genial but</td>\n",
       "      <td>0.095725</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18438</th>\n",
       "      <td>1.527800</td>\n",
       "      <td>2.000708</td>\n",
       "      <td>1.832102</td>\n",
       "      <td>2.000647</td>\n",
       "      <td>the humanizing stuff that will probably sink t...</td>\n",
       "      <td>0.168606</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>1.041650</td>\n",
       "      <td>1.253966</td>\n",
       "      <td>1.324696</td>\n",
       "      <td>1.253917</td>\n",
       "      <td>ultimately feels as flat as the scruffy sands ...</td>\n",
       "      <td>0.070729</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15494</th>\n",
       "      <td>2.916650</td>\n",
       "      <td>3.386043</td>\n",
       "      <td>3.390016</td>\n",
       "      <td>3.386079</td>\n",
       "      <td>the heartbeat of the world</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.867057</td>\n",
       "      <td>2.056115</td>\n",
       "      <td>1.867081</td>\n",
       "      <td>Plot , characters , drama , emotions , ideas -...</td>\n",
       "      <td>0.189058</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.685077</td>\n",
       "      <td>2.121851</td>\n",
       "      <td>2.685069</td>\n",
       "      <td>spy film</td>\n",
       "      <td>0.563226</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39231 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  glove300  glove200  fasttext300  \\\n",
       "32871  2.500000  2.927944 -0.325369    -0.504698   \n",
       "13606  2.222200  3.553551  1.059837     0.957885   \n",
       "36179  3.125000  1.132634  3.513343     3.706977   \n",
       "25756  1.597200  4.084795  1.787006     1.586255   \n",
       "22858  3.888900  3.944363  1.543783     1.478400   \n",
       "12730  3.263900  3.305025  1.114592     0.857292   \n",
       "35887  2.500000  3.626773  2.883826     1.265866   \n",
       "11105  0.972200  1.657110  3.595458     3.985744   \n",
       "20382  2.500000  0.535370  2.756330     2.768387   \n",
       "28114  2.222200 -0.169390  1.586828     2.052366   \n",
       "13151  2.222200  3.159950  2.334090     0.968168   \n",
       "28218  2.083350  0.235095  2.952049     2.409434   \n",
       "5782   1.736100  0.104627  2.307682     2.266327   \n",
       "8801   3.055550  3.131547  1.129413     0.983412   \n",
       "10720  2.500000  1.190324  3.294830     3.331331   \n",
       "754    2.916650  0.542034  2.385976     2.671306   \n",
       "32580  2.777800  0.543297  2.869552     2.672216   \n",
       "32749  3.125000  0.189387  0.684800     2.317148   \n",
       "13020  2.500000  0.483625  2.451274     2.604971   \n",
       "209    3.611100  3.566671  3.362190     1.457098   \n",
       "219    1.944450  3.244617  2.425539     1.137075   \n",
       "6225   2.500000  1.118294  3.770263     3.223149   \n",
       "27832  2.638900  3.055491  1.737216     0.954200   \n",
       "26925  2.500000  1.895097  2.741926    -0.200494   \n",
       "21260  2.222200  0.904465  2.188704     2.976865   \n",
       "35243  1.944450 -0.079572  1.486724     1.990554   \n",
       "13210  2.500000  2.574664  2.134101     0.509909   \n",
       "7736   1.805550  2.981412  1.128193     0.928380   \n",
       "8312   2.361100  0.188519  2.031244     2.218029   \n",
       "16764  1.527800  3.143464  0.822660     1.145528   \n",
       "...         ...       ...       ...          ...   \n",
       "121    0.277780  0.918013  0.881543     0.917683   \n",
       "35819  2.569450  2.865540  2.787867     2.865227   \n",
       "21944  3.333350  3.576277  3.623662     3.575974   \n",
       "20447  3.819450  3.577230  3.539680     3.577519   \n",
       "27051  2.500000  2.633994  2.886008     2.634254   \n",
       "24077  1.319450  1.241668  1.112144     1.241419   \n",
       "12583  2.500000  2.760242  2.550757     2.760021   \n",
       "13017  4.027800  2.038023  2.579744     2.037807   \n",
       "11533  2.152800  1.782701  1.991923     1.782487   \n",
       "6040   3.194450  2.404213  2.351467     2.404012   \n",
       "886    4.583350  3.730188  4.015489     3.729991   \n",
       "16760  2.500000  3.348883  3.079825     3.349073   \n",
       "28392  0.416665  0.544675  0.522990     0.544498   \n",
       "12909  2.500000  2.719110  2.705459     2.718934   \n",
       "1677   2.500000  2.803282  3.018486     2.803449   \n",
       "3021   1.319450  1.577036  1.445697     1.576869   \n",
       "20839  2.500000  2.630355  2.428773     2.630512   \n",
       "33660  4.236100  3.974033  4.076910     3.974186   \n",
       "36865  3.680550  3.563960  3.915903     3.564102   \n",
       "32211  2.777800  1.775960  1.930781     1.775829   \n",
       "10432  1.666650  2.179513  2.399901     2.179390   \n",
       "17000  4.236100  4.583037  4.647821     4.582917   \n",
       "24700  1.944450  1.703803  1.889505     1.703713   \n",
       "24012  2.916650  3.834652  3.608588     3.834574   \n",
       "19639  2.361100  2.603801  2.508075     2.603735   \n",
       "18438  1.527800  2.000708  1.832102     2.000647   \n",
       "1641   1.041650  1.253966  1.324696     1.253917   \n",
       "15494  2.916650  3.386043  3.390016     3.386079   \n",
       "5734   1.250000  1.867057  2.056115     1.867081   \n",
       "6373   2.500000  2.685077  2.121851     2.685069   \n",
       "\n",
       "                                                    sent      dif1      dif2  \n",
       "32871                     Lisa Rinzler 's cinematography  3.253313  3.432642  \n",
       "13606              something else altogether -- clownish  2.493714  2.595666  \n",
       "36179              mainstream audiences have rarely seen  2.380709  2.574344  \n",
       "25756  When the painted backdrops in a movie are more...  2.297788  2.498540  \n",
       "22858  A full world has been presented onscreen , not...  2.400580  2.465962  \n",
       "12730  Perry and Hurley make inspiring efforts to bre...  2.190433  2.447733  \n",
       "35887                                   Charlotte 's web  0.742947  2.360907  \n",
       "11105  you wo n't be talking about the film once you ...  1.938348  2.328634  \n",
       "20382                               Margaret Thatcher 's  2.220960  2.233017  \n",
       "28114                                    apartheid drama  1.756218  2.221756  \n",
       "13151                                        punching up  0.825860  2.191781  \n",
       "28218                                        Desperately  2.716953  2.174339  \n",
       "5782                                         a sad trust  2.203055  2.161700  \n",
       "8801                             Alexandre Dumas classic  2.002134  2.148135  \n",
       "10720                           a fourth-rate Jim Carrey  2.104506  2.141007  \n",
       "754                                           cop comedy  1.843942  2.129272  \n",
       "32580                                  many young people  2.326255  2.128919  \n",
       "32749  I was sent a copyof this film to review on DVD...  0.495413  2.127761  \n",
       "13020                                          cop drama  1.967649  2.121346  \n",
       "209                           pack some serious suspense  0.204481  2.109573  \n",
       "219    To build a feel-good fantasy around a vain dic...  0.819078  2.107542  \n",
       "6225                 What you get with Empire is a movie  2.651969  2.104855  \n",
       "27832  Had the film boasted a clearer , more memorabl...  1.318275  2.101291  \n",
       "26925                                 friend David Cross  0.846830  2.095590  \n",
       "21260                       the movie 's inescapable air  1.284240  2.072400  \n",
       "35243                                  many false scares  1.566296  2.070126  \n",
       "13210                                            pack it  0.440563  2.064755  \n",
       "7736                                   even the corniest  1.853219  2.053032  \n",
       "8312   grab the old lady at the end of my aisle 's wa...  1.842725  2.029509  \n",
       "16764                            idiotic court maneuvers  2.320804  1.997936  \n",
       "...                                                  ...       ...       ...  \n",
       "121    is so deadly dull that watching the proverbial...  0.036470  0.000330  \n",
       "35819                               be quirky at moments  0.077673  0.000313  \n",
       "21944  hugely enjoyable in its own right though not r...  0.047385  0.000303  \n",
       "20447  is the edge of wild , lunatic invention that w...  0.037550  0.000288  \n",
       "27051                           the closing credits roll  0.252013  0.000260  \n",
       "24077                     rambling , repetitive dialogue  0.129524  0.000250  \n",
       "12583                                      religious and  0.209485  0.000221  \n",
       "13017  far more likened to a treasure than a lengthy ...  0.541720  0.000217  \n",
       "11533                                       its own head  0.209222  0.000214  \n",
       "6040   is a few bits funnier than Malle 's dud , if o...  0.052746  0.000201  \n",
       "886    the most transporting or gripping film from Ir...  0.285301  0.000197  \n",
       "16760                       an act of spiritual faith --  0.269058  0.000190  \n",
       "28392  's a bad sign when you 're rooting for the fil...  0.021685  0.000177  \n",
       "12909                      skateboards , and motorcycles  0.013651  0.000177  \n",
       "1677                                   Christophe Honor  0.215204  0.000167  \n",
       "3021   Reign of Fire may be little more than another ...  0.131339  0.000167  \n",
       "20839                                   Collision Course  0.201582  0.000157  \n",
       "33660  An important movie , a reminder of the power o...  0.102877  0.000153  \n",
       "36865  ... works on some levels and is certainly wort...  0.351943  0.000142  \n",
       "32211        This is really just another genre picture .  0.154821  0.000131  \n",
       "10432  no getting around the fact that this is Reveng...  0.220388  0.000124  \n",
       "17000  flat-out amusing , sometimes endearing and oft...  0.064784  0.000120  \n",
       "24700                             been made 40 years ago  0.185702  0.000090  \n",
       "24012                   that Besson has written in years  0.226065  0.000079  \n",
       "19639                                         genial but  0.095725  0.000065  \n",
       "18438  the humanizing stuff that will probably sink t...  0.168606  0.000061  \n",
       "1641   ultimately feels as flat as the scruffy sands ...  0.070729  0.000049  \n",
       "15494                         the heartbeat of the world  0.003973  0.000036  \n",
       "5734   Plot , characters , drama , emotions , ideas -...  0.189058  0.000025  \n",
       "6373                                            spy film  0.563226  0.000009  \n",
       "\n",
       "[39231 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='dif2',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48345520580886103"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(testlabel-predall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4623319727484543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(testlabel-pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5474348079681878"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(testlabel-pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5327289502799112"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(testlabel-pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5565379111866164"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(abs(testlabel-pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575641711911498"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((testlabel-pred2)<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predall</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred4</th>\n",
       "      <th>sent</th>\n",
       "      <th>dif1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16764</th>\n",
       "      <td>1.52780</td>\n",
       "      <td>3.143464</td>\n",
       "      <td>-4.266251</td>\n",
       "      <td>1.773516</td>\n",
       "      <td>0.800650</td>\n",
       "      <td>2.122735</td>\n",
       "      <td>idiotic court maneuvers</td>\n",
       "      <td>7.409715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>2.08335</td>\n",
       "      <td>1.612003</td>\n",
       "      <td>-5.174387</td>\n",
       "      <td>2.637303</td>\n",
       "      <td>1.319473</td>\n",
       "      <td>1.871274</td>\n",
       "      <td>terrible true story</td>\n",
       "      <td>6.786389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29417</th>\n",
       "      <td>0.83335</td>\n",
       "      <td>1.332041</td>\n",
       "      <td>-4.012280</td>\n",
       "      <td>1.565891</td>\n",
       "      <td>0.921886</td>\n",
       "      <td>1.802170</td>\n",
       "      <td>racist Japanese jokes</td>\n",
       "      <td>5.344320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25175</th>\n",
       "      <td>1.45835</td>\n",
       "      <td>1.615784</td>\n",
       "      <td>-3.650715</td>\n",
       "      <td>1.395845</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>2.003977</td>\n",
       "      <td>hate yourself for giving in</td>\n",
       "      <td>5.266498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>3.95835</td>\n",
       "      <td>4.089880</td>\n",
       "      <td>-1.151815</td>\n",
       "      <td>3.989180</td>\n",
       "      <td>3.533503</td>\n",
       "      <td>2.865947</td>\n",
       "      <td>undeniably exceedingly clever</td>\n",
       "      <td>5.241695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32621</th>\n",
       "      <td>2.84720</td>\n",
       "      <td>3.290335</td>\n",
       "      <td>-1.946990</td>\n",
       "      <td>2.995038</td>\n",
       "      <td>2.864503</td>\n",
       "      <td>2.680441</td>\n",
       "      <td>my whole life</td>\n",
       "      <td>5.237325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23352</th>\n",
       "      <td>2.56945</td>\n",
       "      <td>1.028766</td>\n",
       "      <td>-4.187107</td>\n",
       "      <td>1.145247</td>\n",
       "      <td>0.785841</td>\n",
       "      <td>1.429597</td>\n",
       "      <td>hates its characters .</td>\n",
       "      <td>5.215873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>1.94445</td>\n",
       "      <td>1.311156</td>\n",
       "      <td>-3.769286</td>\n",
       "      <td>0.046062</td>\n",
       "      <td>0.979262</td>\n",
       "      <td>-2.272864</td>\n",
       "      <td>lame horror flicks go</td>\n",
       "      <td>5.080442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>1.25000</td>\n",
       "      <td>1.318659</td>\n",
       "      <td>-3.734538</td>\n",
       "      <td>1.728988</td>\n",
       "      <td>0.708525</td>\n",
       "      <td>1.861973</td>\n",
       "      <td>Been there , done that ,</td>\n",
       "      <td>5.053197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35256</th>\n",
       "      <td>1.18055</td>\n",
       "      <td>1.082911</td>\n",
       "      <td>-3.935765</td>\n",
       "      <td>1.447353</td>\n",
       "      <td>0.416615</td>\n",
       "      <td>1.510346</td>\n",
       "      <td>idiotic and absurdly sentimental</td>\n",
       "      <td>5.018676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28234</th>\n",
       "      <td>0.55555</td>\n",
       "      <td>1.206143</td>\n",
       "      <td>-3.743893</td>\n",
       "      <td>-1.414715</td>\n",
       "      <td>0.973823</td>\n",
       "      <td>-3.331754</td>\n",
       "      <td>lame horror flicks</td>\n",
       "      <td>4.950036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15937</th>\n",
       "      <td>4.93055</td>\n",
       "      <td>3.592845</td>\n",
       "      <td>-1.350231</td>\n",
       "      <td>3.319423</td>\n",
       "      <td>3.631007</td>\n",
       "      <td>3.198095</td>\n",
       "      <td>just that damn good .</td>\n",
       "      <td>4.943076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>0.90280</td>\n",
       "      <td>2.298723</td>\n",
       "      <td>-2.567102</td>\n",
       "      <td>3.614352</td>\n",
       "      <td>1.864847</td>\n",
       "      <td>-0.253543</td>\n",
       "      <td>fails to fascinate</td>\n",
       "      <td>4.865825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28028</th>\n",
       "      <td>3.26390</td>\n",
       "      <td>3.554742</td>\n",
       "      <td>-1.203693</td>\n",
       "      <td>2.608348</td>\n",
       "      <td>3.150234</td>\n",
       "      <td>3.098880</td>\n",
       "      <td>stereotypes in good fun</td>\n",
       "      <td>4.758435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23761</th>\n",
       "      <td>2.29165</td>\n",
       "      <td>0.643334</td>\n",
       "      <td>-4.102678</td>\n",
       "      <td>0.493749</td>\n",
       "      <td>0.469990</td>\n",
       "      <td>-3.878061</td>\n",
       "      <td>inexcusable dumb innocence</td>\n",
       "      <td>4.746012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10330</th>\n",
       "      <td>1.11110</td>\n",
       "      <td>0.799620</td>\n",
       "      <td>-3.850348</td>\n",
       "      <td>1.256238</td>\n",
       "      <td>0.526237</td>\n",
       "      <td>0.854090</td>\n",
       "      <td>uncompelling the movie</td>\n",
       "      <td>4.649968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7590</th>\n",
       "      <td>2.56945</td>\n",
       "      <td>2.139768</td>\n",
       "      <td>-2.464850</td>\n",
       "      <td>2.807912</td>\n",
       "      <td>1.387515</td>\n",
       "      <td>2.803204</td>\n",
       "      <td>mean ` funny</td>\n",
       "      <td>4.604618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29226</th>\n",
       "      <td>4.23610</td>\n",
       "      <td>3.607237</td>\n",
       "      <td>-0.925562</td>\n",
       "      <td>3.549102</td>\n",
       "      <td>2.804606</td>\n",
       "      <td>3.876126</td>\n",
       "      <td>very funny movie</td>\n",
       "      <td>4.532799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30255</th>\n",
       "      <td>0.27778</td>\n",
       "      <td>0.540660</td>\n",
       "      <td>-3.982379</td>\n",
       "      <td>0.978868</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>-0.388761</td>\n",
       "      <td>Bad beyond belief and</td>\n",
       "      <td>4.523039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>1.38890</td>\n",
       "      <td>1.594000</td>\n",
       "      <td>-2.812062</td>\n",
       "      <td>1.865183</td>\n",
       "      <td>1.461517</td>\n",
       "      <td>-1.082187</td>\n",
       "      <td>overly old-fashioned</td>\n",
       "      <td>4.406062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19497</th>\n",
       "      <td>1.18055</td>\n",
       "      <td>0.939577</td>\n",
       "      <td>-3.366760</td>\n",
       "      <td>1.118526</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>-2.193924</td>\n",
       "      <td>dim-witted and</td>\n",
       "      <td>4.306337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20966</th>\n",
       "      <td>1.38890</td>\n",
       "      <td>1.023835</td>\n",
       "      <td>-3.258197</td>\n",
       "      <td>0.557600</td>\n",
       "      <td>0.792389</td>\n",
       "      <td>-3.666424</td>\n",
       "      <td>lame-old</td>\n",
       "      <td>4.282032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12580</th>\n",
       "      <td>2.15280</td>\n",
       "      <td>3.195016</td>\n",
       "      <td>-1.086253</td>\n",
       "      <td>2.360134</td>\n",
       "      <td>1.850538</td>\n",
       "      <td>2.288732</td>\n",
       "      <td>perfervid treatment</td>\n",
       "      <td>4.281269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12802</th>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.810512</td>\n",
       "      <td>-3.447777</td>\n",
       "      <td>-3.126788</td>\n",
       "      <td>0.726359</td>\n",
       "      <td>-3.294647</td>\n",
       "      <td>unimaginative and derivative</td>\n",
       "      <td>4.258290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1.66665</td>\n",
       "      <td>0.887078</td>\n",
       "      <td>-3.370141</td>\n",
       "      <td>0.906011</td>\n",
       "      <td>0.755007</td>\n",
       "      <td>0.726825</td>\n",
       "      <td>bad of a movie</td>\n",
       "      <td>4.257218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13598</th>\n",
       "      <td>3.47220</td>\n",
       "      <td>2.001454</td>\n",
       "      <td>-2.240781</td>\n",
       "      <td>1.367451</td>\n",
       "      <td>1.667419</td>\n",
       "      <td>-2.083540</td>\n",
       "      <td>sleazy and fun</td>\n",
       "      <td>4.242235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25194</th>\n",
       "      <td>1.38890</td>\n",
       "      <td>0.845219</td>\n",
       "      <td>-3.377403</td>\n",
       "      <td>1.475599</td>\n",
       "      <td>1.188232</td>\n",
       "      <td>1.059911</td>\n",
       "      <td>absurd plot twists</td>\n",
       "      <td>4.222623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25694</th>\n",
       "      <td>1.59720</td>\n",
       "      <td>0.848615</td>\n",
       "      <td>-3.371109</td>\n",
       "      <td>1.015089</td>\n",
       "      <td>0.799186</td>\n",
       "      <td>0.329789</td>\n",
       "      <td>Lacks the inspiration of the original</td>\n",
       "      <td>4.219724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22325</th>\n",
       "      <td>1.80555</td>\n",
       "      <td>1.075337</td>\n",
       "      <td>-3.115126</td>\n",
       "      <td>1.501023</td>\n",
       "      <td>0.744991</td>\n",
       "      <td>-1.290189</td>\n",
       "      <td>asinine ` twist '</td>\n",
       "      <td>4.190463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15057</th>\n",
       "      <td>3.12500</td>\n",
       "      <td>3.783359</td>\n",
       "      <td>-0.378180</td>\n",
       "      <td>3.764351</td>\n",
       "      <td>2.939329</td>\n",
       "      <td>3.910206</td>\n",
       "      <td>very , very good reasons</td>\n",
       "      <td>4.161539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18742</th>\n",
       "      <td>2.50000</td>\n",
       "      <td>0.250722</td>\n",
       "      <td>2.380199</td>\n",
       "      <td>-0.349818</td>\n",
       "      <td>2.622194</td>\n",
       "      <td>-0.247864</td>\n",
       "      <td>93-minute</td>\n",
       "      <td>-2.129477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>3.33335</td>\n",
       "      <td>1.338330</td>\n",
       "      <td>3.482038</td>\n",
       "      <td>2.047855</td>\n",
       "      <td>3.168097</td>\n",
       "      <td>2.064936</td>\n",
       "      <td>has taken promising material for a black comed...</td>\n",
       "      <td>-2.143709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6463</th>\n",
       "      <td>3.12500</td>\n",
       "      <td>1.079364</td>\n",
       "      <td>3.225287</td>\n",
       "      <td>1.663343</td>\n",
       "      <td>3.084112</td>\n",
       "      <td>1.344796</td>\n",
       "      <td>well-meaningness</td>\n",
       "      <td>-2.145923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28568</th>\n",
       "      <td>1.11110</td>\n",
       "      <td>-0.157719</td>\n",
       "      <td>2.014745</td>\n",
       "      <td>0.079404</td>\n",
       "      <td>2.452826</td>\n",
       "      <td>-1.674961</td>\n",
       "      <td>tremendously sad</td>\n",
       "      <td>-2.172464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27966</th>\n",
       "      <td>3.54165</td>\n",
       "      <td>1.429798</td>\n",
       "      <td>3.616996</td>\n",
       "      <td>1.918185</td>\n",
       "      <td>3.450735</td>\n",
       "      <td>1.513147</td>\n",
       "      <td>Dignified CEO</td>\n",
       "      <td>-2.187198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>1.25000</td>\n",
       "      <td>0.731188</td>\n",
       "      <td>2.944238</td>\n",
       "      <td>1.162614</td>\n",
       "      <td>3.561620</td>\n",
       "      <td>0.952737</td>\n",
       "      <td>determination to immerse you in sheer , unrele...</td>\n",
       "      <td>-2.213050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25310</th>\n",
       "      <td>1.52780</td>\n",
       "      <td>0.997464</td>\n",
       "      <td>3.225884</td>\n",
       "      <td>0.682414</td>\n",
       "      <td>2.240514</td>\n",
       "      <td>0.761110</td>\n",
       "      <td>alone , should scare any sane person away .</td>\n",
       "      <td>-2.228420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32388</th>\n",
       "      <td>0.69445</td>\n",
       "      <td>0.664153</td>\n",
       "      <td>2.898645</td>\n",
       "      <td>0.695953</td>\n",
       "      <td>2.598137</td>\n",
       "      <td>0.554425</td>\n",
       "      <td>makes me say the obvious : Abandon all hope of...</td>\n",
       "      <td>-2.234492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>1.25000</td>\n",
       "      <td>0.875848</td>\n",
       "      <td>3.112173</td>\n",
       "      <td>1.024676</td>\n",
       "      <td>2.255942</td>\n",
       "      <td>0.907560</td>\n",
       "      <td>luridly graphic and laughably unconvincing</td>\n",
       "      <td>-2.236325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>0.83335</td>\n",
       "      <td>1.092664</td>\n",
       "      <td>3.330973</td>\n",
       "      <td>1.054459</td>\n",
       "      <td>2.918001</td>\n",
       "      <td>1.030833</td>\n",
       "      <td>for the toilet and scores a direct hit</td>\n",
       "      <td>-2.238310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38138</th>\n",
       "      <td>1.52780</td>\n",
       "      <td>0.806414</td>\n",
       "      <td>3.050163</td>\n",
       "      <td>0.911788</td>\n",
       "      <td>3.094623</td>\n",
       "      <td>1.037495</td>\n",
       "      <td>There 's no excuse for following up a delightf...</td>\n",
       "      <td>-2.243749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>1.94445</td>\n",
       "      <td>1.576429</td>\n",
       "      <td>3.847698</td>\n",
       "      <td>1.428444</td>\n",
       "      <td>3.081854</td>\n",
       "      <td>1.369447</td>\n",
       "      <td>is that , having created an unusually vivid se...</td>\n",
       "      <td>-2.271268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30091</th>\n",
       "      <td>2.43055</td>\n",
       "      <td>-0.066264</td>\n",
       "      <td>2.212300</td>\n",
       "      <td>1.608668</td>\n",
       "      <td>1.676605</td>\n",
       "      <td>1.284309</td>\n",
       "      <td>brooding and</td>\n",
       "      <td>-2.278564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>2.29165</td>\n",
       "      <td>1.239153</td>\n",
       "      <td>3.523382</td>\n",
       "      <td>1.473222</td>\n",
       "      <td>3.530317</td>\n",
       "      <td>1.795924</td>\n",
       "      <td>Blade II is as estrogen-free as movies get , s...</td>\n",
       "      <td>-2.284229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9211</th>\n",
       "      <td>1.45835</td>\n",
       "      <td>0.697049</td>\n",
       "      <td>2.999301</td>\n",
       "      <td>0.822151</td>\n",
       "      <td>2.656907</td>\n",
       "      <td>0.580546</td>\n",
       "      <td>makes me say the obvious : Abandon all hope of...</td>\n",
       "      <td>-2.302252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35148</th>\n",
       "      <td>1.25000</td>\n",
       "      <td>0.480508</td>\n",
       "      <td>2.785237</td>\n",
       "      <td>0.520543</td>\n",
       "      <td>2.596116</td>\n",
       "      <td>-2.217761</td>\n",
       "      <td>Highly irritating at first</td>\n",
       "      <td>-2.304728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37519</th>\n",
       "      <td>1.04165</td>\n",
       "      <td>1.283537</td>\n",
       "      <td>3.609376</td>\n",
       "      <td>1.132476</td>\n",
       "      <td>2.454010</td>\n",
       "      <td>1.188035</td>\n",
       "      <td>is nearly impossible to look at or understand</td>\n",
       "      <td>-2.325839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25706</th>\n",
       "      <td>0.69445</td>\n",
       "      <td>0.621128</td>\n",
       "      <td>3.000036</td>\n",
       "      <td>0.518529</td>\n",
       "      <td>2.855153</td>\n",
       "      <td>-0.759409</td>\n",
       "      <td>Nobody deserves any prizes here .</td>\n",
       "      <td>-2.378908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21748</th>\n",
       "      <td>3.54165</td>\n",
       "      <td>1.442980</td>\n",
       "      <td>3.823672</td>\n",
       "      <td>0.795550</td>\n",
       "      <td>3.628435</td>\n",
       "      <td>-1.101152</td>\n",
       "      <td>Audacious-impossible yet compelling</td>\n",
       "      <td>-2.380692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>2.29165</td>\n",
       "      <td>0.947626</td>\n",
       "      <td>3.337300</td>\n",
       "      <td>1.120750</td>\n",
       "      <td>3.209942</td>\n",
       "      <td>1.182364</td>\n",
       "      <td>are the two best adjectives to describe Ghost ...</td>\n",
       "      <td>-2.389674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25337</th>\n",
       "      <td>2.15280</td>\n",
       "      <td>1.495386</td>\n",
       "      <td>3.924740</td>\n",
       "      <td>1.469519</td>\n",
       "      <td>3.159424</td>\n",
       "      <td>1.483080</td>\n",
       "      <td>is that , having created an unusually vivid se...</td>\n",
       "      <td>-2.429354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20603</th>\n",
       "      <td>1.31945</td>\n",
       "      <td>0.647243</td>\n",
       "      <td>3.128690</td>\n",
       "      <td>0.585986</td>\n",
       "      <td>1.805588</td>\n",
       "      <td>0.654528</td>\n",
       "      <td>It 's been 13 months and 295 preview screening...</td>\n",
       "      <td>-2.481447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13584</th>\n",
       "      <td>1.38890</td>\n",
       "      <td>0.749365</td>\n",
       "      <td>3.237007</td>\n",
       "      <td>1.393832</td>\n",
       "      <td>3.342894</td>\n",
       "      <td>1.343175</td>\n",
       "      <td>as exciting to watch as two last-place basketball</td>\n",
       "      <td>-2.487642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32729</th>\n",
       "      <td>1.45835</td>\n",
       "      <td>0.103293</td>\n",
       "      <td>2.594899</td>\n",
       "      <td>0.026010</td>\n",
       "      <td>2.565589</td>\n",
       "      <td>-2.951997</td>\n",
       "      <td>Mindless and</td>\n",
       "      <td>-2.491606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27615</th>\n",
       "      <td>1.04165</td>\n",
       "      <td>0.670815</td>\n",
       "      <td>3.193220</td>\n",
       "      <td>0.965781</td>\n",
       "      <td>3.060892</td>\n",
       "      <td>0.657295</td>\n",
       "      <td>I 'm telling you , this is f \\*\\*\\* ed</td>\n",
       "      <td>-2.522405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>2.56945</td>\n",
       "      <td>0.740546</td>\n",
       "      <td>3.276796</td>\n",
       "      <td>1.758330</td>\n",
       "      <td>3.395685</td>\n",
       "      <td>1.988338</td>\n",
       "      <td>Ray Liotta and Jason Patric do some of their b...</td>\n",
       "      <td>-2.536251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>1.59720</td>\n",
       "      <td>1.309572</td>\n",
       "      <td>3.897615</td>\n",
       "      <td>1.572786</td>\n",
       "      <td>4.216946</td>\n",
       "      <td>2.879082</td>\n",
       "      <td>will not go down in the annals of cinema as on...</td>\n",
       "      <td>-2.588044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>3.33335</td>\n",
       "      <td>0.758865</td>\n",
       "      <td>3.429420</td>\n",
       "      <td>0.829377</td>\n",
       "      <td>3.302479</td>\n",
       "      <td>0.836949</td>\n",
       "      <td>As dumb and cheesy as they may be , the cartoo...</td>\n",
       "      <td>-2.670555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14681</th>\n",
       "      <td>2.84720</td>\n",
       "      <td>1.248441</td>\n",
       "      <td>3.998203</td>\n",
       "      <td>1.695153</td>\n",
       "      <td>3.828074</td>\n",
       "      <td>1.382729</td>\n",
       "      <td>like ` masterpiece ' and ` triumph ' and all t...</td>\n",
       "      <td>-2.749762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34915</th>\n",
       "      <td>0.55555</td>\n",
       "      <td>0.411917</td>\n",
       "      <td>3.539573</td>\n",
       "      <td>1.312775</td>\n",
       "      <td>3.004015</td>\n",
       "      <td>1.114567</td>\n",
       "      <td>The most repugnant adaptation of a classic tex...</td>\n",
       "      <td>-3.127656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39231 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label   predall     pred1     pred2     pred3     pred4  \\\n",
       "16764  1.52780  3.143464 -4.266251  1.773516  0.800650  2.122735   \n",
       "2347   2.08335  1.612003 -5.174387  2.637303  1.319473  1.871274   \n",
       "29417  0.83335  1.332041 -4.012280  1.565891  0.921886  1.802170   \n",
       "25175  1.45835  1.615784 -3.650715  1.395845  0.811779  2.003977   \n",
       "7025   3.95835  4.089880 -1.151815  3.989180  3.533503  2.865947   \n",
       "32621  2.84720  3.290335 -1.946990  2.995038  2.864503  2.680441   \n",
       "23352  2.56945  1.028766 -4.187107  1.145247  0.785841  1.429597   \n",
       "955    1.94445  1.311156 -3.769286  0.046062  0.979262 -2.272864   \n",
       "15000  1.25000  1.318659 -3.734538  1.728988  0.708525  1.861973   \n",
       "35256  1.18055  1.082911 -3.935765  1.447353  0.416615  1.510346   \n",
       "28234  0.55555  1.206143 -3.743893 -1.414715  0.973823 -3.331754   \n",
       "15937  4.93055  3.592845 -1.350231  3.319423  3.631007  3.198095   \n",
       "5249   0.90280  2.298723 -2.567102  3.614352  1.864847 -0.253543   \n",
       "28028  3.26390  3.554742 -1.203693  2.608348  3.150234  3.098880   \n",
       "23761  2.29165  0.643334 -4.102678  0.493749  0.469990 -3.878061   \n",
       "10330  1.11110  0.799620 -3.850348  1.256238  0.526237  0.854090   \n",
       "7590   2.56945  2.139768 -2.464850  2.807912  1.387515  2.803204   \n",
       "29226  4.23610  3.607237 -0.925562  3.549102  2.804606  3.876126   \n",
       "30255  0.27778  0.540660 -3.982379  0.978868  0.828400 -0.388761   \n",
       "19302  1.38890  1.594000 -2.812062  1.865183  1.461517 -1.082187   \n",
       "19497  1.18055  0.939577 -3.366760  1.118526  0.792708 -2.193924   \n",
       "20966  1.38890  1.023835 -3.258197  0.557600  0.792389 -3.666424   \n",
       "12580  2.15280  3.195016 -1.086253  2.360134  1.850538  2.288732   \n",
       "12802  0.97220  0.810512 -3.447777 -3.126788  0.726359 -3.294647   \n",
       "1435   1.66665  0.887078 -3.370141  0.906011  0.755007  0.726825   \n",
       "13598  3.47220  2.001454 -2.240781  1.367451  1.667419 -2.083540   \n",
       "25194  1.38890  0.845219 -3.377403  1.475599  1.188232  1.059911   \n",
       "25694  1.59720  0.848615 -3.371109  1.015089  0.799186  0.329789   \n",
       "22325  1.80555  1.075337 -3.115126  1.501023  0.744991 -1.290189   \n",
       "15057  3.12500  3.783359 -0.378180  3.764351  2.939329  3.910206   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "18742  2.50000  0.250722  2.380199 -0.349818  2.622194 -0.247864   \n",
       "12974  3.33335  1.338330  3.482038  2.047855  3.168097  2.064936   \n",
       "6463   3.12500  1.079364  3.225287  1.663343  3.084112  1.344796   \n",
       "28568  1.11110 -0.157719  2.014745  0.079404  2.452826 -1.674961   \n",
       "27966  3.54165  1.429798  3.616996  1.918185  3.450735  1.513147   \n",
       "7474   1.25000  0.731188  2.944238  1.162614  3.561620  0.952737   \n",
       "25310  1.52780  0.997464  3.225884  0.682414  2.240514  0.761110   \n",
       "32388  0.69445  0.664153  2.898645  0.695953  2.598137  0.554425   \n",
       "3824   1.25000  0.875848  3.112173  1.024676  2.255942  0.907560   \n",
       "1622   0.83335  1.092664  3.330973  1.054459  2.918001  1.030833   \n",
       "38138  1.52780  0.806414  3.050163  0.911788  3.094623  1.037495   \n",
       "11384  1.94445  1.576429  3.847698  1.428444  3.081854  1.369447   \n",
       "30091  2.43055 -0.066264  2.212300  1.608668  1.676605  1.284309   \n",
       "2966   2.29165  1.239153  3.523382  1.473222  3.530317  1.795924   \n",
       "9211   1.45835  0.697049  2.999301  0.822151  2.656907  0.580546   \n",
       "35148  1.25000  0.480508  2.785237  0.520543  2.596116 -2.217761   \n",
       "37519  1.04165  1.283537  3.609376  1.132476  2.454010  1.188035   \n",
       "25706  0.69445  0.621128  3.000036  0.518529  2.855153 -0.759409   \n",
       "21748  3.54165  1.442980  3.823672  0.795550  3.628435 -1.101152   \n",
       "9338   2.29165  0.947626  3.337300  1.120750  3.209942  1.182364   \n",
       "25337  2.15280  1.495386  3.924740  1.469519  3.159424  1.483080   \n",
       "20603  1.31945  0.647243  3.128690  0.585986  1.805588  0.654528   \n",
       "13584  1.38890  0.749365  3.237007  1.393832  3.342894  1.343175   \n",
       "32729  1.45835  0.103293  2.594899  0.026010  2.565589 -2.951997   \n",
       "27615  1.04165  0.670815  3.193220  0.965781  3.060892  0.657295   \n",
       "6712   2.56945  0.740546  3.276796  1.758330  3.395685  1.988338   \n",
       "2375   1.59720  1.309572  3.897615  1.572786  4.216946  2.879082   \n",
       "2969   3.33335  0.758865  3.429420  0.829377  3.302479  0.836949   \n",
       "14681  2.84720  1.248441  3.998203  1.695153  3.828074  1.382729   \n",
       "34915  0.55555  0.411917  3.539573  1.312775  3.004015  1.114567   \n",
       "\n",
       "                                                    sent      dif1  \n",
       "16764                            idiotic court maneuvers  7.409715  \n",
       "2347                                 terrible true story  6.786389  \n",
       "29417                              racist Japanese jokes  5.344320  \n",
       "25175                        hate yourself for giving in  5.266498  \n",
       "7025                       undeniably exceedingly clever  5.241695  \n",
       "32621                                      my whole life  5.237325  \n",
       "23352                             hates its characters .  5.215873  \n",
       "955                                lame horror flicks go  5.080442  \n",
       "15000                           Been there , done that ,  5.053197  \n",
       "35256                   idiotic and absurdly sentimental  5.018676  \n",
       "28234                                 lame horror flicks  4.950036  \n",
       "15937                              just that damn good .  4.943076  \n",
       "5249                                  fails to fascinate  4.865825  \n",
       "28028                            stereotypes in good fun  4.758435  \n",
       "23761                         inexcusable dumb innocence  4.746012  \n",
       "10330                             uncompelling the movie  4.649968  \n",
       "7590                                        mean ` funny  4.604618  \n",
       "29226                                   very funny movie  4.532799  \n",
       "30255                              Bad beyond belief and  4.523039  \n",
       "19302                               overly old-fashioned  4.406062  \n",
       "19497                                     dim-witted and  4.306337  \n",
       "20966                                           lame-old  4.282032  \n",
       "12580                                perfervid treatment  4.281269  \n",
       "12802                       unimaginative and derivative  4.258290  \n",
       "1435                                      bad of a movie  4.257218  \n",
       "13598                                     sleazy and fun  4.242235  \n",
       "25194                                 absurd plot twists  4.222623  \n",
       "25694              Lacks the inspiration of the original  4.219724  \n",
       "22325                                  asinine ` twist '  4.190463  \n",
       "15057                           very , very good reasons  4.161539  \n",
       "...                                                  ...       ...  \n",
       "18742                                          93-minute -2.129477  \n",
       "12974  has taken promising material for a black comed... -2.143709  \n",
       "6463                                    well-meaningness -2.145923  \n",
       "28568                                   tremendously sad -2.172464  \n",
       "27966                                      Dignified CEO -2.187198  \n",
       "7474   determination to immerse you in sheer , unrele... -2.213050  \n",
       "25310        alone , should scare any sane person away . -2.228420  \n",
       "32388  makes me say the obvious : Abandon all hope of... -2.234492  \n",
       "3824          luridly graphic and laughably unconvincing -2.236325  \n",
       "1622              for the toilet and scores a direct hit -2.238310  \n",
       "38138  There 's no excuse for following up a delightf... -2.243749  \n",
       "11384  is that , having created an unusually vivid se... -2.271268  \n",
       "30091                                       brooding and -2.278564  \n",
       "2966   Blade II is as estrogen-free as movies get , s... -2.284229  \n",
       "9211   makes me say the obvious : Abandon all hope of... -2.302252  \n",
       "35148                         Highly irritating at first -2.304728  \n",
       "37519      is nearly impossible to look at or understand -2.325839  \n",
       "25706                  Nobody deserves any prizes here . -2.378908  \n",
       "21748                Audacious-impossible yet compelling -2.380692  \n",
       "9338   are the two best adjectives to describe Ghost ... -2.389674  \n",
       "25337  is that , having created an unusually vivid se... -2.429354  \n",
       "20603  It 's been 13 months and 295 preview screening... -2.481447  \n",
       "13584  as exciting to watch as two last-place basketball -2.487642  \n",
       "32729                                       Mindless and -2.491606  \n",
       "27615             I 'm telling you , this is f \\*\\*\\* ed -2.522405  \n",
       "6712   Ray Liotta and Jason Patric do some of their b... -2.536251  \n",
       "2375   will not go down in the annals of cinema as on... -2.588044  \n",
       "2969   As dumb and cheesy as they may be , the cartoo... -2.670555  \n",
       "14681  like ` masterpiece ' and ` triumph ' and all t... -2.749762  \n",
       "34915  The most repugnant adaptation of a classic tex... -3.127656  \n",
       "\n",
       "[39231 rows x 8 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# begin VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import chain\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "class RNN_VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    1. Hu, Zhiting, et al. \"Toward controlled generation of text.\" ICML. 2017.\n",
    "    2. Bowman, Samuel R., et al. \"Generating sentences from a continuous space.\" arXiv preprint arXiv:1511.06349 (2015).\n",
    "    3. Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vocab, h_dim, z_dim, p_word_dropout=0.3, unk_idx=0, pad_idx=1, start_idx=2, eos_idx=3, max_sent_len=15, pretrained_embeddings=None, freeze_embeddings=False, gpu=False):\n",
    "        super(RNN_VAE, self).__init__()\n",
    "\n",
    "        self.UNK_IDX = unk_idx\n",
    "        self.PAD_IDX = pad_idx\n",
    "        self.START_IDX = start_idx\n",
    "        self.EOS_IDX = eos_idx\n",
    "        self.MAX_SENT_LEN = max_sent_len\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.p_word_dropout = p_word_dropout\n",
    "\n",
    "        self.gpu = gpu\n",
    "\n",
    "        \"\"\"\n",
    "        Word embeddings layer\n",
    "        \"\"\"\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb_dim = h_dim\n",
    "            self.word_emb = nn.Embedding(n_vocab, h_dim, self.PAD_IDX)\n",
    "        else:\n",
    "            self.emb_dim = pretrained_embeddings.size(1)\n",
    "            self.word_emb = nn.Embedding(n_vocab, self.emb_dim, self.PAD_IDX)\n",
    "\n",
    "            # Set pretrained embeddings\n",
    "            self.word_emb.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "            if freeze_embeddings:\n",
    "                self.word_emb.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Encoder is GRU with FC layers connected to last hidden unit\n",
    "        \"\"\"\n",
    "        self.encoder = nn.GRU(self.emb_dim, h_dim)\n",
    "        self.q_mu = nn.Linear(h_dim, z_dim)\n",
    "        self.q_logvar = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder is GRU with `z` and `c` appended at its inputs\n",
    "        \"\"\"\n",
    "        self.decoder = nn.GRU(self.emb_dim+z_dim, z_dim, dropout=0.3)\n",
    "        self.decoder_fc = nn.Linear(z_dim, n_vocab)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Grouping the model's parameters: separating encoder, decoder, and discriminator\n",
    "        \"\"\"\n",
    "        self.encoder_params = chain(\n",
    "            self.encoder.parameters(), self.q_mu.parameters(), self.q_logvar.parameters()\n",
    "        )\n",
    "\n",
    "        self.decoder_params = chain(\n",
    "            self.decoder.parameters(), self.decoder_fc.parameters()\n",
    "        )\n",
    "\n",
    "        self.vae_params = chain(\n",
    "            self.word_emb.parameters(), self.encoder_params, self.decoder_params\n",
    "        )\n",
    "        self.vae_params = filter(lambda p: p.requires_grad, self.vae_params)\n",
    "\n",
    "        \"\"\"\n",
    "        Use GPU if set\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward_encoder(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is batch of sentences: seq_len x mbsize\n",
    "        \"\"\"    \n",
    "        inputs = self.word_emb(inputs)\n",
    "        return self.forward_encoder_embed(inputs)\n",
    "\n",
    "    def forward_encoder_embed(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is embeddings of: seq_len x mbsize x emb_dim\n",
    "        \"\"\"\n",
    "        _, h = self.encoder(inputs, None)\n",
    "\n",
    "        # Forward to latent\n",
    "        h = h.view(-1, self.h_dim)\n",
    "        mu = self.q_mu(h)\n",
    "        logvar = self.q_logvar(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std*eps; eps ~ N(0, I)\n",
    "        \"\"\"\n",
    "        eps = Variable(torch.randn(self.z_dim))\n",
    "        eps = eps.cuda() if self.gpu else eps\n",
    "        z = mu + torch.exp(logvar/2) * eps\n",
    "        \n",
    "        return z/z.pow(2).sum().pow(0.5)\n",
    "\n",
    "    def sample_z_prior(self, mbsize):\n",
    "        \"\"\"\n",
    "        Sample z ~ p(z) = N(0, I)\n",
    "        \"\"\"\n",
    "        z = Variable(torch.randn(mbsize, self.z_dim))\n",
    "        z = z.cuda() if self.gpu else z\n",
    "        \n",
    "        return z/z.pow(2).sum().pow(0.5)\n",
    "\n",
    "    def forward_decoder(self, inputs, z):\n",
    "        \"\"\"\n",
    "        Inputs must be embeddings: seq_len x mbsize\n",
    "        \"\"\"\n",
    "        dec_inputs = self.word_dropout(inputs)\n",
    "\n",
    "        # Forward\n",
    "        seq_len = dec_inputs.size(0)\n",
    "\n",
    "        # 1 x mbsize x (z_dim+c_dim)\n",
    "        init_h = z.unsqueeze(0)\n",
    "        inputs_emb = self.word_emb(dec_inputs)  # seq_len x mbsize x emb_dim\n",
    "        inputs_emb = torch.cat([inputs_emb, init_h.repeat(seq_len, 1, 1)], 2)\n",
    "\n",
    "        outputs, _ = self.decoder(inputs_emb, init_h)\n",
    "        seq_len, mbsize, _ = outputs.size()\n",
    "\n",
    "        outputs = outputs.view(seq_len*mbsize, -1)\n",
    "        y = self.decoder_fc(outputs)\n",
    "        y = y.view(seq_len, mbsize, self.n_vocab)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------\n",
    "        sentence: sequence of word indices.\n",
    "        use_c_prior: whether to sample `c` from prior or from `discriminator`.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        recon_loss: reconstruction loss of VAE.\n",
    "        kl_loss: KL-div loss of VAE.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        mbsize = sentence.size(1)\n",
    "\n",
    "        # sentence: '<start> I want to fly <eos>'\n",
    "        # enc_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_targets: 'I want to fly <eos> <pad>'\n",
    "        pad_words = Variable(torch.LongTensor([self.PAD_IDX])).repeat(1, mbsize)\n",
    "        pad_words = pad_words.cuda() if self.gpu else pad_words\n",
    "\n",
    "        enc_inputs = sentence\n",
    "        dec_inputs = sentence\n",
    "        dec_targets = torch.cat([sentence[1:], pad_words], dim=0)\n",
    "\n",
    "        # Encoder: sentence -> z\n",
    "        mu, logvar = self.forward_encoder(enc_inputs)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "\n",
    "        # Decoder: sentence -> y\n",
    "        y = self.forward_decoder(dec_inputs, z)       \n",
    "        \n",
    "        recon_loss = F.cross_entropy(\n",
    "            y.view(-1, self.n_vocab), dec_targets.view(-1), size_average=True\n",
    "        )\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar, 1))\n",
    "\n",
    "        return recon_loss, kl_loss\n",
    "\n",
    "    def generate_sentences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate sentences and corresponding z of (batch_size x max_sent_len)\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            z = self.sample_z_prior(1)\n",
    "            samples.append(self.sample_sentence(z, raw=True))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def sample_sentence(self, z, raw=False, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single sentence from p(x|z,c) according to given temperature.\n",
    "        `raw = True` means this returns sentence as in dataset which is useful\n",
    "        to train discriminator. `False` means that this will return list of\n",
    "        `word_idx` which is useful for evaluation.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "\n",
    "        z= z.view(1, 1, -1)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        if raw:\n",
    "            outputs.append(self.START_IDX)\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            emb = self.word_emb(word).view(1, 1, -1)\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "            output, h = self.decoder(emb, h)\n",
    "            y = self.decoder_fc(output).view(-1)\n",
    "            y = F.softmax(y/temp, dim=0)\n",
    "\n",
    "            idx = torch.multinomial(y,1)\n",
    "\n",
    "            word = Variable(torch.LongTensor([int(idx)]))\n",
    "            word = word.cuda() if self.gpu else word\n",
    "\n",
    "            idx = int(idx)\n",
    "\n",
    "            if not raw and idx == self.EOS_IDX:\n",
    "                break\n",
    "\n",
    "            outputs.append(idx)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        if raw:\n",
    "            outputs = Variable(torch.LongTensor(outputs)).unsqueeze(0)\n",
    "            return outputs.cuda() if self.gpu else outputs\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def generate_soft_embed(self, mbsize, temp=1):\n",
    "        \"\"\"\n",
    "        Generate soft embeddings of (mbsize x emb_dim) along with target z\n",
    "        and c for each row (mbsize x {z_dim, c_dim})\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        targets_z = []\n",
    "\n",
    "        for _ in range(mbsize):\n",
    "            z = self.sample_z_prior(1)\n",
    "\n",
    "            samples.append(self.sample_soft_embed(z, temp=1))\n",
    "            targets_z.append(z)\n",
    "\n",
    "        X_gen = torch.cat(samples, dim=0)\n",
    "        targets_z = torch.cat(targets_z, dim=0)\n",
    "\n",
    "        return X_gen, targets_z\n",
    "\n",
    "    def sample_soft_embed(self, z, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single soft embedded sentence from p(x|z,c) and temperature.\n",
    "        Soft embeddings are calculated as weighted average of word_emb\n",
    "        according to p(x|z,c).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        z = z.view(1, 1, -1)\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "        emb = self.word_emb(word).view(1, 1, -1)\n",
    "        emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = [self.word_emb(word).view(1, -1)]\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            output, h = self.decoder(emb, h)\n",
    "            o = self.decoder_fc(output).view(-1)\n",
    "\n",
    "            # Sample softmax with temperature\n",
    "            y = F.softmax(o / temp, dim=0)\n",
    "\n",
    "            # Take expectation of embedding given output prob -> soft embedding\n",
    "            # <y, w> = 1 x n_vocab * n_vocab x emb_dim\n",
    "            emb = y.unsqueeze(0) @ self.word_emb.weight\n",
    "            emb = emb.view(1, 1, -1)\n",
    "\n",
    "            # Save resulting soft embedding\n",
    "            outputs.append(emb.view(1, -1))\n",
    "\n",
    "            # Append with z and c for the next input\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        # 1 x 16 x emb_dim\n",
    "        outputs = torch.cat(outputs, dim=0).unsqueeze(0)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        return outputs.cuda() if self.gpu else outputs\n",
    "\n",
    "    def word_dropout(self, inputs):\n",
    "        \"\"\"\n",
    "        Do word dropout: with prob `p_word_dropout`, set the word to '<unk>'.\n",
    "        \"\"\"\n",
    "        if isinstance(inputs, Variable):\n",
    "            data = inputs.data.clone()\n",
    "        else:\n",
    "            data = inputs.clone()\n",
    "\n",
    "        # Sample masks: elems with val 1 will be set to <unk>\n",
    "        mask = torch.from_numpy(\n",
    "            np.random.binomial(1, p=self.p_word_dropout, size=tuple(data.size()))\n",
    "                     .astype('uint8')\n",
    "        )\n",
    "\n",
    "        if self.gpu:\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        # Set to <unk>\n",
    "        data[mask] = self.UNK_IDX\n",
    "\n",
    "        return Variable(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "mb_size = 32\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "lr_decay_every = 1000000\n",
    "n_iter = 20000\n",
    "log_interval = 1000\n",
    "z_dim = 128\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "VAEmodel = RNN_VAE(\n",
    "    len(allTEXT.vocab), h_dim, z_dim, p_word_dropout=0.3,max_sent_len=40,\n",
    "    pretrained_embeddings=allTEXT.vocab.vectors, freeze_embeddings=False,\n",
    "    gpu=True\n",
    ")\n",
    "##################### GPU!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Annealing for KL term\n",
    "kld_start_inc = 3000\n",
    "kld_weight = 0.03\n",
    "kld_max = 0.15\n",
    "kld_inc = (kld_max - kld_weight) / (n_iter - kld_start_inc)\n",
    "\n",
    "trainer = optim.Adam(VAEmodel.vae_params, lr=lr)\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "dataset=alltrain, batch_size=mb_size,\n",
    "sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
    "   \n",
    "def save_model():\n",
    "    if not os.path.exists('Amazon/models/'):\n",
    "        os.makedirs('Amazon/models/')\n",
    "\n",
    "    torch.save(model.state_dict(), 'Amazon/models/{}.bin'.format('Amazon_Beauty300test_baseVAE_sph'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = VAEmodel\n",
    "self.eval()\n",
    "mbsize = inputs.size(1)\n",
    "pad_words = Variable(torch.LongTensor([self.PAD_IDX])).repeat(1, mbsize)\n",
    "pad_words = pad_words.cuda() if self.gpu else pad_words\n",
    "enc_inputs = inputs\n",
    "dec_inputs = inputs\n",
    "dec_targets = torch.cat([inputs[1:], pad_words], dim=0)\n",
    "\n",
    "# Encoder: sentence -> z\n",
    "mu, logvar = self.forward_encoder(enc_inputs)\n",
    "z = self.sample_z(mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,    195,     10,      6,  12936,      6,     19,    376,\n",
       "            268,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = self.sample_z_prior(1)\n",
    "self.sample_sentence(sz, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 1.8068; Recon: 1.8065; KL: 0.0098; Grad_norm: 0.0000;\n",
      "Sample: \"which <pad> and <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> and and to could <pad> <pad> that as to <pad> <pad> <pad> <pad> <pad> and <pad> <pad> <pad> <pad> <pad> <pad> <pad> at <pad> <pad> butler <pad> <pad>\"\n",
      "\n",
      "\n",
      "allmodel sample prediction:  1.8835241794586182\n",
      "Beautymodel sample prediction:  1.6922941207885742\n",
      "sample abs dif:  0.19123005867004395\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iter-200; Loss: 1.3939; Recon: 1.3938; KL: 0.0026; Grad_norm: 0.0000;\n",
      "Sample: \"State <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\"\n",
      "\n",
      "\n",
      "allmodel sample prediction:  1.8108689785003662\n",
      "Beautymodel sample prediction:  1.5510139465332031\n",
      "sample abs dif:  0.2598550319671631\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c6e518e26858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             self.batches = pool(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                 \u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    129\u001b[0m                          for d in (train_data, val_data, test_data) if d)\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for it in range(100000):\n",
    "    batch = next(iter(train_iter))\n",
    "    inputs = batch.Text\n",
    "    labels = batch.Label\n",
    "\n",
    "    recon_loss, kl_loss = VAEmodel.forward(inputs)\n",
    "    loss = (recon_loss + kld_weight * kl_loss)#*(pre_weight*pre_dif)\n",
    "    #print(\"pre_weight*pre_dif: \",pre_weight*pre_dif)\n",
    "\n",
    "    # Anneal kl_weight\n",
    "    if it > kld_start_inc and kld_weight < kld_max:\n",
    "        kld_weight += kld_inc\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm(VAEmodel.vae_params, 5)\n",
    "    trainer.step()\n",
    "    trainer.zero_grad()\n",
    "\n",
    "\n",
    "    #if it % log_interval == 0:\n",
    "    if it%200==0:\n",
    "        #original_sent = ' '.join([TEXT.vocab.itos[i] for i in inputs[:,0][1:]])\n",
    "        #m = predict_sentiment(original_sent,Bmodel,BTEXT)\n",
    "        #f = predict_sentiment(original_sent,Amodel,ATEXT)\n",
    "        #print(original_sent)\n",
    "        #print(\"Bmodel original prediction: \",m)\n",
    "        #print(\"Amodel original prediction: \",f)\n",
    "        #print(\"abs original dif: \",abs(m-f))\n",
    "\n",
    "\n",
    "        z = VAEmodel.sample_z_prior(1)\n",
    "        sample_idxs = VAEmodel.sample_sentence(z)\n",
    "        sample_sent = ' '.join([allTEXT.vocab.itos[i] for i in sample_idxs])\n",
    "\n",
    "        print('Iter-{}; Loss: {:.4f}; Recon: {:.4f}; KL: {:.4f}; Grad_norm: {:.4f};'\n",
    "              .format(it, loss.data[0], recon_loss.data[0], kl_loss.data[0], grad_norm))\n",
    "        print('Sample: \"{}\"'.format(sample_sent))\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "        if sample_sent:\n",
    "            m0 = predict_sentiment(sample_sent,allmodel,allTEXT)\n",
    "            f0 = predict_sentiment(sample_sent,Beautymodel,BeautyTEXT)\n",
    "            pre_dif_sample = abs(f0-m0)\n",
    "            '''\n",
    "            if pre_dif_sample>1.5:\n",
    "                f = open('baseVAE_sph_log','a')\n",
    "                f.write(str(it)+'\\t'+str(pre_dif_sample)+'\\t'+str(m0)+'\\t'+str(f0)+'\\n')\n",
    "                f.write(sample_sent+'\\n')\n",
    "                f.close()\n",
    "            '''\n",
    "            print(\"allmodel sample prediction: \",m0)\n",
    "            print(\"Beautymodel sample prediction: \",f0)\n",
    "            print(\"sample abs dif: \",abs(m0-f0))\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # Anneal learning rate\n",
    "    new_lr = lr * (0.5 ** (it // lr_decay_every))\n",
    "    for param_group in trainer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    if it%1000==0:\n",
    "        #print(\"saving model Amazon_Beauty300test_baseVAE_sph.bin\")\n",
    "        print(\"\\n\")\n",
    "        #save_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'SSTmodel/{}.bin'.format('all_1_VAE')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
