{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../stanford-corenlp-full-2018-10-05/stanfordSentimentTreebank/dictionary.txt','r') as f:\n",
    "    dic = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!|0\\n',\n",
       " \"! '|22935\\n\",\n",
       " \"! ''|18235\\n\",\n",
       " '! Alas|179257\\n',\n",
       " '! Brilliant|22936\\n',\n",
       " '! Brilliant !|40532\\n',\n",
       " \"! Brilliant ! '|22937\\n\",\n",
       " \"! C'mon|60624\\n\",\n",
       " \"! Gollum 's ` performance ' is incredible|13402\\n\",\n",
       " '! Oh , look at that clever angle ! Wow , a jump cut !|179258\\n',\n",
       " '! Romething|140882\\n',\n",
       " '! Run|179259\\n',\n",
       " '! The Movie|60625\\n',\n",
       " '! The camera twirls ! Oh , look at that clever angle ! Wow , a jump cut !|179260\\n',\n",
       " '! True Hollywood Story|140883\\n',\n",
       " '! Wow|179261\\n',\n",
       " '! Zoom !|179262\\n',\n",
       " '!?|220445\\n',\n",
       " \"!? '|220446\\n\",\n",
       " '#|60626\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty300.tsv...\n"
     ]
    }
   ],
   "source": [
    "BeautyTEXT = data.Field(tokenize='spacy')\n",
    "BeautyLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300.tsv...\")\n",
    "Beautytrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Beauty300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', BeautyTEXT),('Label', BeautyLABEL)])[0]\n",
    "\n",
    "BeautyTEXT.build_vocab(Beautytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "BeautyLABEL.build_vocab(Beautytrain)\n",
    "\n",
    "BeautyLABEL.vocab.stoi['1']=1\n",
    "BeautyLABEL.vocab.stoi['2']=2\n",
    "BeautyLABEL.vocab.stoi['3']=3\n",
    "BeautyLABEL.vocab.stoi['4']=4\n",
    "BeautyLABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Apparel300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ApparelTEXT = data.Field(tokenize='spacy')\n",
    "ApparelLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "Appareltrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Apparel300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ApparelTEXT),('Label', ApparelLABEL)])[0]\n",
    "\n",
    "ApparelTEXT.build_vocab(Appareltrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ApparelLABEL.build_vocab(Appareltrain)\n",
    "\n",
    "ApparelLABEL.vocab.stoi['1']=1\n",
    "ApparelLABEL.vocab.stoi['2']=2\n",
    "ApparelLABEL.vocab.stoi['3']=3\n",
    "ApparelLABEL.vocab.stoi['4']=4\n",
    "ApparelLABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Jewelry300.tsv...\n"
     ]
    }
   ],
   "source": [
    "JewelryTEXT = data.Field(tokenize='spacy')\n",
    "JewelryLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Jewelry300.tsv...\")\n",
    "Jewelrytrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Jewelry300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', JewelryTEXT),('Label', JewelryLABEL)])[0]\n",
    "\n",
    "JewelryTEXT.build_vocab(Jewelrytrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "JewelryLABEL.build_vocab(Jewelrytrain)\n",
    "\n",
    "JewelryLABEL.vocab.stoi['1']=1\n",
    "JewelryLABEL.vocab.stoi['2']=2\n",
    "JewelryLABEL.vocab.stoi['3']=3\n",
    "JewelryLABEL.vocab.stoi['4']=4\n",
    "JewelryLABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Shoes300.tsv...\n"
     ]
    }
   ],
   "source": [
    "ShoesTEXT = data.Field(tokenize='spacy')\n",
    "ShoesLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Shoes300.tsv...\")\n",
    "Shoestrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Shoes300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ShoesTEXT),('Label', ShoesLABEL)])[0]\n",
    "\n",
    "ShoesTEXT.build_vocab(Shoestrain, max_size=60000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "ShoesLABEL.build_vocab(Shoestrain)\n",
    "\n",
    "ShoesLABEL.vocab.stoi['1']=1\n",
    "ShoesLABEL.vocab.stoi['2']=2\n",
    "ShoesLABEL.vocab.stoi['3']=3\n",
    "ShoesLABEL.vocab.stoi['4']=4\n",
    "ShoesLABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_Beautyvocab = sorted(BeautyTEXT.vocab.freqs.items(), key=operator.itemgetter(1),reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "common1 = set.intersection(set(BeautyTEXT.vocab.itos),set(ShoesTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "common2 = set.intersection(set(ApparelTEXT.vocab.itos),set(ShoesTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "common3 = set.intersection(set(JewelryTEXT.vocab.itos),set(ShoesTEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict={}\n",
    "cdict['<unk>']=0\n",
    "cdict['<pad>']=1\n",
    "i=2\n",
    "for x in common:\n",
    "    if x!='<unk>' and x!='<pad>':\n",
    "        cdict[x]=i\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17486"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ShoesTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16904"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(JewelryTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8594"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18035"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ApparelTEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20219"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BeautyTEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8166"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ApparelTEXT.vocab.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '.': 2,\n",
       "             'i': 3,\n",
       "             'it': 4,\n",
       "             'the': 5,\n",
       "             'and': 6,\n",
       "             '!': 7,\n",
       "             ',': 8,\n",
       "             'a': 9,\n",
       "             'this': 10,\n",
       "             'my': 11,\n",
       "             'to': 12,\n",
       "             'for': 13,\n",
       "             'is': 14,\n",
       "             'great': 15,\n",
       "             'product': 16,\n",
       "             'love': 17,\n",
       "             'not': 18,\n",
       "             'of': 19,\n",
       "             'but': 20,\n",
       "             'have': 21,\n",
       "             'in': 22,\n",
       "             'very': 23,\n",
       "             'on': 24,\n",
       "             \"n't\": 25,\n",
       "             'good': 26,\n",
       "             'hair': 27,\n",
       "             'was': 28,\n",
       "             'as': 29,\n",
       "             'with': 30,\n",
       "             'so': 31,\n",
       "             'use': 32,\n",
       "             'like': 33,\n",
       "             'that': 34,\n",
       "             'you': 35,\n",
       "             \"'s\": 36,\n",
       "             'skin': 37,\n",
       "             'works': 38,\n",
       "             'does': 39,\n",
       "             'are': 40,\n",
       "             'really': 41,\n",
       "             'do': 42,\n",
       "             'just': 43,\n",
       "             'well': 44,\n",
       "             'they': 45,\n",
       "             'all': 46,\n",
       "             'will': 47,\n",
       "             'one': 48,\n",
       "             '>': 49,\n",
       "             '<': 50,\n",
       "             'num': 51,\n",
       "             '-': 52,\n",
       "             'nice': 53,\n",
       "             'these': 54,\n",
       "             'me': 55,\n",
       "             'used': 56,\n",
       "             'be': 57,\n",
       "             '...': 58,\n",
       "             'using': 59,\n",
       "             'price': 60,\n",
       "             'would': 61,\n",
       "             'at': 62,\n",
       "             'did': 63,\n",
       "             'out': 64,\n",
       "             'than': 65,\n",
       "             'your': 66,\n",
       "             'time': 67,\n",
       "             'no': 68,\n",
       "             'too': 69,\n",
       "             'after': 70,\n",
       "             'has': 71,\n",
       "             'color': 72,\n",
       "             'more': 73,\n",
       "             'best': 74,\n",
       "             'what': 75,\n",
       "             'had': 76,\n",
       "             'them': 77,\n",
       "             'get': 78,\n",
       "             'when': 79,\n",
       "             'much': 80,\n",
       "             'only': 81,\n",
       "             'smell': 82,\n",
       "             'or': 83,\n",
       "             'buy': 84,\n",
       "             'can': 85,\n",
       "             'if': 86,\n",
       "             'again': 87,\n",
       "             'smells': 88,\n",
       "             'up': 89,\n",
       "             'been': 90,\n",
       "             'work': 91,\n",
       "             'from': 92,\n",
       "             \"'ve\": 93,\n",
       "             'little': 94,\n",
       "             'long': 95,\n",
       "             'better': 96,\n",
       "             'face': 97,\n",
       "             'perfect': 98,\n",
       "             'quality': 99,\n",
       "             'stuff': 100,\n",
       "             'soft': 101,\n",
       "             'other': 102,\n",
       "             'am': 103,\n",
       "             'recommend': 104,\n",
       "             \"'m\": 105,\n",
       "             'bought': 106,\n",
       "             'brush': 107,\n",
       "             'day': 108,\n",
       "             'easy': 109,\n",
       "             'feel': 110,\n",
       "             'scent': 111,\n",
       "             'makes': 112,\n",
       "             'years': 113,\n",
       "             'excellent': 114,\n",
       "             'dry': 115,\n",
       "             '\"': 116,\n",
       "             'oil': 117,\n",
       "             'even': 118,\n",
       "             'amazing': 119,\n",
       "             'an': 120,\n",
       "             'because': 121,\n",
       "             'any': 122,\n",
       "             'also': 123,\n",
       "             'about': 124,\n",
       "             'its': 125,\n",
       "             'now': 126,\n",
       "             'got': 127,\n",
       "             'products': 128,\n",
       "             'make': 129,\n",
       "             'look': 130,\n",
       "             'way': 131,\n",
       "             'off': 132,\n",
       "             'made': 133,\n",
       "             'ever': 134,\n",
       "             'happy': 135,\n",
       "             'fast': 136,\n",
       "             'how': 137,\n",
       "             'still': 138,\n",
       "             'some': 139,\n",
       "             '..': 140,\n",
       "             'feels': 141,\n",
       "             'expected': 142,\n",
       "             'definitely': 143,\n",
       "             'light': 144,\n",
       "             'looks': 145,\n",
       "             'clean': 146,\n",
       "             'smooth': 147,\n",
       "             'first': 148,\n",
       "             'purchase': 149,\n",
       "             'awesome': 150,\n",
       "             'tried': 151,\n",
       "             'were': 152,\n",
       "             'before': 153,\n",
       "             'see': 154,\n",
       "             'there': 155,\n",
       "             'last': 156,\n",
       "             'pretty': 157,\n",
       "             ')': 158,\n",
       "             'over': 159,\n",
       "             'bottle': 160,\n",
       "             '(': 161,\n",
       "             'far': 162,\n",
       "             'without': 163,\n",
       "             'find': 164,\n",
       "             'wonderful': 165,\n",
       "             'could': 166,\n",
       "             'ca': 167,\n",
       "             'which': 168,\n",
       "             'bit': 169,\n",
       "             'small': 170,\n",
       "             'looking': 171,\n",
       "             'favorite': 172,\n",
       "             'cream': 173,\n",
       "             'need': 174,\n",
       "             'received': 175,\n",
       "             'came': 176,\n",
       "             'by': 177,\n",
       "             'lot': 178,\n",
       "             'every': 179,\n",
       "             'never': 180,\n",
       "             '&': 181,\n",
       "             'she': 182,\n",
       "             'money': 183,\n",
       "             'natural': 184,\n",
       "             'shampoo': 185,\n",
       "             'difference': 186,\n",
       "             'order': 187,\n",
       "             'back': 188,\n",
       "             'fine': 189,\n",
       "             'super': 190,\n",
       "             'put': 191,\n",
       "             'go': 192,\n",
       "             'worth': 193,\n",
       "             'few': 194,\n",
       "             'many': 195,\n",
       "             'soap': 196,\n",
       "             'thank': 197,\n",
       "             'brushes': 198,\n",
       "             'size': 199,\n",
       "             '2': 200,\n",
       "             'keep': 201,\n",
       "             'always': 202,\n",
       "             'we': 203,\n",
       "             'job': 204,\n",
       "             'leaves': 205,\n",
       "             'thick': 206,\n",
       "             'results': 207,\n",
       "             'loves': 208,\n",
       "             'goes': 209,\n",
       "             'item': 210,\n",
       "             'try': 211,\n",
       "             'worked': 212,\n",
       "             'arrived': 213,\n",
       "             'highly': 214,\n",
       "             'think': 215,\n",
       "             'loved': 216,\n",
       "             'sure': 217,\n",
       "             'ok': 218,\n",
       "             'two': 219,\n",
       "             '/': 220,\n",
       "             'hard': 221,\n",
       "             'beautiful': 222,\n",
       "             'quickly': 223,\n",
       "             'found': 224,\n",
       "             'lotion': 225,\n",
       "             'right': 226,\n",
       "             'apply': 227,\n",
       "             'makeup': 228,\n",
       "             'seems': 229,\n",
       "             'exactly': 230,\n",
       "             'same': 231,\n",
       "             'colors': 232,\n",
       "             'days': 233,\n",
       "             'old': 234,\n",
       "             'though': 235,\n",
       "             'then': 236,\n",
       "             'feeling': 237,\n",
       "             'enough': 238,\n",
       "             'shipping': 239,\n",
       "             'body': 240,\n",
       "             'teeth': 241,\n",
       "             'while': 242,\n",
       "             'ordered': 243,\n",
       "             'weeks': 244,\n",
       "             'since': 245,\n",
       "             'wish': 246,\n",
       "             '3': 247,\n",
       "             'bad': 248,\n",
       "             'purchased': 249,\n",
       "             'brand': 250,\n",
       "             'fragrance': 251,\n",
       "             'her': 252,\n",
       "             'thought': 253,\n",
       "             'different': 254,\n",
       "             'strong': 255,\n",
       "             '?': 256,\n",
       "             'helps': 257,\n",
       "             'absolutely': 258,\n",
       "             'thanks': 259,\n",
       "             'wear': 260,\n",
       "             'greasy': 261,\n",
       "             'longer': 262,\n",
       "             'less': 263,\n",
       "             'new': 264,\n",
       "             'amazon': 265,\n",
       "             'expensive': 266,\n",
       "             'give': 267,\n",
       "             'nt': 268,\n",
       "             ':)': 269,\n",
       "             'eyes': 270,\n",
       "             'sensitive': 271,\n",
       "             'eye': 272,\n",
       "             'week': 273,\n",
       "             'hold': 274,\n",
       "             'delivery': 275,\n",
       "             'nothing': 276,\n",
       "             'months': 277,\n",
       "             'big': 278,\n",
       "             'thing': 279,\n",
       "             'lasts': 280,\n",
       "             'once': 281,\n",
       "             'wash': 282,\n",
       "             'cheap': 283,\n",
       "             'another': 284,\n",
       "             'daughter': 285,\n",
       "             'know': 286,\n",
       "             'however': 287,\n",
       "             'disappointed': 288,\n",
       "             'described': 289,\n",
       "             'leave': 290,\n",
       "             'most': 291,\n",
       "             'shave': 292,\n",
       "             'buying': 293,\n",
       "             'say': 294,\n",
       "             'conditioner': 295,\n",
       "             'times': 296,\n",
       "             'wife': 297,\n",
       "             'oily': 298,\n",
       "             'wanted': 299,\n",
       "             'want': 300,\n",
       "             'razor': 301,\n",
       "             'easily': 302,\n",
       "             'nail': 303,\n",
       "             'going': 304,\n",
       "             'actually': 305,\n",
       "             'away': 306,\n",
       "             'nails': 307,\n",
       "             'needed': 308,\n",
       "             'down': 309,\n",
       "             'moisturizer': 310,\n",
       "             'husband': 311,\n",
       "             'set': 312,\n",
       "             'fresh': 313,\n",
       "             'year': 314,\n",
       "             'pleased': 315,\n",
       "             'anything': 316,\n",
       "             '$': 317,\n",
       "             'cute': 318,\n",
       "             'gel': 319,\n",
       "             'head': 320,\n",
       "             'getting': 321,\n",
       "             'dark': 322,\n",
       "             'gift': 323,\n",
       "             'spray': 324,\n",
       "             'he': 325,\n",
       "             'keeps': 326,\n",
       "             'working': 327,\n",
       "             'fit': 328,\n",
       "             'help': 329,\n",
       "             'night': 330,\n",
       "             'into': 331,\n",
       "             'quick': 332,\n",
       "             'free': 333,\n",
       "             'should': 334,\n",
       "             \"'ll\": 335,\n",
       "             ';': 336,\n",
       "             '5': 337,\n",
       "             'take': 338,\n",
       "             'almost': 339,\n",
       "             'review': 340,\n",
       "             'yet': 341,\n",
       "             'around': 342,\n",
       "             'everything': 343,\n",
       "             'water': 344,\n",
       "             'polish': 345,\n",
       "             'who': 346,\n",
       "             'gets': 347,\n",
       "             'wo': 348,\n",
       "             'mascara': 349,\n",
       "             'recommended': 350,\n",
       "             'perfume': 351,\n",
       "             'hands': 352,\n",
       "             'gives': 353,\n",
       "             'their': 354,\n",
       "             'something': 355,\n",
       "             'lashes': 356,\n",
       "             'shower': 357,\n",
       "             'stay': 358,\n",
       "             'come': 359,\n",
       "             'thin': 360,\n",
       "             \"'\": 361,\n",
       "             'top': 362,\n",
       "             'started': 363,\n",
       "             'heavy': 364,\n",
       "             'real': 365,\n",
       "             'several': 366,\n",
       "             'acne': 367,\n",
       "             'month': 368,\n",
       "             'service': 369,\n",
       "             'stays': 370,\n",
       "             'serum': 371,\n",
       "             'high': 372,\n",
       "             'white': 373,\n",
       "             'perfectly': 374,\n",
       "             'daily': 375,\n",
       "             '+': 376,\n",
       "             'noticed': 377,\n",
       "             'store': 378,\n",
       "             'waste': 379,\n",
       "             '4': 380,\n",
       "             'already': 381,\n",
       "             'glad': 382,\n",
       "             'under': 383,\n",
       "             'powder': 384,\n",
       "             'gave': 385,\n",
       "             'both': 386,\n",
       "             'quite': 387,\n",
       "             'ones': 388,\n",
       "             'oils': 389,\n",
       "             'cut': 390,\n",
       "             'value': 391,\n",
       "             'travel': 392,\n",
       "             'couple': 393,\n",
       "             \"'re\": 394,\n",
       "             'texture': 395,\n",
       "             'especially': 396,\n",
       "             'comes': 397,\n",
       "             'full': 398,\n",
       "             'case': 399,\n",
       "             'says': 400,\n",
       "             'liked': 401,\n",
       "             'lip': 402,\n",
       "             'such': 403,\n",
       "             'through': 404,\n",
       "             'deal': 405,\n",
       "             'others': 406,\n",
       "             'wig': 407,\n",
       "             'fantastic': 408,\n",
       "             'hot': 409,\n",
       "             'seller': 410,\n",
       "             'must': 411,\n",
       "             'advertised': 412,\n",
       "             'hand': 413,\n",
       "             'being': 414,\n",
       "             'uses': 415,\n",
       "             'continue': 416,\n",
       "             'second': 417,\n",
       "             'clear': 418,\n",
       "             'problem': 419,\n",
       "             'sticky': 420,\n",
       "             'left': 421,\n",
       "             'lips': 422,\n",
       "             'here': 423,\n",
       "             'curly': 424,\n",
       "             'part': 425,\n",
       "             'stars': 426,\n",
       "             'place': 427,\n",
       "             'said': 428,\n",
       "             'amount': 429,\n",
       "             'okay': 430,\n",
       "             'takes': 431,\n",
       "             'seem': 432,\n",
       "             'satisfied': 433,\n",
       "             'mouth': 434,\n",
       "             'compliments': 435,\n",
       "             'able': 436,\n",
       "             'bottles': 437,\n",
       "             'care': 438,\n",
       "             'picture': 439,\n",
       "             'effective': 440,\n",
       "             'helped': 441,\n",
       "             'scalp': 442,\n",
       "             'took': 443,\n",
       "             'our': 444,\n",
       "             'stores': 445,\n",
       "             'kind': 446,\n",
       "             'people': 447,\n",
       "             'everyday': 448,\n",
       "             'reviews': 449,\n",
       "             'broke': 450,\n",
       "             'curls': 451,\n",
       "             'else': 452,\n",
       "             'felt': 453,\n",
       "             'line': 454,\n",
       "             'plastic': 455,\n",
       "             'regular': 456,\n",
       "             'black': 457,\n",
       "             'maybe': 458,\n",
       "             'smaller': 459,\n",
       "             'red': 460,\n",
       "             'feet': 461,\n",
       "             'foundation': 462,\n",
       "             'cheaper': 463,\n",
       "             'having': 464,\n",
       "             's': 465,\n",
       "             'blades': 466,\n",
       "             'drying': 467,\n",
       "             'break': 468,\n",
       "             'close': 469,\n",
       "             'nicely': 470,\n",
       "             'short': 471,\n",
       "             'bag': 472,\n",
       "             'packaging': 473,\n",
       "             'those': 474,\n",
       "             'trying': 475,\n",
       "             '1': 476,\n",
       "             'own': 477,\n",
       "             'done': 478,\n",
       "             'extremely': 479,\n",
       "             'making': 480,\n",
       "             'tell': 481,\n",
       "             'looked': 482,\n",
       "             'anyone': 483,\n",
       "             'morning': 484,\n",
       "             'his': 485,\n",
       "             'facial': 486,\n",
       "             'lovely': 487,\n",
       "             'twice': 488,\n",
       "             'broken': 489,\n",
       "             'shiny': 490,\n",
       "             'sunscreen': 491,\n",
       "             'may': 492,\n",
       "             'deodorant': 493,\n",
       "             'ago': 494,\n",
       "             'original': 495,\n",
       "             ':': 496,\n",
       "             'gentle': 497,\n",
       "             'half': 498,\n",
       "             'pink': 499,\n",
       "             'bristles': 500,\n",
       "             'ordering': 501,\n",
       "             'moisturizing': 502,\n",
       "             'coverage': 503,\n",
       "             'iron': 504,\n",
       "             'probably': 505,\n",
       "             'seen': 506,\n",
       "             'three': 507,\n",
       "             'wet': 508,\n",
       "             \"'d\": 509,\n",
       "             'sharp': 510,\n",
       "             'holds': 511,\n",
       "             'mask': 512,\n",
       "             'why': 513,\n",
       "             'brands': 514,\n",
       "             'change': 515,\n",
       "             'hours': 516,\n",
       "             'package': 517,\n",
       "             'problems': 518,\n",
       "             'shaving': 519,\n",
       "             'where': 520,\n",
       "             'friend': 521,\n",
       "             'box': 522,\n",
       "             'summer': 523,\n",
       "             'overall': 524,\n",
       "             'wearing': 525,\n",
       "             'silky': 526,\n",
       "             'application': 527,\n",
       "             'company': 528,\n",
       "             'essential': 529,\n",
       "             'sun': 530,\n",
       "             'plus': 531,\n",
       "             'each': 532,\n",
       "             'return': 533,\n",
       "             'shine': 534,\n",
       "             'instead': 535,\n",
       "             'myself': 536,\n",
       "             'son': 537,\n",
       "             'brown': 538,\n",
       "             'comfortable': 539,\n",
       "             'delivered': 540,\n",
       "             'm': 541,\n",
       "             'lipstick': 542,\n",
       "             'stick': 543,\n",
       "             'beard': 544,\n",
       "             'easier': 545,\n",
       "             'usually': 546,\n",
       "             'toothbrush': 547,\n",
       "             'non': 548,\n",
       "             'flat': 549,\n",
       "             'improvement': 550,\n",
       "             'side': 551,\n",
       "             'large': 552,\n",
       "             'completely': 553,\n",
       "             'went': 554,\n",
       "             'impressed': 555,\n",
       "             'cologne': 556,\n",
       "             'lines': 557,\n",
       "             'notice': 558,\n",
       "             'fits': 559,\n",
       "             'customer': 560,\n",
       "             'taste': 561,\n",
       "             'things': 562,\n",
       "             'extra': 563,\n",
       "             'guess': 564,\n",
       "             'lasting': 565,\n",
       "             'mirror': 566,\n",
       "             'soon': 567,\n",
       "             'replacement': 568,\n",
       "             'whole': 569,\n",
       "             'applying': 570,\n",
       "             'handle': 571,\n",
       "             'remove': 572,\n",
       "             'life': 573,\n",
       "             'bath': 574,\n",
       "             'end': 575,\n",
       "             'everyone': 576,\n",
       "             'ingredients': 577,\n",
       "             'stopped': 578,\n",
       "             'although': 579,\n",
       "             'needs': 580,\n",
       "             'wait': 581,\n",
       "             'comb': 582,\n",
       "             'home': 583,\n",
       "             'next': 584,\n",
       "             'either': 585,\n",
       "             'healthy': 586,\n",
       "             'prefer': 587,\n",
       "             'pump': 588,\n",
       "             'cool': 589,\n",
       "             'pay': 590,\n",
       "             'mine': 591,\n",
       "             'straight': 592,\n",
       "             'cleanser': 593,\n",
       "             'cost': 594,\n",
       "             'minutes': 595,\n",
       "             'until': 596,\n",
       "             'purchasing': 597,\n",
       "             'add': 598,\n",
       "             'shaver': 599,\n",
       "             '%': 600,\n",
       "             'burn': 601,\n",
       "             'butter': 602,\n",
       "             'cleaning': 603,\n",
       "             'toothpaste': 604,\n",
       "             'smelling': 605,\n",
       "             'dries': 606,\n",
       "             'dryer': 607,\n",
       "             'totally': 608,\n",
       "             'within': 609,\n",
       "             'bigger': 610,\n",
       "             'least': 611,\n",
       "             'cover': 612,\n",
       "             'horrible': 613,\n",
       "             'shipped': 614,\n",
       "             'wrong': 615,\n",
       "             'spots': 616,\n",
       "             'difficult': 617,\n",
       "             'heat': 618,\n",
       "             'past': 619,\n",
       "             'during': 620,\n",
       "             'pleasant': 621,\n",
       "             'kit': 622,\n",
       "             'kids': 623,\n",
       "             'refreshing': 624,\n",
       "             'softer': 625,\n",
       "             'fall': 626,\n",
       "             'let': 627,\n",
       "             'area': 628,\n",
       "             'cleans': 629,\n",
       "             'believe': 630,\n",
       "             'lots': 631,\n",
       "             'condition': 632,\n",
       "             'sturdy': 633,\n",
       "             'hope': 634,\n",
       "             'residue': 635,\n",
       "             'huge': 636,\n",
       "             'saw': 637,\n",
       "             'sent': 638,\n",
       "             'lasted': 639,\n",
       "             'might': 640,\n",
       "             'tube': 641,\n",
       "             'live': 642,\n",
       "             'thicker': 643,\n",
       "             'priced': 644,\n",
       "             'run': 645,\n",
       "             'fake': 646,\n",
       "             'likes': 647,\n",
       "             'star': 648,\n",
       "             'c': 649,\n",
       "             'heads': 650,\n",
       "             'pack': 651,\n",
       "             'read': 652,\n",
       "             'family': 653,\n",
       "             'gone': 654,\n",
       "             'larger': 655,\n",
       "             'salon': 656,\n",
       "             'anymore': 657,\n",
       "             'finally': 658,\n",
       "             'friends': 659,\n",
       "             'doing': 660,\n",
       "             'fact': 661,\n",
       "             'between': 662,\n",
       "             'terrible': 663,\n",
       "             'container': 664,\n",
       "             'mix': 665,\n",
       "             'packaged': 666,\n",
       "             'absorbs': 667,\n",
       "             'baby': 668,\n",
       "             'bright': 669,\n",
       "             'neck': 670,\n",
       "             'type': 671,\n",
       "             ':(': 672,\n",
       "             'liquid': 673,\n",
       "             'fan': 674,\n",
       "             'shade': 675,\n",
       "             'applied': 676,\n",
       "             'tan': 677,\n",
       "             'hairs': 678,\n",
       "             'immediately': 679,\n",
       "             'moisture': 680,\n",
       "             'reason': 681,\n",
       "             'finish': 682,\n",
       "             'mom': 683,\n",
       "             'darker': 684,\n",
       "             'expecting': 685,\n",
       "             'length': 686,\n",
       "             'weight': 687,\n",
       "             'wonders': 688,\n",
       "             'beauty': 689,\n",
       "             'coat': 690,\n",
       "             'cause': 691,\n",
       "             'hoping': 692,\n",
       "             'issues': 693,\n",
       "             'open': 694,\n",
       "             'supposed': 695,\n",
       "             'u': 696,\n",
       "             'sometimes': 697,\n",
       "             'blue': 698,\n",
       "             'pores': 699,\n",
       "             'smoothly': 700,\n",
       "             'clippers': 701,\n",
       "             'normal': 702,\n",
       "             'creamy': 703,\n",
       "             'curl': 704,\n",
       "             'pain': 705,\n",
       "             'due': 706,\n",
       "             'market': 707,\n",
       "             'nose': 708,\n",
       "             'otherwise': 709,\n",
       "             'rather': 710,\n",
       "             'razors': 711,\n",
       "             'receive': 712,\n",
       "             'stop': 713,\n",
       "             'available': 714,\n",
       "             'odor': 715,\n",
       "             'power': 716,\n",
       "             'slightly': 717,\n",
       "             'surprised': 718,\n",
       "             'battery': 719,\n",
       "             'eyeliner': 720,\n",
       "             'often': 721,\n",
       "             'orange': 722,\n",
       "             'unfortunately': 723,\n",
       "             'consistency': 724,\n",
       "             'dried': 725,\n",
       "             'vitamin': 726,\n",
       "             'irritation': 727,\n",
       "             'wrinkles': 728,\n",
       "             'lighter': 729,\n",
       "             'opened': 730,\n",
       "             'pricey': 731,\n",
       "             'control': 732,\n",
       "             'someone': 733,\n",
       "             'style': 734,\n",
       "             'trimmer': 735,\n",
       "             'yes': 736,\n",
       "             'low': 737,\n",
       "             'tight': 738,\n",
       "             'balm': 739,\n",
       "             'noticeable': 740,\n",
       "             '--': 741,\n",
       "             'truly': 742,\n",
       "             '[': 743,\n",
       "             'base': 744,\n",
       "             'legs': 745,\n",
       "             'eyelashes': 746,\n",
       "             'smoother': 747,\n",
       "             'tone': 748,\n",
       "             'touch': 749,\n",
       "             'blade': 750,\n",
       "             'coconut': 751,\n",
       "             'fabulous': 752,\n",
       "             'issue': 753,\n",
       "             'sample': 754,\n",
       "             ']': 755,\n",
       "             'fun': 756,\n",
       "             'pure': 757,\n",
       "             'tiny': 758,\n",
       "             'expectations': 759,\n",
       "             'hate': 760,\n",
       "             '*': 761,\n",
       "             'lather': 762,\n",
       "             'covers': 763,\n",
       "             'frizzy': 764,\n",
       "             'itself': 765,\n",
       "             'organic': 766,\n",
       "             'piece': 767,\n",
       "             'blow': 768,\n",
       "             'cap': 769,\n",
       "             'enjoy': 770,\n",
       "             'formula': 771,\n",
       "             'growth': 772,\n",
       "             'instructions': 773,\n",
       "             'local': 774,\n",
       "             're': 775,\n",
       "             'tool': 776,\n",
       "             'blend': 777,\n",
       "             'careful': 778,\n",
       "             'decent': 779,\n",
       "             'expect': 780,\n",
       "             'matte': 781,\n",
       "             'purse': 782,\n",
       "             'returned': 783,\n",
       "             'scrub': 784,\n",
       "             'sweet': 785,\n",
       "             'washing': 786,\n",
       "             'discontinued': 787,\n",
       "             'excited': 788,\n",
       "             'forever': 789,\n",
       "             'experience': 790,\n",
       "             'cleaner': 791,\n",
       "             'deep': 792,\n",
       "             'save': 793,\n",
       "             'volume': 794,\n",
       "             'glue': 795,\n",
       "             'apart': 796,\n",
       "             'fair': 797,\n",
       "             'fell': 798,\n",
       "             'along': 799,\n",
       "             'damaged': 800,\n",
       "             'paid': 801,\n",
       "             'affordable': 802,\n",
       "             'blonde': 803,\n",
       "             'curling': 804,\n",
       "             'putting': 805,\n",
       "             'smelled': 806,\n",
       "             'wax': 807,\n",
       "             'glow': 808,\n",
       "             'lavender': 809,\n",
       "             'stiff': 810,\n",
       "             'together': 811,\n",
       "             'effect': 812,\n",
       "             'frizz': 813,\n",
       "             'weird': 814,\n",
       "             'eyebrows': 815,\n",
       "             'inside': 816,\n",
       "             'name': 817,\n",
       "             'poor': 818,\n",
       "             'toner': 819,\n",
       "             'gums': 820,\n",
       "             'online': 821,\n",
       "             'simple': 822,\n",
       "             'true': 823,\n",
       "             'version': 824,\n",
       "             'wow': 825,\n",
       "             'blends': 826,\n",
       "             'complaints': 827,\n",
       "             'medium': 828,\n",
       "             'promised': 829,\n",
       "             'gotten': 830,\n",
       "             'mess': 831,\n",
       "             'moisturized': 832,\n",
       "             'tea': 833,\n",
       "             'trust': 834,\n",
       "             'creams': 835,\n",
       "             'house': 836,\n",
       "             'oz': 837,\n",
       "             'reasonable': 838,\n",
       "             'sponge': 839,\n",
       "             'whitening': 840,\n",
       "             'carry': 841,\n",
       "             'cuts': 842,\n",
       "             'green': 843,\n",
       "             'subtle': 844,\n",
       "             'awful': 845,\n",
       "             'kept': 846,\n",
       "             'rid': 847,\n",
       "             'seemed': 848,\n",
       "             'replace': 849,\n",
       "             'sleep': 850,\n",
       "             'clips': 851,\n",
       "             'compared': 852,\n",
       "             'dentist': 853,\n",
       "             'design': 854,\n",
       "             'please': 855,\n",
       "             'professional': 856,\n",
       "             'shea': 857,\n",
       "             'similar': 858,\n",
       "             'treatment': 859,\n",
       "             'jar': 860,\n",
       "             'liner': 861,\n",
       "             'bed': 862,\n",
       "             'early': 863,\n",
       "             'honest': 864,\n",
       "             'later': 865,\n",
       "             'today': 866,\n",
       "             'us': 867,\n",
       "             'bottom': 868,\n",
       "             'cleansing': 869,\n",
       "             'decided': 870,\n",
       "             'future': 871,\n",
       "             'lol': 872,\n",
       "             'convenient': 873,\n",
       "             'fingers': 874,\n",
       "             'items': 875,\n",
       "             'pencil': 876,\n",
       "             'beat': 877,\n",
       "             'ends': 878,\n",
       "             'shape': 879,\n",
       "             'taking': 880,\n",
       "             'added': 881,\n",
       "             'changed': 882,\n",
       "             'pull': 883,\n",
       "             'rough': 884,\n",
       "             'useful': 885,\n",
       "             'brushing': 886,\n",
       "             'date': 887,\n",
       "             'durable': 888,\n",
       "             'irritate': 889,\n",
       "             'lightweight': 890,\n",
       "             'messy': 891,\n",
       "             'scents': 892,\n",
       "             'ended': 893,\n",
       "             'floss': 894,\n",
       "             'primer': 895,\n",
       "             'rub': 896,\n",
       "             'start': 897,\n",
       "             'clip': 898,\n",
       "             'shadow': 899,\n",
       "             'timely': 900,\n",
       "             'tip': 901,\n",
       "             'special': 902,\n",
       "             'ðŸ‘': 903,\n",
       "             'blush': 904,\n",
       "             'except': 905,\n",
       "             'gorgeous': 906,\n",
       "             'opinion': 907,\n",
       "             'protection': 908,\n",
       "             'worst': 909,\n",
       "             'chemicals': 910,\n",
       "             'flimsy': 911,\n",
       "             'sister': 912,\n",
       "             'given': 913,\n",
       "             'harsh': 914,\n",
       "             'keeping': 915,\n",
       "             'beautifully': 916,\n",
       "             'turned': 917,\n",
       "             'damage': 918,\n",
       "             'matter': 919,\n",
       "             'pictured': 920,\n",
       "             'tooth': 921,\n",
       "             'areas': 922,\n",
       "             'combination': 923,\n",
       "             'drops': 924,\n",
       "             'electric': 925,\n",
       "             'him': 926,\n",
       "             'purple': 927,\n",
       "             't': 928,\n",
       "             'breakouts': 929,\n",
       "             'etc': 930,\n",
       "             'girl': 931,\n",
       "             'itchy': 932,\n",
       "             'material': 933,\n",
       "             'plan': 934,\n",
       "             'result': 935,\n",
       "             'tweezers': 936,\n",
       "             'variety': 937,\n",
       "             'charge': 938,\n",
       "             'flavor': 939,\n",
       "             'crazy': 940,\n",
       "             'giving': 941,\n",
       "             'honestly': 942,\n",
       "             'hydrated': 943,\n",
       "             'men': 944,\n",
       "             'rose': 945,\n",
       "             'tree': 946,\n",
       "             'barely': 947,\n",
       "             'course': 948,\n",
       "             'description': 949,\n",
       "             'five': 950,\n",
       "             'helpful': 951,\n",
       "             'n': 952,\n",
       "             'pigmented': 953,\n",
       "             'powerful': 954,\n",
       "             'simply': 955,\n",
       "             'yellow': 956,\n",
       "             'age': 957,\n",
       "             'chemical': 958,\n",
       "             'eczema': 959,\n",
       "             'falling': 960,\n",
       "             'gloss': 961,\n",
       "             'lotions': 962,\n",
       "             'tanning': 963,\n",
       "             '~': 964,\n",
       "             'air': 965,\n",
       "             'cutting': 966,\n",
       "             'fairly': 967,\n",
       "             'grow': 968,\n",
       "             'handy': 969,\n",
       "             'idea': 970,\n",
       "             'loose': 971,\n",
       "             'person': 972,\n",
       "             'removing': 973,\n",
       "             'seeing': 974,\n",
       "             'soaps': 975,\n",
       "             've': 976,\n",
       "             'four': 977,\n",
       "             'hurt': 978,\n",
       "             'lost': 979,\n",
       "             'remover': 980,\n",
       "             'rich': 981,\n",
       "             'become': 982,\n",
       "             'concealer': 983,\n",
       "             'firm': 984,\n",
       "             'overpowering': 985,\n",
       "             'solid': 986,\n",
       "             'alcohol': 987,\n",
       "             'coming': 988,\n",
       "             'dandruff': 989,\n",
       "             'daughters': 990,\n",
       "             'properly': 991,\n",
       "             'provides': 992,\n",
       "             'soothing': 993,\n",
       "             'file': 994,\n",
       "             'mild': 995,\n",
       "             'told': 996,\n",
       "             'bar': 997,\n",
       "             'hoped': 998,\n",
       "             'loss': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautyTEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Apparel300_vocab','w') as f:\n",
    "    json.dump(ApparelTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Beauty300_vocab','w') as f:\n",
    "    json.dump(BeautyTEXT.vocab.stoi,f)\n",
    "    \n",
    "with open('Jewelry300_vocab','w') as f:\n",
    "    json.dump(JewelryTEXT.vocab.stoi,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_iterator = data.BucketIterator.splits(\\n    train, \\n    batch_size=BATCH_SIZE, \\n    sort_key=lambda x: len(x.Text), \\n    repeat=False)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "Beautytrain, Beautyvalid = Beautytrain.split(split_ratio=0.8)\n",
    "Beautytrain_iterator, Beautyvalid_iterator = data.BucketIterator.splits(\n",
    "    (Beautytrain, Beautyvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Appareltrain, Apparelvalid = Appareltrain.split(split_ratio=0.999)\n",
    "Appareltrain_iterator, Apparelvalid_iterator = data.BucketIterator.splits(\n",
    "    (Appareltrain, Apparelvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Jewelrytrain, Jewelryvalid = Jewelrytrain.split(split_ratio=0.8)\n",
    "Jewelrytrain_iterator, Jewelryvalid_iterator = data.BucketIterator.splits(\n",
    "    (Jewelrytrain, Jewelryvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "\n",
    "Shoestrain, Shoesvalid = Shoestrain.split(split_ratio=0.8)\n",
    "Shoestrain_iterator, Shoesvalid_iterator = data.BucketIterator.splits(\n",
    "    (Shoestrain, Shoesvalid), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''\n",
    "train_iterator = data.BucketIterator.splits(\n",
    "    train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.Text), \n",
    "    repeat=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #print(\"embedded shape: \", embedded.shape)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        #print(\"output.shape: \",output.shape)\n",
    "        #print(\"output[-1].shape: \",output[-1].shape)\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        #print(\"cell.shape: \",cell.shape)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        #print(\"hidden.shape: \",hidden.shape)\n",
    "        \n",
    "        y = self.fc(hidden.squeeze(0))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        #return self.fc(hidden.squeeze(0))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20219"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BeautyTEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(20219, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "BeautyINPUT_DIM = len(BeautyTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Beautymodel = RNN(BeautyINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Beautymodel parameters: \")\n",
    "print(Beautymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = BeautyTEXT.vocab.vectors\n",
    "\n",
    "Beautymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Beautyoptimizer = optim.Adam(Beautymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Beautymodel = Beautymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparelmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(18035, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ApparelINPUT_DIM = len(ApparelTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Apparelmodel = RNN(ApparelINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Apparelmodel parameters: \")\n",
    "print(Apparelmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ApparelTEXT.vocab.vectors\n",
    "\n",
    "Apparelmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Appareloptimizer = optim.Adam(Apparelmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Apparelmodel = Apparelmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jewelrymodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(16904, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "JewelryINPUT_DIM = len(JewelryTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Jewelrymodel = RNN(JewelryINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Jewelrymodel parameters: \")\n",
    "print(Jewelrymodel.parameters)\n",
    "\n",
    "pretrained_embeddings = JewelryTEXT.vocab.vectors\n",
    "\n",
    "Jewelrymodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Jewelryoptimizer = optim.Adam(Jewelrymodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Jewelrymodel = Jewelrymodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shoesmodel parameters: \n",
      "<bound method Module.parameters of RNN(\n",
      "  (embedding): Embedding(17486, 300)\n",
      "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "ShoesINPUT_DIM = len(ShoesTEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 500\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "Shoesmodel = RNN(ShoesINPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "print(\"Shoesmodel parameters: \")\n",
    "print(Shoesmodel.parameters)\n",
    "\n",
    "pretrained_embeddings = ShoesTEXT.vocab.vectors\n",
    "\n",
    "Shoesmodel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "import torch.optim as optim\n",
    "Shoesoptimizer = optim.Adam(Shoesmodel.parameters(),lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device=torch.device('cpu')\n",
    "Shoesmodel = Shoesmodel.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(preds,y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # turns on dropout and batch normalization and allow gradient update\n",
    "    \n",
    "    i=0\n",
    "    for batch in iterator:\n",
    "        i=i+1\n",
    "        \n",
    "        optimizer.zero_grad() # set accumulated gradient to 0 for every start of a batch\n",
    "        \n",
    "        predictions = model(batch.Text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Label)\n",
    "        \n",
    "        acc = accuracy(predictions, batch.Label)\n",
    "        \n",
    "        loss.backward() # calculate gradient\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"train batch loss: \", loss.item())\n",
    "            print(\"train accuracy: \", acc.item())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() #turns off dropout and batch normalization\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for batch in iterator:\n",
    "            i=i+1\n",
    "            predictions = model(batch.Text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = accuracy(predictions, batch.Label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i%200 ==0:\n",
    "                print(\"eval batch loss: \", loss.item())\n",
    "                print(\"eval accuracy: \", acc.item())\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "#model = torch.load('fmodel')\n",
    "\n",
    "import timeit\n",
    "#start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.6944787502288818\n",
      "train accuracy:  0.25\n",
      "train batch loss:  1.1130222082138062\n",
      "train accuracy:  0.1875\n",
      "train batch loss:  0.42840540409088135\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.49681568145751953\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.7590948343276978\n",
      "train accuracy:  0.5\n",
      "train batch loss:  1.0518995523452759\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5709025263786316\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  1.2898609638214111\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5770605206489563\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5210078954696655\n",
      "train accuracy:  0.46875\n",
      "train batch loss:  0.6203491687774658\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.4728987216949463\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4706098437309265\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.7257290482521057\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.46271830797195435\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  1.1187175512313843\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.7729892730712891\n",
      "train accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval batch loss:  0.7729634046554565\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.9614523649215698\n",
      "eval accuracy:  0.5\n",
      "Epoch: 01, Train Loss: 0.880, Train Acc: 51.81%, Val. Loss: 1.550, Val. Acc: 48.15%\n",
      "time duration:     61.461701856926084\n",
      "train batch loss:  1.2095149755477905\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.46582889556884766\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  1.0260478258132935\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7600288987159729\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3890240788459778\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5501645803451538\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.6296619176864624\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.6033787727355957\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.7950732111930847\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  1.120182991027832\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.32525211572647095\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.7090725898742676\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4918961226940155\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.6308704614639282\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.6101078987121582\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.48233890533447266\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.6898689270019531\n",
      "train accuracy:  0.5625\n",
      "eval batch loss:  0.6861712336540222\n",
      "eval accuracy:  0.65625\n",
      "eval batch loss:  0.7685175538063049\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 02, Train Loss: 0.575, Train Acc: 60.90%, Val. Loss: 1.005, Val. Acc: 58.53%\n",
      "time duration:     63.987115731462836\n",
      "train batch loss:  0.9769548177719116\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7020853757858276\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5793048739433289\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.35020849108695984\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2054305076599121\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  1.0087592601776123\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.8122888207435608\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.09895996749401093\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.2993268370628357\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5882315039634705\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.43079373240470886\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3382006287574768\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2918552756309509\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.32438886165618896\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5257395505905151\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.26228296756744385\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5249297618865967\n",
      "train accuracy:  0.625\n",
      "eval batch loss:  0.6453770399093628\n",
      "eval accuracy:  0.65625\n",
      "eval batch loss:  0.6676900386810303\n",
      "eval accuracy:  0.46875\n",
      "Epoch: 03, Train Loss: 0.507, Train Acc: 63.55%, Val. Loss: 0.910, Val. Acc: 54.30%\n",
      "time duration:     66.10549740679562\n",
      "train batch loss:  0.3652540445327759\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4803811311721802\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.49160489439964294\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3665144741535187\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.26893576979637146\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5546836853027344\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4393306076526642\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.31738367676734924\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.20582519471645355\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5602772831916809\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4164278507232666\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.44739818572998047\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5062764286994934\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5851519703865051\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.945551335811615\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.5309925079345703\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.7295385599136353\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6147526502609253\n",
      "eval accuracy:  0.6875\n",
      "eval batch loss:  0.5083547830581665\n",
      "eval accuracy:  0.625\n",
      "Epoch: 04, Train Loss: 0.470, Train Acc: 65.21%, Val. Loss: 0.863, Val. Acc: 55.87%\n",
      "time duration:     65.48048909381032\n",
      "train batch loss:  0.420634925365448\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.5621141195297241\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.330331951379776\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.7480893731117249\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.30035942792892456\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.39139053225517273\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.37045466899871826\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.7229379415512085\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.6232441067695618\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.2707490921020508\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6749610900878906\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5940496921539307\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3424467444419861\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.7704634070396423\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.359281063079834\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.37443920969963074\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.4152337312698364\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6419727206230164\n",
      "eval accuracy:  0.625\n",
      "eval batch loss:  0.5183955430984497\n",
      "eval accuracy:  0.65625\n",
      "Epoch: 05, Train Loss: 0.439, Train Acc: 66.31%, Val. Loss: 0.827, Val. Acc: 60.19%\n",
      "time duration:     65.17329184524715\n",
      "train batch loss:  0.3269195556640625\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.43777841329574585\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3048725724220276\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.26024141907691956\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.41194993257522583\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.30826300382614136\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.28788384795188904\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2424384504556656\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.17134344577789307\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.6267094016075134\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3506917953491211\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3959586024284363\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.3463447093963623\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.7922376394271851\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4230286478996277\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.41716933250427246\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.29550063610076904\n",
      "train accuracy:  0.59375\n",
      "eval batch loss:  0.6891484260559082\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.527066707611084\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 06, Train Loss: 0.410, Train Acc: 67.45%, Val. Loss: 0.817, Val. Acc: 61.61%\n",
      "time duration:     64.86875606700778\n",
      "train batch loss:  0.36113300919532776\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1416870653629303\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.32553941011428833\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.20867997407913208\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.37415486574172974\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.32049620151519775\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.29419147968292236\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.5719939470291138\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.23627442121505737\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24191337823867798\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.27522405982017517\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.686618447303772\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5005362033843994\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.8340542316436768\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.6820176839828491\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.4295312762260437\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.31743329763412476\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.5753248929977417\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5527840852737427\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 07, Train Loss: 0.392, Train Acc: 68.31%, Val. Loss: 0.806, Val. Acc: 62.17%\n",
      "time duration:     64.6489941868931\n",
      "train batch loss:  0.3334934711456299\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5944424867630005\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.49264979362487793\n",
      "train accuracy:  0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.37804922461509705\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3283217251300812\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.3317772150039673\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3294980525970459\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3682803213596344\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.23413047194480896\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.4025174379348755\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.36335405707359314\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23460334539413452\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.3531022071838379\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.5043470859527588\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3070678114891052\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4309746026992798\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.48209208250045776\n",
      "train accuracy:  0.53125\n",
      "eval batch loss:  0.6830652952194214\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6398743391036987\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 08, Train Loss: 0.377, Train Acc: 69.36%, Val. Loss: 0.716, Val. Acc: 64.57%\n",
      "time duration:     65.13433141261339\n",
      "train batch loss:  0.38326045870780945\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.6411615014076233\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.4228762984275818\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.22093802690505981\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.3459961414337158\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.34084200859069824\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.44390860199928284\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.5280784964561462\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.5976966619491577\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.357913076877594\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4018459618091583\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.38559165596961975\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2764211893081665\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1488812267780304\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.24249784648418427\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4439336657524109\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.31470000743865967\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.6700568795204163\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.6044360399246216\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 09, Train Loss: 0.362, Train Acc: 70.02%, Val. Loss: 0.697, Val. Acc: 65.47%\n",
      "time duration:     64.77526031062007\n",
      "train batch loss:  0.49614417552948\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.36990129947662354\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4415575861930847\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.694683313369751\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.33005931973457336\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.5681099891662598\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.31491973996162415\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5795221328735352\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.42833343148231506\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.721993088722229\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.20143216848373413\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5759652256965637\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.4874257743358612\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4003485441207886\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22145524621009827\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.1825898289680481\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.40516963601112366\n",
      "train accuracy:  0.65625\n",
      "eval batch loss:  0.7768604755401611\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.42945581674575806\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 10, Train Loss: 0.345, Train Acc: 70.60%, Val. Loss: 0.633, Val. Acc: 65.83%\n",
      "time duration:     65.08825089037418\n",
      "train batch loss:  0.2976674437522888\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4197145700454712\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.18362358212471008\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.214758962392807\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23769327998161316\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.32214200496673584\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.29512184858322144\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23752161860466003\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18766836822032928\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28675028681755066\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.35278528928756714\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3591375946998596\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.2513686418533325\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.6147065758705139\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.42832401394844055\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.6109654903411865\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2694217562675476\n",
      "train accuracy:  0.6875\n",
      "eval batch loss:  0.5274425745010376\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.7826899886131287\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 11, Train Loss: 0.331, Train Acc: 71.49%, Val. Loss: 0.668, Val. Acc: 66.55%\n",
      "time duration:     65.0604446362704\n",
      "train batch loss:  0.15514393150806427\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.18327566981315613\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.698167085647583\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.5872822403907776\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.35742318630218506\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.72477126121521\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.19497565925121307\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.12775175273418427\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.21729697287082672\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.6118708252906799\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.4881320893764496\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.4450601041316986\n",
      "train accuracy:  0.5\n",
      "train batch loss:  0.26149657368659973\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.38034218549728394\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.3644663691520691\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19595645368099213\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.36132973432540894\n",
      "train accuracy:  0.75\n",
      "eval batch loss:  0.6868850588798523\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.5453809499740601\n",
      "eval accuracy:  0.5\n",
      "Epoch: 12, Train Loss: 0.315, Train Acc: 72.23%, Val. Loss: 0.661, Val. Acc: 65.41%\n",
      "time duration:     65.12047510407865\n",
      "train batch loss:  0.2616892457008362\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.5521905422210693\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4805822968482971\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.13453252613544464\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.4897410273551941\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.31596291065216064\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2818089723587036\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.7784060835838318\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1315118819475174\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.24552498757839203\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.20516839623451233\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3225706219673157\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.35877639055252075\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4492284655570984\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.24394825100898743\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.195804163813591\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.36999985575675964\n",
      "train accuracy:  0.84375\n",
      "eval batch loss:  0.7308954000473022\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.6344813704490662\n",
      "eval accuracy:  0.5625\n",
      "Epoch: 13, Train Loss: 0.311, Train Acc: 72.38%, Val. Loss: 0.626, Val. Acc: 65.53%\n",
      "time duration:     65.14614174701273\n",
      "train batch loss:  0.1705777794122696\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.20642131567001343\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18812213838100433\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.35176563262939453\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.14422696828842163\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16021817922592163\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.4905363917350769\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2942260801792145\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.3226441740989685\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.2048337161540985\n",
      "train accuracy:  0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.3432580232620239\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.27045273780822754\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17084985971450806\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.3183668255805969\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2928873300552368\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.26063424348831177\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.20320530235767365\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.6661416292190552\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6164524555206299\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 14, Train Loss: 0.301, Train Acc: 73.19%, Val. Loss: 0.655, Val. Acc: 65.66%\n",
      "time duration:     65.0846634004265\n",
      "train batch loss:  0.2700929045677185\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2551226317882538\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.12264645099639893\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.26584261655807495\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.23913303017616272\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.35731056332588196\n",
      "train accuracy:  0.53125\n",
      "train batch loss:  0.3845929503440857\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19813621044158936\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.21857649087905884\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.7380242347717285\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.16726216673851013\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.4967685043811798\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.3238386809825897\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.24119573831558228\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.2740669846534729\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.35979998111724854\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.1531888246536255\n",
      "train accuracy:  0.875\n",
      "eval batch loss:  0.6906813979148865\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5761730074882507\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 15, Train Loss: 0.291, Train Acc: 73.80%, Val. Loss: 0.617, Val. Acc: 65.15%\n",
      "time duration:     64.7607021201402\n",
      "train batch loss:  0.19602037966251373\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2612307071685791\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.15778818726539612\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.3601011335849762\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23266714811325073\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11398052424192429\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.14279577136039734\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.35728588700294495\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.22172628343105316\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18653005361557007\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.1671086549758911\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22237318754196167\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3729792833328247\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.16631707549095154\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2041517198085785\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.16337385773658752\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.24863433837890625\n",
      "train accuracy:  0.71875\n",
      "eval batch loss:  0.7570111751556396\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.8035313487052917\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 16, Train Loss: 0.281, Train Acc: 74.19%, Val. Loss: 0.663, Val. Acc: 66.05%\n",
      "time duration:     64.87291954644024\n",
      "train batch loss:  0.46751317381858826\n",
      "train accuracy:  0.5625\n",
      "train batch loss:  0.21864929795265198\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.22391125559806824\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.3845176100730896\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2481372356414795\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.34966611862182617\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.12780314683914185\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.28608742356300354\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.16264861822128296\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.21620361506938934\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.4806903004646301\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.32791250944137573\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.15427084267139435\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.1749197095632553\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.2747229337692261\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.1927386224269867\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.195176899433136\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.6684154272079468\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.6596065759658813\n",
      "eval accuracy:  0.5\n",
      "Epoch: 17, Train Loss: 0.271, Train Acc: 74.94%, Val. Loss: 0.643, Val. Acc: 66.00%\n",
      "time duration:     65.0987827796489\n",
      "train batch loss:  0.238729327917099\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.5986758470535278\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.1344938576221466\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.2039276659488678\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.17159107327461243\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.4173009991645813\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2896788716316223\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.5015656352043152\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.27226102352142334\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.5821436643600464\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.339300274848938\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.1956527829170227\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.2643859386444092\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.18448558449745178\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2683703303337097\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23763850331306458\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.28572580218315125\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.7056087255477905\n",
      "eval accuracy:  0.78125\n",
      "eval batch loss:  0.5818093419075012\n",
      "eval accuracy:  0.59375\n",
      "Epoch: 18, Train Loss: 0.261, Train Acc: 75.45%, Val. Loss: 0.680, Val. Acc: 65.56%\n",
      "time duration:     64.94138947874308\n",
      "train batch loss:  0.3672240376472473\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19960153102874756\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.20460394024848938\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.2223706841468811\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.08655233681201935\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.3087875247001648\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.24967491626739502\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.36652636528015137\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.19135238230228424\n",
      "train accuracy:  0.875\n",
      "train batch loss:  0.33380454778671265\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.17247605323791504\n",
      "train accuracy:  0.75\n",
      "train batch loss:  0.22989532351493835\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.4490343928337097\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.22760671377182007\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.10328425467014313\n",
      "train accuracy:  0.90625\n",
      "train batch loss:  0.4623459577560425\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.16722367703914642\n",
      "train accuracy:  0.78125\n",
      "eval batch loss:  0.7248859405517578\n",
      "eval accuracy:  0.71875\n",
      "eval batch loss:  0.5342824459075928\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 19, Train Loss: 0.253, Train Acc: 76.16%, Val. Loss: 0.677, Val. Acc: 65.30%\n",
      "time duration:     65.20977753400803\n",
      "train batch loss:  0.2620387077331543\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.1917487233877182\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.22305592894554138\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.2669419050216675\n",
      "train accuracy:  0.78125\n",
      "train batch loss:  0.19561882317066193\n",
      "train accuracy:  0.71875\n",
      "train batch loss:  0.23810388147830963\n",
      "train accuracy:  0.6875\n",
      "train batch loss:  0.24251951277256012\n",
      "train accuracy:  0.625\n",
      "train batch loss:  0.08557289093732834\n",
      "train accuracy:  0.96875\n",
      "train batch loss:  0.2730984687805176\n",
      "train accuracy:  0.59375\n",
      "train batch loss:  0.3542650043964386\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.1964203119277954\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.18238666653633118\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.11651552468538284\n",
      "train accuracy:  0.8125\n",
      "train batch loss:  0.2347961813211441\n",
      "train accuracy:  0.65625\n",
      "train batch loss:  0.2049972414970398\n",
      "train accuracy:  0.84375\n",
      "train batch loss:  0.20733310282230377\n",
      "train accuracy:  0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss:  0.2177867889404297\n",
      "train accuracy:  0.75\n",
      "eval batch loss:  0.7113252878189087\n",
      "eval accuracy:  0.75\n",
      "eval batch loss:  0.5569972991943359\n",
      "eval accuracy:  0.53125\n",
      "Epoch: 20, Train Loss: 0.245, Train Acc: 76.76%, Val. Loss: 0.686, Val. Acc: 65.28%\n",
      "time duration:     65.28521787002683\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "#print(\"loading previous frnn3 model...\")\n",
    "#model = torch.load('frnn3')\n",
    "try:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        train_loss, train_acc = train(Shoesmodel, Shoestrain_iterator, Shoesoptimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(Shoesmodel, Shoesvalid_iterator, criterion)\n",
    "        #print(\"saving model:   frnn8\")\n",
    "        #torch.save(model,'frnn8')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "        #print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%')\n",
    "\n",
    "        stop = timeit.default_timer()\n",
    "        print(\"time duration:    \", stop - start)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"interrupt\")\n",
    "    print('Exiting from training early')\n",
    "\n",
    "#print(\"save frnn8 again:\")\n",
    "#torch.save(model,'frnn8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# prediction\n",
    "####################\n",
    "\n",
    "'''\n",
    "print('loading frnn4:')\n",
    "model = torch.load('frnn4',map_location=lambda storage,loc:storage)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "print(\"valid loss: \",valid_loss)\n",
    "print(\"valid acc: \",valid_acc)\n",
    "\n",
    "    \n",
    "print(\"prediction of frnn8.....\")\n",
    "    \n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(sentence,model):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "with open('../sent/ori_gender_data/male_sent_test_less700.tsv','r') as f:\n",
    "    mtest = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_test_less700.tsv','r') as f:\n",
    "    ftest = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftest]\n",
    "ms = [line.split('\\t')[0] for line in mtest]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtest]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftest]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fprem:\")\n",
    "print(fprem[:10])\n",
    "print(\"10 fpref:\")\n",
    "print(fpref[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file fpref_frnn8.txt...\")\n",
    "with open('fpref_frnn8.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file fprem_frnn8.txt...\")\n",
    "with open('fprem_frnn8.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "\n",
    "\n",
    "'''\n",
    "with open('../sent/ori_gender_data/male_sent_tmp_train.tsv','r') as f:\n",
    "    mtrain = f.readlines()\n",
    "\n",
    "with open('../sent/ori_gender_data/female_sent_tmp_train.tsv','r') as f:\n",
    "    ftrain = f.readlines()\n",
    "\n",
    "fs = [line.split('\\t')[0] for line in ftrain]\n",
    "ms = [line.split('\\t')[0] for line in mtrain]\n",
    "\n",
    "mlabel = [int(line.split('\\t')[1].strip('\\n')) for line in mtrain]\n",
    "flabel = [int(line.split('\\t')[1].strip('\\n')) for line in ftrain]\n",
    "\n",
    "fprem = [predict_sentiment(x,model) for x in ms]\n",
    "fpref = [predict_sentiment(x,model) for x in fs]\n",
    "\n",
    "print(\"10 fpref on female_sent_tmp_train.tsv:\")\n",
    "print(fpref[:10])\n",
    "print(\"10 fprem on male_sent_tmp_train.tsv:\")\n",
    "print(fprem[:10])\n",
    "     \n",
    "      \n",
    "print(\"writing fpref to file :fpre_female_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_female_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fpref))\n",
    "print(\"writing fprem to file :fpre_male_sent_tmp_train_frnn4.txt...\")\n",
    "with open('fpre_male_sent_tmp_train_frnn4.txt','w') as f:\n",
    "    f.write(str(fprem))\n",
    "\n",
    "\n",
    "print(\"fpref accuracy:    \",(np.array([round(x) for x in fpref])==np.array(flabel)).mean())\n",
    "print(\"fprem accuracy:    \",(np.array([round(x) for x in fprem])==np.array(mlabel)).mean())\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
