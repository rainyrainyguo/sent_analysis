{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import chain\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "class RNN_VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    1. Hu, Zhiting, et al. \"Toward controlled generation of text.\" ICML. 2017.\n",
    "    2. Bowman, Samuel R., et al. \"Generating sentences from a continuous space.\" arXiv preprint arXiv:1511.06349 (2015).\n",
    "    3. Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vocab, h_dim, z_dim, p_word_dropout=0.3, unk_idx=0, pad_idx=1, start_idx=2, eos_idx=3, max_sent_len=15, pretrained_embeddings=None, freeze_embeddings=False, gpu=False):\n",
    "        super(RNN_VAE, self).__init__()\n",
    "\n",
    "        self.UNK_IDX = unk_idx\n",
    "        self.PAD_IDX = pad_idx\n",
    "        self.START_IDX = start_idx\n",
    "        self.EOS_IDX = eos_idx\n",
    "        self.MAX_SENT_LEN = max_sent_len\n",
    "\n",
    "        self.n_vocab = n_vocab\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.p_word_dropout = p_word_dropout\n",
    "\n",
    "        self.gpu = gpu\n",
    "\n",
    "        \"\"\"\n",
    "        Word embeddings layer\n",
    "        \"\"\"\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb_dim = h_dim\n",
    "            self.word_emb = nn.Embedding(n_vocab, h_dim, self.PAD_IDX)\n",
    "        else:\n",
    "            self.emb_dim = pretrained_embeddings.size(1)\n",
    "            self.word_emb = nn.Embedding(n_vocab, self.emb_dim, self.PAD_IDX)\n",
    "\n",
    "            # Set pretrained embeddings\n",
    "            self.word_emb.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "            if freeze_embeddings:\n",
    "                self.word_emb.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Encoder is GRU with FC layers connected to last hidden unit\n",
    "        \"\"\"\n",
    "        self.encoder = nn.GRU(self.emb_dim, h_dim)\n",
    "        self.q_mu = nn.Linear(h_dim, z_dim)\n",
    "        self.q_logvar = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder is GRU with `z` and `c` appended at its inputs\n",
    "        \"\"\"\n",
    "        self.decoder = nn.GRU(self.emb_dim+z_dim, z_dim, dropout=0.3)\n",
    "        self.decoder_fc = nn.Linear(z_dim, n_vocab)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Grouping the model's parameters: separating encoder, decoder, and discriminator\n",
    "        \"\"\"\n",
    "        self.encoder_params = chain(\n",
    "            self.encoder.parameters(), self.q_mu.parameters(), self.q_logvar.parameters()\n",
    "        )\n",
    "\n",
    "        self.decoder_params = chain(\n",
    "            self.decoder.parameters(), self.decoder_fc.parameters()\n",
    "        )\n",
    "\n",
    "        self.vae_params = chain(\n",
    "            self.word_emb.parameters(), self.encoder_params, self.decoder_params\n",
    "        )\n",
    "        self.vae_params = filter(lambda p: p.requires_grad, self.vae_params)\n",
    "\n",
    "        \"\"\"\n",
    "        Use GPU if set\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward_encoder(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is batch of sentences: seq_len x mbsize\n",
    "        \"\"\"    \n",
    "        inputs = self.word_emb(inputs)\n",
    "        return self.forward_encoder_embed(inputs)\n",
    "\n",
    "    def forward_encoder_embed(self, inputs):\n",
    "        \"\"\"\n",
    "        Inputs is embeddings of: seq_len x mbsize x emb_dim\n",
    "        \"\"\"\n",
    "        _, h = self.encoder(inputs, None)\n",
    "\n",
    "        # Forward to latent\n",
    "        h = h.view(-1, self.h_dim)\n",
    "        mu = self.q_mu(h)\n",
    "        logvar = self.q_logvar(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std*eps; eps ~ N(0, I)\n",
    "        \"\"\"\n",
    "        eps = Variable(torch.randn(self.z_dim))\n",
    "        eps = eps.cuda() if self.gpu else eps\n",
    "        z = mu + torch.exp(logvar/2) * eps\n",
    "        \n",
    "        return z/z.pow(2).sum().pow(0.5)\n",
    "\n",
    "    def sample_z_prior(self, mbsize):\n",
    "        \"\"\"\n",
    "        Sample z ~ p(z) = N(0, I)\n",
    "        \"\"\"\n",
    "        z = Variable(torch.randn(mbsize, self.z_dim))\n",
    "        z = z.cuda() if self.gpu else z\n",
    "        \n",
    "        return z/z.pow(2).sum().pow(0.5)\n",
    "\n",
    "    def forward_decoder(self, inputs, z):\n",
    "        \"\"\"\n",
    "        Inputs must be embeddings: seq_len x mbsize\n",
    "        \"\"\"\n",
    "        dec_inputs = self.word_dropout(inputs)\n",
    "\n",
    "        # Forward\n",
    "        seq_len = dec_inputs.size(0)\n",
    "\n",
    "        # 1 x mbsize x (z_dim+c_dim)\n",
    "        init_h = z.unsqueeze(0)\n",
    "        inputs_emb = self.word_emb(dec_inputs)  # seq_len x mbsize x emb_dim\n",
    "        inputs_emb = torch.cat([inputs_emb, init_h.repeat(seq_len, 1, 1)], 2)\n",
    "\n",
    "        outputs, _ = self.decoder(inputs_emb, init_h)\n",
    "        seq_len, mbsize, _ = outputs.size()\n",
    "\n",
    "        outputs = outputs.view(seq_len*mbsize, -1)\n",
    "        y = self.decoder_fc(outputs)\n",
    "        y = y.view(seq_len, mbsize, self.n_vocab)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------\n",
    "        sentence: sequence of word indices.\n",
    "        use_c_prior: whether to sample `c` from prior or from `discriminator`.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        recon_loss: reconstruction loss of VAE.\n",
    "        kl_loss: KL-div loss of VAE.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        mbsize = sentence.size(1)\n",
    "\n",
    "        # sentence: '<start> I want to fly <eos>'\n",
    "        # enc_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_inputs: '<start> I want to fly <eos>'\n",
    "        # dec_targets: 'I want to fly <eos> <pad>'\n",
    "        pad_words = Variable(torch.LongTensor([self.PAD_IDX])).repeat(1, mbsize)\n",
    "        pad_words = pad_words.cuda() if self.gpu else pad_words\n",
    "\n",
    "        enc_inputs = sentence\n",
    "        dec_inputs = sentence\n",
    "        dec_targets = torch.cat([sentence[1:], pad_words], dim=0)\n",
    "\n",
    "        # Encoder: sentence -> z\n",
    "        mu, logvar = self.forward_encoder(enc_inputs)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "\n",
    "        # Decoder: sentence -> y\n",
    "        y = self.forward_decoder(dec_inputs, z)       \n",
    "        \n",
    "        recon_loss = F.cross_entropy(\n",
    "            y.view(-1, self.n_vocab), dec_targets.view(-1), size_average=True\n",
    "        )\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar, 1))\n",
    "\n",
    "        return recon_loss, kl_loss\n",
    "\n",
    "    def generate_sentences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate sentences and corresponding z of (batch_size x max_sent_len)\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            z = self.sample_z_prior(1)\n",
    "            samples.append(self.sample_sentence(z, raw=True))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def sample_sentence(self, z, raw=False, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single sentence from p(x|z,c) according to given temperature.\n",
    "        `raw = True` means this returns sentence as in dataset which is useful\n",
    "        to train discriminator. `False` means that this will return list of\n",
    "        `word_idx` which is useful for evaluation.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "\n",
    "        z= z.view(1, 1, -1)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        if raw:\n",
    "            outputs.append(self.START_IDX)\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            emb = self.word_emb(word).view(1, 1, -1)\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "            output, h = self.decoder(emb, h)\n",
    "            y = self.decoder_fc(output).view(-1)\n",
    "            y = F.softmax(y/temp, dim=0)\n",
    "\n",
    "            idx = torch.multinomial(y,1)\n",
    "\n",
    "            word = Variable(torch.LongTensor([int(idx)]))\n",
    "            word = word.cuda() if self.gpu else word\n",
    "\n",
    "            idx = int(idx)\n",
    "\n",
    "            if not raw and idx == self.EOS_IDX:\n",
    "                break\n",
    "\n",
    "            outputs.append(idx)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        if raw:\n",
    "            outputs = Variable(torch.LongTensor(outputs)).unsqueeze(0)\n",
    "            return outputs.cuda() if self.gpu else outputs\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def generate_soft_embed(self, mbsize, temp=1):\n",
    "        \"\"\"\n",
    "        Generate soft embeddings of (mbsize x emb_dim) along with target z\n",
    "        and c for each row (mbsize x {z_dim, c_dim})\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        targets_z = []\n",
    "\n",
    "        for _ in range(mbsize):\n",
    "            z = self.sample_z_prior(1)\n",
    "\n",
    "            samples.append(self.sample_soft_embed(z, temp=1))\n",
    "            targets_z.append(z)\n",
    "\n",
    "        X_gen = torch.cat(samples, dim=0)\n",
    "        targets_z = torch.cat(targets_z, dim=0)\n",
    "\n",
    "        return X_gen, targets_z\n",
    "\n",
    "    def sample_soft_embed(self, z, temp=1):\n",
    "        \"\"\"\n",
    "        Sample single soft embedded sentence from p(x|z,c) and temperature.\n",
    "        Soft embeddings are calculated as weighted average of word_emb\n",
    "        according to p(x|z,c).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        z = z.view(1, 1, -1)\n",
    "\n",
    "        word = torch.LongTensor([self.START_IDX])\n",
    "        word = word.cuda() if self.gpu else word\n",
    "        word = Variable(word)  # '<start>'\n",
    "        emb = self.word_emb(word).view(1, 1, -1)\n",
    "        emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        h = z\n",
    "\n",
    "        if not isinstance(h, Variable):\n",
    "            h = Variable(h)\n",
    "\n",
    "        outputs = [self.word_emb(word).view(1, -1)]\n",
    "\n",
    "        for i in range(self.MAX_SENT_LEN):\n",
    "            output, h = self.decoder(emb, h)\n",
    "            o = self.decoder_fc(output).view(-1)\n",
    "\n",
    "            # Sample softmax with temperature\n",
    "            y = F.softmax(o / temp, dim=0)\n",
    "\n",
    "            # Take expectation of embedding given output prob -> soft embedding\n",
    "            # <y, w> = 1 x n_vocab * n_vocab x emb_dim\n",
    "            emb = y.unsqueeze(0) @ self.word_emb.weight\n",
    "            emb = emb.view(1, 1, -1)\n",
    "\n",
    "            # Save resulting soft embedding\n",
    "            outputs.append(emb.view(1, -1))\n",
    "\n",
    "            # Append with z and c for the next input\n",
    "            emb = torch.cat([emb, z], 2)\n",
    "\n",
    "        # 1 x 16 x emb_dim\n",
    "        outputs = torch.cat(outputs, dim=0).unsqueeze(0)\n",
    "\n",
    "        # Back to default state: train\n",
    "        self.train()\n",
    "\n",
    "        return outputs.cuda() if self.gpu else outputs\n",
    "\n",
    "    def word_dropout(self, inputs):\n",
    "        \"\"\"\n",
    "        Do word dropout: with prob `p_word_dropout`, set the word to '<unk>'.\n",
    "        \"\"\"\n",
    "        if isinstance(inputs, Variable):\n",
    "            data = inputs.data.clone()\n",
    "        else:\n",
    "            data = inputs.clone()\n",
    "\n",
    "        # Sample masks: elems with val 1 will be set to <unk>\n",
    "        mask = torch.from_numpy(\n",
    "            np.random.binomial(1, p=self.p_word_dropout, size=tuple(data.size()))\n",
    "                     .astype('uint8')\n",
    "        )\n",
    "\n",
    "        if self.gpu:\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        # Set to <unk>\n",
    "        data[mask] = self.UNK_IDX\n",
    "\n",
    "        return Variable(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty300.tsv...\n",
      "creating vocab for BTEXT\n",
      "loading dataset clean_Apparel300.tsv...\n",
      "creating vocab for ATEXT\n"
     ]
    }
   ],
   "source": [
    "#B: Beauty\n",
    "#A: Apparel\n",
    "\n",
    "###########################################################################\n",
    "BTEXT = data.Field(tokenize='spacy')\n",
    "BLABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300.tsv...\")\n",
    "Btrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Beauty300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', BTEXT),('Label', BLABEL)])[0]\n",
    "\n",
    "print(\"creating vocab for BTEXT\")\n",
    "BTEXT.build_vocab(Btrain, max_size=60000, vectors=\"glove.6B.100d\",min_freq=1)\n",
    "BLABEL.build_vocab(Btrain)\n",
    "\n",
    "BLABEL.vocab.stoi['1']=1\n",
    "BLABEL.vocab.stoi['2']=2\n",
    "BLABEL.vocab.stoi['3']=3\n",
    "BLABEL.vocab.stoi['4']=4\n",
    "BLABEL.vocab.stoi['5']=5\n",
    "\n",
    "\n",
    "#################################\n",
    "ATEXT = data.Field(tokenize='spacy')\n",
    "ALABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "print(\"loading dataset clean_Apparel300.tsv...\")\n",
    "Atrain  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Apparel300.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', ATEXT),('Label', ALABEL)])[0]\n",
    "\n",
    "print(\"creating vocab for ATEXT\")\n",
    "ATEXT.build_vocab(Atrain, max_size=60000, vectors=\"glove.6B.100d\",min_freq=1)\n",
    "ALABEL.build_vocab(Atrain)\n",
    "\n",
    "ALABEL.vocab.stoi['1']=1\n",
    "ALABEL.vocab.stoi['2']=2\n",
    "ALABEL.vocab.stoi['3']=3\n",
    "ALABEL.vocab.stoi['4']=4\n",
    "ALABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Beauty_classifier model and Beauty_classifier model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torch/serialization.py:333: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):       \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))        \n",
    "        y = self.fc(hidden.squeeze(0))\n",
    "        return y\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "#frnn = torch.load('frnn8', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "#mrnn = torch.load('mrnn8', map_location=lambda storage, loc: storage) #force to load on CPU\n",
    "print(\"loading Beauty_classifier model and Beauty_classifier model....\")\n",
    "Bmodel = torch.load('Amazon/models/Beauty_classifier')\n",
    "Amodel = torch.load('Amazon/models/Apparel_classifier')\n",
    "Bmodel = Bmodel.to(device)\n",
    "Amodel = Amodel.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def predict_sentiment(sentence,model,TEXT):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    model.eval()\n",
    "    prediction = model(tensor)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "def predict_word(word):\n",
    "    print('frnn: ',predict_sentiment(word,frnn,fTEXT))\n",
    "    print('mrnn:  ',predict_sentiment(word,mrnn,BTEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset clean_Beauty300test.tsv...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################      \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from ctextgen.dataset import *\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "mb_size = 32\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "lr_decay_every = 1000000\n",
    "n_iter = 20000\n",
    "log_interval = 1000\n",
    "z_dim = 128\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "from torchtext import data\n",
    "\n",
    "#TEXT = data.Field(init_token='<start>', eos_token='<eos>', lower=True, tokenize='spacy',fix_length=20)\n",
    "TEXT = data.Field(init_token='<start>', eos_token='<eos>', lower=True, tokenize='spacy')\n",
    "LABEL = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "print(\"loading dataset clean_Beauty300test.tsv...\")\n",
    "train  = data.TabularDataset.splits(\n",
    "        path='../counter-sent-generation3/VAE/data/official_Amazon/', \n",
    "        train='clean_Beauty300test.tsv',\n",
    "        format='tsv',\n",
    "        fields=[('Text', TEXT),('Label', LABEL)])[0]\n",
    "\n",
    "TEXT.build_vocab(train, max_size=100000, vectors=\"fasttext.en.300d\",min_freq=1)\n",
    "#TEXT.build_vocab(train, max_size=10000)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "LABEL.vocab.stoi['1']=1\n",
    "LABEL.vocab.stoi['2']=2\n",
    "LABEL.vocab.stoi['3']=3\n",
    "LABEL.vocab.stoi['4']=4\n",
    "LABEL.vocab.stoi['5']=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojy/anaconda3/envs/pt4/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNN_VAE(\n",
    "    len(TEXT.vocab), h_dim, z_dim, p_word_dropout=0.3,max_sent_len=40,\n",
    "    pretrained_embeddings=TEXT.vocab.vectors, freeze_embeddings=False,\n",
    "    gpu=False\n",
    ")\n",
    "##################### 注意这里有设置GPU!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Annealing for KL term\n",
    "kld_start_inc = 3000\n",
    "kld_weight = 0.03\n",
    "kld_max = 0.15\n",
    "kld_inc = (kld_max - kld_weight) / (n_iter - kld_start_inc)\n",
    "\n",
    "trainer = optim.Adam(model.vae_params, lr=lr)\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "dataset=train, batch_size=mb_size,\n",
    "sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
    "\n",
    "\n",
    "#print(\"loading previous 200yelp_nofix_max40_predif.bin model\")\n",
    "#model.load_state_dict(torch.load('models/{}.bin'.format('pre_yelp128_all_sent_trainVAE_sph_less100')))\n",
    "    \n",
    "def save_model():\n",
    "    if not os.path.exists('Amazon/models/'):\n",
    "        os.makedirs('Amazon/models/')\n",
    "\n",
    "    torch.save(model.state_dict(), 'Amazon/models/{}.bin'.format('Amazon_Beauty300test_baseVAE_sph'))   \n",
    "    \n",
    "# Amazon_Beauty300test_baseVAE_sph 是用 clean_Beauty300test.tsv 训练的 base_VAE model, 词汇max=10000, max_sent_len = 40\n",
    "    \n",
    "# pre_yelp128_all_sent_trainVAE_less100.bin 是用 all_sent_trainVAE_less100.tsv 训练的base_VAE model, 词汇max=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a Tensor of type torch.cuda.LongTensor but found a type torch.LongTensor for sequence element 1  in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3abd560771d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecon_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkld_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#*(pre_weight*pre_dif)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(\"pre_weight*pre_dif: \",pre_weight*pre_dif)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7c0797ae18a6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0menc_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mdec_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mdec_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Encoder: sentence -> z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a Tensor of type torch.cuda.LongTensor but found a type torch.LongTensor for sequence element 1  in sequence argument at position #1 'tensors'"
     ]
    }
   ],
   "source": [
    "for it in range(100000):\n",
    "    batch = next(iter(train_iter))\n",
    "    inputs = batch.Text\n",
    "    labels = batch.Label\n",
    "\n",
    "    recon_loss, kl_loss = model.forward(inputs)\n",
    "    loss = (recon_loss + kld_weight * kl_loss)#*(pre_weight*pre_dif)\n",
    "    #print(\"pre_weight*pre_dif: \",pre_weight*pre_dif)\n",
    "\n",
    "    # Anneal kl_weight\n",
    "    if it > kld_start_inc and kld_weight < kld_max:\n",
    "        kld_weight += kld_inc\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm(model.vae_params, 5)\n",
    "    trainer.step()\n",
    "    trainer.zero_grad()\n",
    "\n",
    "\n",
    "    #if it % log_interval == 0:\n",
    "    if it%200==0:\n",
    "        #original_sent = ' '.join([TEXT.vocab.itos[i] for i in inputs[:,0][1:]])\n",
    "        #m = predict_sentiment(original_sent,Bmodel,BTEXT)\n",
    "        #f = predict_sentiment(original_sent,Amodel,ATEXT)\n",
    "        #print(original_sent)\n",
    "        #print(\"Bmodel original prediction: \",m)\n",
    "        #print(\"Amodel original prediction: \",f)\n",
    "        #print(\"abs original dif: \",abs(m-f))\n",
    "\n",
    "\n",
    "        z = model.sample_z_prior(1)\n",
    "        sample_idxs = model.sample_sentence(z)\n",
    "        sample_sent = ' '.join([TEXT.vocab.itos[i] for i in sample_idxs])\n",
    "\n",
    "        print('Iter-{}; Loss: {:.4f}; Recon: {:.4f}; KL: {:.4f}; Grad_norm: {:.4f};'\n",
    "              .format(it, loss.data[0], recon_loss.data[0], kl_loss.data[0], grad_norm))\n",
    "        print('Sample: \"{}\"'.format(sample_sent))\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "        if sample_sent:\n",
    "            m0 = predict_sentiment(sample_sent,Bmodel,BTEXT)\n",
    "            f0 = predict_sentiment(sample_sent,Amodel,ATEXT)\n",
    "            pre_dif_sample = abs(f0-m0)\n",
    "            '''\n",
    "            if pre_dif_sample>1.5:\n",
    "                f = open('baseVAE_sph_log','a')\n",
    "                f.write(str(it)+'\\t'+str(pre_dif_sample)+'\\t'+str(m0)+'\\t'+str(f0)+'\\n')\n",
    "                f.write(sample_sent+'\\n')\n",
    "                f.close()\n",
    "            '''\n",
    "            print(\"Bmodel sample prediction: \",m0)\n",
    "            print(\"Amodel sample prediction: \",f0)\n",
    "            print(\"sample abs dif: \",abs(m0-f0))\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # Anneal learning rate\n",
    "    new_lr = lr * (0.5 ** (it // lr_decay_every))\n",
    "    for param_group in trainer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    if it%1000==0:\n",
    "        #print(\"saving model Amazon_Beauty300test_baseVAE_sph.bin\")\n",
    "        print(\"\\n\")\n",
    "        #save_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Amazon/models/{}.bin'.format('Amazon_Beauty300test_baseVAE_sph')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
